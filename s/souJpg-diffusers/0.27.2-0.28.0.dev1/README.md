# Comparing `tmp/souJpg_diffusers-0.27.2-py3-none-any.whl.zip` & `tmp/souJpg_diffusers-0.28.0.dev1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,401 +1,401 @@
-Zip file size: 2025798 bytes, number of entries: 399
--rw-r--r--  2.0 unx    28672 b- defN 24-Mar-25 00:10 diffusers/__init__.py
--rw-r--r--  2.0 unx    31927 b- defN 24-Mar-25 00:10 diffusers/configuration_utils.py
--rw-r--r--  2.0 unx     1271 b- defN 24-Mar-25 00:09 diffusers/dependency_versions_check.py
--rw-r--r--  2.0 unx     1485 b- defN 24-Mar-25 00:09 diffusers/dependency_versions_table.py
--rw-r--r--  2.0 unx    40698 b- defN 24-Mar-25 00:09 diffusers/image_processor.py
--rw-r--r--  2.0 unx    14743 b- defN 24-Mar-25 00:09 diffusers/optimization.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-25 00:09 diffusers/py.typed
--rw-r--r--  2.0 unx    18932 b- defN 24-Mar-25 00:09 diffusers/training_utils.py
--rw-r--r--  2.0 unx      920 b- defN 24-Mar-25 00:09 diffusers/commands/__init__.py
--rw-r--r--  2.0 unx     1317 b- defN 24-Mar-25 00:09 diffusers/commands/diffusers_cli.py
--rw-r--r--  2.0 unx     2870 b- defN 24-Mar-25 00:09 diffusers/commands/env.py
--rw-r--r--  2.0 unx     5423 b- defN 24-Mar-25 00:09 diffusers/commands/fp16_safetensors.py
--rw-r--r--  2.0 unx       38 b- defN 24-Mar-25 00:09 diffusers/experimental/__init__.py
--rw-r--r--  2.0 unx       57 b- defN 24-Mar-25 00:09 diffusers/experimental/rl/__init__.py
--rw-r--r--  2.0 unx     6033 b- defN 24-Mar-25 00:09 diffusers/experimental/rl/value_guided_sampling.py
--rw-r--r--  2.0 unx     3705 b- defN 24-Mar-25 00:09 diffusers/loaders/__init__.py
--rw-r--r--  2.0 unx     7664 b- defN 24-Mar-25 00:09 diffusers/loaders/autoencoder.py
--rw-r--r--  2.0 unx     7107 b- defN 24-Mar-25 00:09 diffusers/loaders/controlnet.py
--rw-r--r--  2.0 unx    14330 b- defN 24-Mar-25 00:10 diffusers/loaders/ip_adapter.py
--rw-r--r--  2.0 unx    62835 b- defN 24-Mar-25 00:10 diffusers/loaders/lora.py
--rw-r--r--  2.0 unx    14365 b- defN 24-Mar-25 00:10 diffusers/loaders/lora_conversion_utils.py
--rw-r--r--  2.0 unx     8385 b- defN 24-Mar-25 00:09 diffusers/loaders/peft.py
--rw-r--r--  2.0 unx    13517 b- defN 24-Mar-25 00:09 diffusers/loaders/single_file.py
--rw-r--r--  2.0 unx    67296 b- defN 24-Mar-25 00:10 diffusers/loaders/single_file_utils.py
--rw-r--r--  2.0 unx    26563 b- defN 24-Mar-25 00:10 diffusers/loaders/textual_inversion.py
--rw-r--r--  2.0 unx    46429 b- defN 24-Mar-25 00:10 diffusers/loaders/unet.py
--rw-r--r--  2.0 unx     2423 b- defN 24-Mar-25 00:09 diffusers/loaders/utils.py
--rw-r--r--  2.0 unx     4307 b- defN 24-Mar-25 00:09 diffusers/models/__init__.py
--rw-r--r--  2.0 unx     4490 b- defN 24-Mar-25 00:09 diffusers/models/activations.py
--rw-r--r--  2.0 unx    24723 b- defN 24-Mar-25 00:09 diffusers/models/adapter.py
--rw-r--r--  2.0 unx    27962 b- defN 24-Mar-25 00:10 diffusers/models/attention.py
--rw-r--r--  2.0 unx    20250 b- defN 24-Mar-25 00:09 diffusers/models/attention_flax.py
--rw-r--r--  2.0 unx   108049 b- defN 24-Mar-25 00:10 diffusers/models/attention_processor.py
--rw-r--r--  2.0 unx    43152 b- defN 24-Mar-25 00:10 diffusers/models/controlnet.py
--rw-r--r--  2.0 unx    16710 b- defN 24-Mar-25 00:09 diffusers/models/controlnet_flax.py
--rw-r--r--  2.0 unx    12490 b- defN 24-Mar-25 00:09 diffusers/models/downsampling.py
--rw-r--r--  2.0 unx     1105 b- defN 24-Mar-25 00:09 diffusers/models/dual_transformer_2d.py
--rw-r--r--  2.0 unx    34635 b- defN 24-Mar-25 00:09 diffusers/models/embeddings.py
--rw-r--r--  2.0 unx     3445 b- defN 24-Mar-25 00:09 diffusers/models/embeddings_flax.py
--rw-r--r--  2.0 unx    18829 b- defN 24-Mar-25 00:09 diffusers/models/lora.py
--rw-r--r--  2.0 unx     5332 b- defN 24-Mar-25 00:10 diffusers/models/modeling_flax_pytorch_utils.py
--rw-r--r--  2.0 unx    27368 b- defN 24-Mar-25 00:09 diffusers/models/modeling_flax_utils.py
--rw-r--r--  2.0 unx      512 b- defN 24-Mar-25 00:09 diffusers/models/modeling_outputs.py
--rw-r--r--  2.0 unx     6974 b- defN 24-Mar-25 00:10 diffusers/models/modeling_pytorch_flax_utils.py
--rw-r--r--  2.0 unx    48262 b- defN 24-Mar-25 00:10 diffusers/models/modeling_utils.py
--rw-r--r--  2.0 unx     9508 b- defN 24-Mar-25 00:09 diffusers/models/normalization.py
--rw-r--r--  2.0 unx      877 b- defN 24-Mar-25 00:09 diffusers/models/prior_transformer.py
--rw-r--r--  2.0 unx    32398 b- defN 24-Mar-25 00:09 diffusers/models/resnet.py
--rw-r--r--  2.0 unx     4021 b- defN 24-Mar-25 00:09 diffusers/models/resnet_flax.py
--rw-r--r--  2.0 unx     4196 b- defN 24-Mar-25 00:09 diffusers/models/t5_film_transformer.py
--rw-r--r--  2.0 unx     1492 b- defN 24-Mar-25 00:09 diffusers/models/transformer_2d.py
--rw-r--r--  2.0 unx     2082 b- defN 24-Mar-25 00:10 diffusers/models/transformer_temporal.py
--rw-r--r--  2.0 unx     1323 b- defN 24-Mar-25 00:09 diffusers/models/unet_1d.py
--rw-r--r--  2.0 unx     9963 b- defN 24-Mar-25 00:09 diffusers/models/unet_1d_blocks.py
--rw-r--r--  2.0 unx     1324 b- defN 24-Mar-25 00:09 diffusers/models/unet_2d.py
--rw-r--r--  2.0 unx    18582 b- defN 24-Mar-25 00:09 diffusers/models/unet_2d_blocks.py
--rw-r--r--  2.0 unx     1480 b- defN 24-Mar-25 00:09 diffusers/models/unet_2d_condition.py
--rw-r--r--  2.0 unx    16731 b- defN 24-Mar-25 00:09 diffusers/models/upsampling.py
--rw-r--r--  2.0 unx    31942 b- defN 24-Mar-25 00:09 diffusers/models/vae_flax.py
--rw-r--r--  2.0 unx     7612 b- defN 24-Mar-25 00:09 diffusers/models/vq_model.py
--rw-r--r--  2.0 unx      278 b- defN 24-Mar-25 00:09 diffusers/models/autoencoders/__init__.py
--rw-r--r--  2.0 unx     7798 b- defN 24-Mar-25 00:09 diffusers/models/autoencoders/autoencoder_asym_kl.py
--rw-r--r--  2.0 unx    21391 b- defN 24-Mar-25 00:09 diffusers/models/autoencoders/autoencoder_kl.py
--rw-r--r--  2.0 unx    16272 b- defN 24-Mar-25 00:09 diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py
--rw-r--r--  2.0 unx    15991 b- defN 24-Mar-25 00:09 diffusers/models/autoencoders/autoencoder_tiny.py
--rw-r--r--  2.0 unx    18655 b- defN 24-Mar-25 00:09 diffusers/models/autoencoders/consistency_decoder_vae.py
--rw-r--r--  2.0 unx    36674 b- defN 24-Mar-25 00:09 diffusers/models/autoencoders/vae.py
--rw-r--r--  2.0 unx      344 b- defN 24-Mar-25 00:09 diffusers/models/transformers/__init__.py
--rw-r--r--  2.0 unx     7665 b- defN 24-Mar-25 00:09 diffusers/models/transformers/dual_transformer_2d.py
--rw-r--r--  2.0 unx    17348 b- defN 24-Mar-25 00:09 diffusers/models/transformers/prior_transformer.py
--rw-r--r--  2.0 unx    16163 b- defN 24-Mar-25 00:09 diffusers/models/transformers/t5_film_transformer.py
--rw-r--r--  2.0 unx    23855 b- defN 24-Mar-25 00:10 diffusers/models/transformers/transformer_2d.py
--rw-r--r--  2.0 unx    16821 b- defN 24-Mar-25 00:09 diffusers/models/transformers/transformer_temporal.py
--rw-r--r--  2.0 unx      695 b- defN 24-Mar-25 00:09 diffusers/models/unets/__init__.py
--rw-r--r--  2.0 unx    10794 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_1d.py
--rw-r--r--  2.0 unx    27093 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_1d_blocks.py
--rw-r--r--  2.0 unx    16581 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_2d.py
--rw-r--r--  2.0 unx   147891 b- defN 24-Mar-25 00:10 diffusers/models/unets/unet_2d_blocks.py
--rw-r--r--  2.0 unx    15572 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_2d_blocks_flax.py
--rw-r--r--  2.0 unx    67039 b- defN 24-Mar-25 00:10 diffusers/models/unets/unet_2d_condition.py
--rw-r--r--  2.0 unx    22269 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_2d_condition_flax.py
--rw-r--r--  2.0 unx    89511 b- defN 24-Mar-25 00:10 diffusers/models/unets/unet_3d_blocks.py
--rw-r--r--  2.0 unx    34691 b- defN 24-Mar-25 00:10 diffusers/models/unets/unet_3d_condition.py
--rw-r--r--  2.0 unx    32728 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_i2vgen_xl.py
--rw-r--r--  2.0 unx    20758 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_kandinsky3.py
--rw-r--r--  2.0 unx    42225 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_motion_model.py
--rw-r--r--  2.0 unx    22104 b- defN 24-Mar-25 00:09 diffusers/models/unets/unet_spatio_temporal_condition.py
--rw-r--r--  2.0 unx    28254 b- defN 24-Mar-25 00:10 diffusers/models/unets/unet_stable_cascade.py
--rw-r--r--  2.0 unx    17338 b- defN 24-Mar-25 00:09 diffusers/models/unets/uvit_2d.py
--rw-r--r--  2.0 unx    22315 b- defN 24-Mar-25 00:09 diffusers/pipelines/__init__.py
--rw-r--r--  2.0 unx    50268 b- defN 24-Mar-25 00:10 diffusers/pipelines/auto_pipeline.py
--rw-r--r--  2.0 unx     7564 b- defN 24-Mar-25 00:10 diffusers/pipelines/free_init_utils.py
--rw-r--r--  2.0 unx     8329 b- defN 24-Mar-25 00:09 diffusers/pipelines/onnx_utils.py
--rw-r--r--  2.0 unx    27479 b- defN 24-Mar-25 00:09 diffusers/pipelines/pipeline_flax_utils.py
--rw-r--r--  2.0 unx    20613 b- defN 24-Mar-25 00:09 diffusers/pipelines/pipeline_loading_utils.py
--rw-r--r--  2.0 unx    87939 b- defN 24-Mar-25 00:10 diffusers/pipelines/pipeline_utils.py
--rw-r--r--  2.0 unx     1793 b- defN 24-Mar-25 00:09 diffusers/pipelines/amused/__init__.py
--rw-r--r--  2.0 unx    15652 b- defN 24-Mar-25 00:09 diffusers/pipelines/amused/pipeline_amused.py
--rw-r--r--  2.0 unx    17109 b- defN 24-Mar-25 00:10 diffusers/pipelines/amused/pipeline_amused_img2img.py
--rw-r--r--  2.0 unx    18806 b- defN 24-Mar-25 00:09 diffusers/pipelines/amused/pipeline_amused_inpaint.py
--rw-r--r--  2.0 unx     1584 b- defN 24-Mar-25 00:09 diffusers/pipelines/animatediff/__init__.py
--rw-r--r--  2.0 unx    41491 b- defN 24-Mar-25 00:10 diffusers/pipelines/animatediff/pipeline_animatediff.py
--rw-r--r--  2.0 unx    48805 b- defN 24-Mar-25 00:10 diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py
--rw-r--r--  2.0 unx      717 b- defN 24-Mar-25 00:09 diffusers/pipelines/animatediff/pipeline_output.py
--rw-r--r--  2.0 unx     1419 b- defN 24-Mar-25 00:09 diffusers/pipelines/audioldm/__init__.py
--rw-r--r--  2.0 unx    26054 b- defN 24-Mar-25 00:09 diffusers/pipelines/audioldm/pipeline_audioldm.py
--rw-r--r--  2.0 unx     1605 b- defN 24-Mar-25 00:09 diffusers/pipelines/audioldm2/__init__.py
--rw-r--r--  2.0 unx    72277 b- defN 24-Mar-25 00:09 diffusers/pipelines/audioldm2/modeling_audioldm2.py
--rw-r--r--  2.0 unx    49174 b- defN 24-Mar-25 00:09 diffusers/pipelines/audioldm2/pipeline_audioldm2.py
--rw-r--r--  2.0 unx      697 b- defN 24-Mar-25 00:09 diffusers/pipelines/blip_diffusion/__init__.py
--rw-r--r--  2.0 unx    16747 b- defN 24-Mar-25 00:09 diffusers/pipelines/blip_diffusion/blip_image_processing.py
--rw-r--r--  2.0 unx    27308 b- defN 24-Mar-25 00:09 diffusers/pipelines/blip_diffusion/modeling_blip2.py
--rw-r--r--  2.0 unx     9007 b- defN 24-Mar-25 00:09 diffusers/pipelines/blip_diffusion/modeling_ctx_clip.py
--rw-r--r--  2.0 unx    15011 b- defN 24-Mar-25 00:09 diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py
--rw-r--r--  2.0 unx      484 b- defN 24-Mar-25 00:09 diffusers/pipelines/consistency_models/__init__.py
--rw-r--r--  2.0 unx    12393 b- defN 24-Mar-25 00:09 diffusers/pipelines/consistency_models/pipeline_consistency_models.py
--rw-r--r--  2.0 unx     3483 b- defN 24-Mar-25 00:09 diffusers/pipelines/controlnet/__init__.py
--rw-r--r--  2.0 unx     9510 b- defN 24-Mar-25 00:09 diffusers/pipelines/controlnet/multicontrolnet.py
--rw-r--r--  2.0 unx    67128 b- defN 24-Mar-25 00:10 diffusers/pipelines/controlnet/pipeline_controlnet.py
--rw-r--r--  2.0 unx    17373 b- defN 24-Mar-25 00:09 diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py
--rw-r--r--  2.0 unx    66454 b- defN 24-Mar-25 00:09 diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py
--rw-r--r--  2.0 unx    81386 b- defN 24-Mar-25 00:10 diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py
--rw-r--r--  2.0 unx    93308 b- defN 24-Mar-25 00:10 diffusers/pipelines/controlnet/pipeline_controlnet_inpaint_sd_xl.py
--rw-r--r--  2.0 unx    77396 b- defN 24-Mar-25 00:10 diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py
--rw-r--r--  2.0 unx    84855 b- defN 24-Mar-25 00:10 diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py
--rw-r--r--  2.0 unx    22663 b- defN 24-Mar-25 00:09 diffusers/pipelines/controlnet/pipeline_flax_controlnet.py
--rw-r--r--  2.0 unx      453 b- defN 24-Mar-25 00:09 diffusers/pipelines/dance_diffusion/__init__.py
--rw-r--r--  2.0 unx     6322 b- defN 24-Mar-25 00:09 diffusers/pipelines/dance_diffusion/pipeline_dance_diffusion.py
--rw-r--r--  2.0 unx      411 b- defN 24-Mar-25 00:09 diffusers/pipelines/ddim/__init__.py
--rw-r--r--  2.0 unx     6603 b- defN 24-Mar-25 00:09 diffusers/pipelines/ddim/pipeline_ddim.py
--rw-r--r--  2.0 unx      425 b- defN 24-Mar-25 00:09 diffusers/pipelines/ddpm/__init__.py
--rw-r--r--  2.0 unx     4959 b- defN 24-Mar-25 00:09 diffusers/pipelines/ddpm/pipeline_ddpm.py
--rw-r--r--  2.0 unx     2975 b- defN 24-Mar-25 00:09 diffusers/pipelines/deepfloyd_if/__init__.py
--rw-r--r--  2.0 unx    35933 b- defN 24-Mar-25 00:09 diffusers/pipelines/deepfloyd_if/pipeline_if.py
--rw-r--r--  2.0 unx    40250 b- defN 24-Mar-25 00:10 diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py
--rw-r--r--  2.0 unx    45509 b- defN 24-Mar-25 00:10 diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py
--rw-r--r--  2.0 unx    45549 b- defN 24-Mar-25 00:10 diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py
--rw-r--r--  2.0 unx    50365 b- defN 24-Mar-25 00:10 diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py
--rw-r--r--  2.0 unx    40292 b- defN 24-Mar-25 00:09 diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py
--rw-r--r--  2.0 unx     1140 b- defN 24-Mar-25 00:09 diffusers/pipelines/deepfloyd_if/pipeline_output.py
--rw-r--r--  2.0 unx     2117 b- defN 24-Mar-25 00:09 diffusers/pipelines/deepfloyd_if/safety_checker.py
--rw-r--r--  2.0 unx     5164 b- defN 24-Mar-25 00:09 diffusers/pipelines/deepfloyd_if/timesteps.py
--rw-r--r--  2.0 unx     1601 b- defN 24-Mar-25 00:09 diffusers/pipelines/deepfloyd_if/watermark.py
--rw-r--r--  2.0 unx     5470 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/__init__.py
--rw-r--r--  2.0 unx     1783 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/alt_diffusion/__init__.py
--rw-r--r--  2.0 unx     5580 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/alt_diffusion/modeling_roberta_series.py
--rw-r--r--  2.0 unx    48491 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/alt_diffusion/pipeline_alt_diffusion.py
--rw-r--r--  2.0 unx    51792 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/alt_diffusion/pipeline_alt_diffusion_img2img.py
--rw-r--r--  2.0 unx      928 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/alt_diffusion/pipeline_output.py
--rw-r--r--  2.0 unx      507 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/audio_diffusion/__init__.py
--rw-r--r--  2.0 unx     5764 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/audio_diffusion/mel.py
--rw-r--r--  2.0 unx    13231 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/audio_diffusion/pipeline_audio_diffusion.py
--rw-r--r--  2.0 unx      448 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/latent_diffusion_uncond/__init__.py
--rw-r--r--  2.0 unx     5381 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/latent_diffusion_uncond/pipeline_latent_diffusion_uncond.py
--rw-r--r--  2.0 unx      412 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/pndm/__init__.py
--rw-r--r--  2.0 unx     4658 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/pndm/pipeline_pndm.py
--rw-r--r--  2.0 unx      425 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/repaint/__init__.py
--rw-r--r--  2.0 unx    10049 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/repaint/pipeline_repaint.py
--rw-r--r--  2.0 unx      441 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/score_sde_ve/__init__.py
--rw-r--r--  2.0 unx     4390 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/score_sde_ve/pipeline_score_sde_ve.py
--rw-r--r--  2.0 unx     2588 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/spectrogram_diffusion/__init__.py
--rw-r--r--  2.0 unx     3100 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/spectrogram_diffusion/continuous_encoder.py
--rw-r--r--  2.0 unx    25096 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/spectrogram_diffusion/midi_utils.py
--rw-r--r--  2.0 unx     2923 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/spectrogram_diffusion/notes_encoder.py
--rw-r--r--  2.0 unx    11553 b- defN 24-Mar-25 00:10 diffusers/pipelines/deprecated/spectrogram_diffusion/pipeline_spectrogram_diffusion.py
--rw-r--r--  2.0 unx     2111 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stable_diffusion_variants/__init__.py
--rw-r--r--  2.0 unx    47812 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_cycle_diffusion.py
--rw-r--r--  2.0 unx    27814 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_onnx_stable_diffusion_inpaint_legacy.py
--rw-r--r--  2.0 unx    42346 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_inpaint_legacy.py
--rw-r--r--  2.0 unx    41304 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_model_editing.py
--rw-r--r--  2.0 unx    40999 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_paradigms.py
--rw-r--r--  2.0 unx    63419 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_pix2pix_zero.py
--rw-r--r--  2.0 unx      453 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stochastic_karras_ve/__init__.py
--rw-r--r--  2.0 unx     5277 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/stochastic_karras_ve/pipeline_stochastic_karras_ve.py
--rw-r--r--  2.0 unx     2838 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/versatile_diffusion/__init__.py
--rw-r--r--  2.0 unx   115479 b- defN 24-Mar-25 00:10 diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py
--rw-r--r--  2.0 unx    21916 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion.py
--rw-r--r--  2.0 unx    27122 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py
--rw-r--r--  2.0 unx    19641 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py
--rw-r--r--  2.0 unx    22840 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py
--rw-r--r--  2.0 unx     1650 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/vq_diffusion/__init__.py
--rw-r--r--  2.0 unx    15474 b- defN 24-Mar-25 00:09 diffusers/pipelines/deprecated/vq_diffusion/pipeline_vq_diffusion.py
--rw-r--r--  2.0 unx      408 b- defN 24-Mar-25 00:09 diffusers/pipelines/dit/__init__.py
--rw-r--r--  2.0 unx     9869 b- defN 24-Mar-25 00:09 diffusers/pipelines/dit/pipeline_dit.py
--rw-r--r--  2.0 unx     1307 b- defN 24-Mar-25 00:09 diffusers/pipelines/i2vgen_xl/__init__.py
--rw-r--r--  2.0 unx    37709 b- defN 24-Mar-25 00:09 diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py
--rw-r--r--  2.0 unx     2312 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky/__init__.py
--rw-r--r--  2.0 unx    17740 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky/pipeline_kandinsky.py
--rw-r--r--  2.0 unx    39237 b- defN 24-Mar-25 00:10 diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py
--rw-r--r--  2.0 unx    21855 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py
--rw-r--r--  2.0 unx    28573 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py
--rw-r--r--  2.0 unx    23845 b- defN 24-Mar-25 00:10 diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py
--rw-r--r--  2.0 unx     1022 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky/text_encoder.py
--rw-r--r--  2.0 unx     2796 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky2_2/__init__.py
--rw-r--r--  2.0 unx    14198 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2.py
--rw-r--r--  2.0 unx    43737 b- defN 24-Mar-25 00:10 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py
--rw-r--r--  2.0 unx    14178 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet.py
--rw-r--r--  2.0 unx    17433 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet_img2img.py
--rw-r--r--  2.0 unx    18007 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_img2img.py
--rw-r--r--  2.0 unx    24850 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_inpainting.py
--rw-r--r--  2.0 unx    25434 b- defN 24-Mar-25 00:10 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior.py
--rw-r--r--  2.0 unx    24981 b- defN 24-Mar-25 00:10 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior_emb2emb.py
--rw-r--r--  2.0 unx     1461 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky3/__init__.py
--rw-r--r--  2.0 unx     3273 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky3/convert_kandinsky3_unet.py
--rw-r--r--  2.0 unx    28116 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py
--rw-r--r--  2.0 unx    31349 b- defN 24-Mar-25 00:09 diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py
--rw-r--r--  2.0 unx     1560 b- defN 24-Mar-25 00:09 diffusers/pipelines/latent_consistency_models/__init__.py
--rw-r--r--  2.0 unx    47979 b- defN 24-Mar-25 00:10 diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py
--rw-r--r--  2.0 unx    44898 b- defN 24-Mar-25 00:10 diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py
--rw-r--r--  2.0 unx     1542 b- defN 24-Mar-25 00:09 diffusers/pipelines/latent_diffusion/__init__.py
--rw-r--r--  2.0 unx    32819 b- defN 24-Mar-25 00:09 diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py
--rw-r--r--  2.0 unx     8061 b- defN 24-Mar-25 00:09 diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py
--rw-r--r--  2.0 unx     1783 b- defN 24-Mar-25 00:09 diffusers/pipelines/ledits_pp/__init__.py
--rw-r--r--  2.0 unx    74992 b- defN 24-Mar-25 00:09 diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion.py
--rw-r--r--  2.0 unx    86976 b- defN 24-Mar-25 00:10 diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion_xl.py
--rw-r--r--  2.0 unx     1579 b- defN 24-Mar-25 00:09 diffusers/pipelines/ledits_pp/pipeline_output.py
--rw-r--r--  2.0 unx     1411 b- defN 24-Mar-25 00:09 diffusers/pipelines/musicldm/__init__.py
--rw-r--r--  2.0 unx    30230 b- defN 24-Mar-25 00:09 diffusers/pipelines/musicldm/pipeline_musicldm.py
--rw-r--r--  2.0 unx     1566 b- defN 24-Mar-25 00:09 diffusers/pipelines/paint_by_example/__init__.py
--rw-r--r--  2.0 unx     2484 b- defN 24-Mar-25 00:09 diffusers/pipelines/paint_by_example/image_encoder.py
--rw-r--r--  2.0 unx    30956 b- defN 24-Mar-25 00:09 diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py
--rw-r--r--  2.0 unx     1299 b- defN 24-Mar-25 00:09 diffusers/pipelines/pia/__init__.py
--rw-r--r--  2.0 unx    49389 b- defN 24-Mar-25 00:10 diffusers/pipelines/pia/pipeline_pia.py
--rw-r--r--  2.0 unx     1308 b- defN 24-Mar-25 00:09 diffusers/pipelines/pixart_alpha/__init__.py
--rw-r--r--  2.0 unx    44691 b- defN 24-Mar-25 00:09 diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py
--rw-r--r--  2.0 unx     1443 b- defN 24-Mar-25 00:09 diffusers/pipelines/semantic_stable_diffusion/__init__.py
--rw-r--r--  2.0 unx      822 b- defN 24-Mar-25 00:09 diffusers/pipelines/semantic_stable_diffusion/pipeline_output.py
--rw-r--r--  2.0 unx    38689 b- defN 24-Mar-25 00:09 diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py
--rw-r--r--  2.0 unx     2093 b- defN 24-Mar-25 00:09 diffusers/pipelines/shap_e/__init__.py
--rw-r--r--  2.0 unx     4942 b- defN 24-Mar-25 00:09 diffusers/pipelines/shap_e/camera.py
--rw-r--r--  2.0 unx    13192 b- defN 24-Mar-25 00:09 diffusers/pipelines/shap_e/pipeline_shap_e.py
--rw-r--r--  2.0 unx    12951 b- defN 24-Mar-25 00:10 diffusers/pipelines/shap_e/pipeline_shap_e_img2img.py
--rw-r--r--  2.0 unx    39148 b- defN 24-Mar-25 00:09 diffusers/pipelines/shap_e/renderer.py
--rw-r--r--  2.0 unx     1672 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_cascade/__init__.py
--rw-r--r--  2.0 unx    24370 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py
--rw-r--r--  2.0 unx    17569 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_cascade/pipeline_stable_cascade_combined.py
--rw-r--r--  2.0 unx    31270 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_cascade/pipeline_stable_cascade_prior.py
--rw-r--r--  2.0 unx     9314 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/__init__.py
--rw-r--r--  2.0 unx     1094 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/clip_image_project_model.py
--rw-r--r--  2.0 unx    80517 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/convert_from_ckpt.py
--rw-r--r--  2.0 unx    20576 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py
--rw-r--r--  2.0 unx    22504 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py
--rw-r--r--  2.0 unx    25958 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py
--rw-r--r--  2.0 unx    24324 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py
--rw-r--r--  2.0 unx    28500 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py
--rw-r--r--  2.0 unx    29114 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py
--rw-r--r--  2.0 unx    27913 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py
--rw-r--r--  2.0 unx     1496 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_output.py
--rw-r--r--  2.0 unx    52504 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py
--rw-r--r--  2.0 unx    43284 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py
--rw-r--r--  2.0 unx    22274 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py
--rw-r--r--  2.0 unx    57066 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py
--rw-r--r--  2.0 unx    73542 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py
--rw-r--r--  2.0 unx    40591 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py
--rw-r--r--  2.0 unx    23166 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py
--rw-r--r--  2.0 unx    39543 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py
--rw-r--r--  2.0 unx    45135 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py
--rw-r--r--  2.0 unx    39866 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py
--rw-r--r--  2.0 unx     5734 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/safety_checker.py
--rw-r--r--  2.0 unx     4476 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/safety_checker_flax.py
--rw-r--r--  2.0 unx     1890 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion/stable_unclip_image_normalizer.py
--rw-r--r--  2.0 unx     1390 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_attend_and_excite/__init__.py
--rw-r--r--  2.0 unx    50905 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py
--rw-r--r--  2.0 unx     1358 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_diffedit/__init__.py
--rw-r--r--  2.0 unx    78053 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py
--rw-r--r--  2.0 unx     1568 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_gligen/__init__.py
--rw-r--r--  2.0 unx    42994 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py
--rw-r--r--  2.0 unx    51187 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py
--rw-r--r--  2.0 unx     1924 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py
--rw-r--r--  2.0 unx    33326 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py
--rw-r--r--  2.0 unx    45167 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py
--rw-r--r--  2.0 unx     1346 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_ldm3d/__init__.py
--rw-r--r--  2.0 unx    49301 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py
--rw-r--r--  2.0 unx     1358 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_panorama/__init__.py
--rw-r--r--  2.0 unx    48750 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_panorama/pipeline_stable_diffusion_panorama.py
--rw-r--r--  2.0 unx     2751 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_safe/__init__.py
--rw-r--r--  2.0 unx     1459 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_safe/pipeline_output.py
--rw-r--r--  2.0 unx    39088 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py
--rw-r--r--  2.0 unx     5049 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_safe/safety_checker.py
--rw-r--r--  2.0 unx     1338 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_sag/__init__.py
--rw-r--r--  2.0 unx    44638 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py
--rw-r--r--  2.0 unx     3022 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_xl/__init__.py
--rw-r--r--  2.0 unx    11243 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_xl/pipeline_flax_stable_diffusion_xl.py
--rw-r--r--  2.0 unx     1037 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_xl/pipeline_output.py
--rw-r--r--  2.0 unx    65444 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py
--rw-r--r--  2.0 unx    75081 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py
--rw-r--r--  2.0 unx    93138 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py
--rw-r--r--  2.0 unx    51914 b- defN 24-Mar-25 00:10 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py
--rw-r--r--  2.0 unx     1242 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_diffusion_xl/watermark.py
--rw-r--r--  2.0 unx     1551 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_video_diffusion/__init__.py
--rw-r--r--  2.0 unx    29461 b- defN 24-Mar-25 00:09 diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py
--rw-r--r--  2.0 unx     1556 b- defN 24-Mar-25 00:09 diffusers/pipelines/t2i_adapter/__init__.py
--rw-r--r--  2.0 unx    45842 b- defN 24-Mar-25 00:10 diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py
--rw-r--r--  2.0 unx    67294 b- defN 24-Mar-25 00:10 diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py
--rw-r--r--  2.0 unx     1979 b- defN 24-Mar-25 00:09 diffusers/pipelines/text_to_video_synthesis/__init__.py
--rw-r--r--  2.0 unx      723 b- defN 24-Mar-25 00:09 diffusers/pipelines/text_to_video_synthesis/pipeline_output.py
--rw-r--r--  2.0 unx    32191 b- defN 24-Mar-25 00:09 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py
--rw-r--r--  2.0 unx    37044 b- defN 24-Mar-25 00:09 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py
--rw-r--r--  2.0 unx    45014 b- defN 24-Mar-25 00:10 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py
--rw-r--r--  2.0 unx    63688 b- defN 24-Mar-25 00:09 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py
--rw-r--r--  2.0 unx     1752 b- defN 24-Mar-25 00:09 diffusers/pipelines/unclip/__init__.py
--rw-r--r--  2.0 unx    22170 b- defN 24-Mar-25 00:09 diffusers/pipelines/unclip/pipeline_unclip.py
--rw-r--r--  2.0 unx    19072 b- defN 24-Mar-25 00:09 diffusers/pipelines/unclip/pipeline_unclip_image_variation.py
--rw-r--r--  2.0 unx     4278 b- defN 24-Mar-25 00:09 diffusers/pipelines/unclip/text_proj.py
--rw-r--r--  2.0 unx     1814 b- defN 24-Mar-25 00:09 diffusers/pipelines/unidiffuser/__init__.py
--rw-r--r--  2.0 unx    14113 b- defN 24-Mar-25 00:09 diffusers/pipelines/unidiffuser/modeling_text_decoder.py
--rw-r--r--  2.0 unx    54328 b- defN 24-Mar-25 00:09 diffusers/pipelines/unidiffuser/modeling_uvit.py
--rw-r--r--  2.0 unx    68508 b- defN 24-Mar-25 00:09 diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py
--rw-r--r--  2.0 unx     2100 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/__init__.py
--rw-r--r--  2.0 unx     6955 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/modeling_paella_vq_model.py
--rw-r--r--  2.0 unx     2839 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/modeling_wuerstchen_common.py
--rw-r--r--  2.0 unx    10423 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/modeling_wuerstchen_diffnext.py
--rw-r--r--  2.0 unx     8553 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/modeling_wuerstchen_prior.py
--rw-r--r--  2.0 unx    20583 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py
--rw-r--r--  2.0 unx    16421 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py
--rw-r--r--  2.0 unx    23833 b- defN 24-Mar-25 00:09 diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py
--rw-r--r--  2.0 unx    10029 b- defN 24-Mar-25 00:09 diffusers/schedulers/__init__.py
--rw-r--r--  2.0 unx     6615 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_amused.py
--rw-r--r--  2.0 unx     6871 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_consistency_decoder.py
--rw-r--r--  2.0 unx    18912 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_consistency_models.py
--rw-r--r--  2.0 unx    24954 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_ddim.py
--rw-r--r--  2.0 unx    13110 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_ddim_flax.py
--rw-r--r--  2.0 unx    17847 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_ddim_inverse.py
--rw-r--r--  2.0 unx    31602 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_ddim_parallel.py
--rw-r--r--  2.0 unx    26229 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_ddpm.py
--rw-r--r--  2.0 unx    12463 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_ddpm_flax.py
--rw-r--r--  2.0 unx    31248 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_ddpm_parallel.py
--rw-r--r--  2.0 unx     8999 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_ddpm_wuerstchen.py
--rw-r--r--  2.0 unx    34972 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_deis_multistep.py
--rw-r--r--  2.0 unx    47985 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_dpmsolver_multistep.py
--rw-r--r--  2.0 unx    28731 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py
--rw-r--r--  2.0 unx    43521 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py
--rw-r--r--  2.0 unx    24328 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_dpmsolver_sde.py
--rw-r--r--  2.0 unx    44972 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_dpmsolver_singlestep.py
--rw-r--r--  2.0 unx    30541 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_edm_dpmsolver_multistep.py
--rw-r--r--  2.0 unx    16046 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_edm_euler.py
--rw-r--r--  2.0 unx    21228 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_euler_ancestral_discrete.py
--rw-r--r--  2.0 unx    25379 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_euler_discrete.py
--rw-r--r--  2.0 unx    10801 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_euler_discrete_flax.py
--rw-r--r--  2.0 unx    21085 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_heun_discrete.py
--rw-r--r--  2.0 unx     8782 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_ipndm.py
--rw-r--r--  2.0 unx    22550 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py
--rw-r--r--  2.0 unx    21238 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_k_dpm_2_discrete.py
--rw-r--r--  2.0 unx     9636 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_karras_ve_flax.py
--rw-r--r--  2.0 unx    32342 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_lcm.py
--rw-r--r--  2.0 unx    20737 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_lms_discrete.py
--rw-r--r--  2.0 unx    11077 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_lms_discrete_flax.py
--rw-r--r--  2.0 unx    21814 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_pndm.py
--rw-r--r--  2.0 unx    21539 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_pndm_flax.py
--rw-r--r--  2.0 unx    15342 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_repaint.py
--rw-r--r--  2.0 unx    50357 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_sasolver.py
--rw-r--r--  2.0 unx    13421 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_sde_ve.py
--rw-r--r--  2.0 unx    12134 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_sde_ve_flax.py
--rw-r--r--  2.0 unx    34398 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_tcd.py
--rw-r--r--  2.0 unx    15046 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_unclip.py
--rw-r--r--  2.0 unx    37534 b- defN 24-Mar-25 00:10 diffusers/schedulers/scheduling_unipc_multistep.py
--rw-r--r--  2.0 unx     8407 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_utils.py
--rw-r--r--  2.0 unx    12368 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_utils_flax.py
--rw-r--r--  2.0 unx    23009 b- defN 24-Mar-25 00:09 diffusers/schedulers/scheduling_vq_diffusion.py
--rw-r--r--  2.0 unx     1348 b- defN 24-Mar-25 00:09 diffusers/schedulers/deprecated/__init__.py
--rw-r--r--  2.0 unx     9842 b- defN 24-Mar-25 00:09 diffusers/schedulers/deprecated/scheduling_karras_ve.py
--rw-r--r--  2.0 unx     4294 b- defN 24-Mar-25 00:09 diffusers/schedulers/deprecated/scheduling_sde_vp.py
--rw-r--r--  2.0 unx     3693 b- defN 24-Mar-25 00:09 diffusers/utils/__init__.py
--rw-r--r--  2.0 unx     1839 b- defN 24-Mar-25 00:09 diffusers/utils/accelerate_utils.py
--rw-r--r--  2.0 unx     2442 b- defN 24-Mar-25 00:09 diffusers/utils/constants.py
--rw-r--r--  2.0 unx     2101 b- defN 24-Mar-25 00:09 diffusers/utils/deprecation_utils.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Mar-25 00:10 diffusers/utils/doc_utils.py
--rw-r--r--  2.0 unx     2358 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_flax_and_transformers_objects.py
--rw-r--r--  2.0 unx     5316 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_flax_objects.py
--rw-r--r--  2.0 unx      506 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_note_seq_objects.py
--rw-r--r--  2.0 unx      493 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_onnx_objects.py
--rw-r--r--  2.0 unx    29293 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_pt_objects.py
--rw-r--r--  2.0 unx      948 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_torch_and_librosa_objects.py
--rw-r--r--  2.0 unx      537 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_torch_and_scipy_objects.py
--rw-r--r--  2.0 unx      550 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_torch_and_torchsde_objects.py
--rw-r--r--  2.0 unx     1151 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py
--rw-r--r--  2.0 unx     3023 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py
--rw-r--r--  2.0 unx    47757 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_torch_and_transformers_objects.py
--rw-r--r--  2.0 unx      621 b- defN 24-Mar-25 00:09 diffusers/utils/dummy_transformers_and_torch_and_note_seq_objects.py
--rw-r--r--  2.0 unx    19130 b- defN 24-Mar-25 00:10 diffusers/utils/dynamic_modules_utils.py
--rw-r--r--  2.0 unx     4425 b- defN 24-Mar-25 00:09 diffusers/utils/export_utils.py
--rw-r--r--  2.0 unx    21214 b- defN 24-Mar-25 00:09 diffusers/utils/hub_utils.py
--rw-r--r--  2.0 unx    26041 b- defN 24-Mar-25 00:09 diffusers/utils/import_utils.py
--rw-r--r--  2.0 unx     1546 b- defN 24-Mar-25 00:09 diffusers/utils/loading_utils.py
--rw-r--r--  2.0 unx     9396 b- defN 24-Mar-25 00:10 diffusers/utils/logging.py
--rw-r--r--  2.0 unx      550 b- defN 24-Mar-25 00:09 diffusers/utils/model_card_template.md
--rw-r--r--  2.0 unx     4953 b- defN 24-Mar-25 00:09 diffusers/utils/outputs.py
--rw-r--r--  2.0 unx    10180 b- defN 24-Mar-25 00:10 diffusers/utils/peft_utils.py
--rw-r--r--  2.0 unx     1979 b- defN 24-Mar-25 00:09 diffusers/utils/pil_utils.py
--rw-r--r--  2.0 unx    12954 b- defN 24-Mar-25 00:10 diffusers/utils/state_dict_utils.py
--rw-r--r--  2.0 unx    34708 b- defN 24-Mar-25 00:09 diffusers/utils/testing_utils.py
--rw-r--r--  2.0 unx     6233 b- defN 24-Mar-25 00:10 diffusers/utils/torch_utils.py
--rw-r--r--  2.0 unx     4333 b- defN 24-Mar-25 00:09 diffusers/utils/versions.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Mar-25 00:20 souJpg_diffusers-0.27.2.dist-info/LICENSE
--rw-r--r--  2.0 unx    19096 b- defN 24-Mar-25 00:20 souJpg_diffusers-0.27.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-25 00:20 souJpg_diffusers-0.27.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       72 b- defN 24-Mar-25 00:20 souJpg_diffusers-0.27.2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       10 b- defN 24-Mar-25 00:20 souJpg_diffusers-0.27.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    43624 b- defN 24-Mar-25 00:20 souJpg_diffusers-0.27.2.dist-info/RECORD
-399 files, 8414160 bytes uncompressed, 1953960 bytes compressed:  76.8%
+Zip file size: 2026546 bytes, number of entries: 399
+-rw-r--r--  2.0 unx    28677 b- defN 24-Mar-19 14:19 diffusers/__init__.py
+-rw-r--r--  2.0 unx    31927 b- defN 24-Mar-15 22:39 diffusers/configuration_utils.py
+-rw-r--r--  2.0 unx     1271 b- defN 24-Mar-15 22:39 diffusers/dependency_versions_check.py
+-rw-r--r--  2.0 unx     1485 b- defN 24-Jan-23 23:01 diffusers/dependency_versions_table.py
+-rw-r--r--  2.0 unx    40698 b- defN 24-Mar-15 22:39 diffusers/image_processor.py
+-rw-r--r--  2.0 unx    14743 b- defN 24-Mar-15 22:39 diffusers/optimization.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Jan-23 23:01 diffusers/py.typed
+-rw-r--r--  2.0 unx    18932 b- defN 24-Mar-15 22:42 diffusers/training_utils.py
+-rw-r--r--  2.0 unx      920 b- defN 24-Mar-15 22:39 diffusers/commands/__init__.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-Mar-15 22:39 diffusers/commands/diffusers_cli.py
+-rw-r--r--  2.0 unx     2870 b- defN 24-Mar-15 22:39 diffusers/commands/env.py
+-rw-r--r--  2.0 unx     5423 b- defN 24-Mar-15 22:39 diffusers/commands/fp16_safetensors.py
+-rw-r--r--  2.0 unx       38 b- defN 24-Jan-23 23:01 diffusers/experimental/__init__.py
+-rw-r--r--  2.0 unx       57 b- defN 24-Jan-23 23:01 diffusers/experimental/rl/__init__.py
+-rw-r--r--  2.0 unx     6033 b- defN 24-Mar-15 22:39 diffusers/experimental/rl/value_guided_sampling.py
+-rw-r--r--  2.0 unx     3705 b- defN 24-Jan-23 23:01 diffusers/loaders/__init__.py
+-rw-r--r--  2.0 unx     7664 b- defN 24-Mar-15 22:39 diffusers/loaders/autoencoder.py
+-rw-r--r--  2.0 unx     7107 b- defN 24-Mar-15 22:39 diffusers/loaders/controlnet.py
+-rw-r--r--  2.0 unx    14330 b- defN 24-Mar-15 22:42 diffusers/loaders/ip_adapter.py
+-rw-r--r--  2.0 unx    62835 b- defN 24-Mar-15 22:42 diffusers/loaders/lora.py
+-rw-r--r--  2.0 unx    12528 b- defN 24-Mar-19 14:19 diffusers/loaders/lora_conversion_utils.py
+-rw-r--r--  2.0 unx     8385 b- defN 24-Mar-15 22:39 diffusers/loaders/peft.py
+-rw-r--r--  2.0 unx    13517 b- defN 24-Mar-15 22:42 diffusers/loaders/single_file.py
+-rw-r--r--  2.0 unx    67296 b- defN 24-Mar-15 22:42 diffusers/loaders/single_file_utils.py
+-rw-r--r--  2.0 unx    26563 b- defN 24-Mar-15 22:39 diffusers/loaders/textual_inversion.py
+-rw-r--r--  2.0 unx    46429 b- defN 24-Mar-15 22:42 diffusers/loaders/unet.py
+-rw-r--r--  2.0 unx     2423 b- defN 24-Mar-15 22:39 diffusers/loaders/utils.py
+-rw-r--r--  2.0 unx     4307 b- defN 24-Mar-15 22:42 diffusers/models/__init__.py
+-rw-r--r--  2.0 unx     4490 b- defN 24-Mar-15 22:42 diffusers/models/activations.py
+-rw-r--r--  2.0 unx    24723 b- defN 24-Jan-23 23:01 diffusers/models/adapter.py
+-rw-r--r--  2.0 unx    27961 b- defN 24-Mar-15 22:42 diffusers/models/attention.py
+-rw-r--r--  2.0 unx    20250 b- defN 24-Mar-15 22:39 diffusers/models/attention_flax.py
+-rw-r--r--  2.0 unx   108051 b- defN 24-Mar-19 14:19 diffusers/models/attention_processor.py
+-rw-r--r--  2.0 unx    43567 b- defN 24-May-25 23:30 diffusers/models/controlnet.py
+-rw-r--r--  2.0 unx    16710 b- defN 24-Mar-15 22:39 diffusers/models/controlnet_flax.py
+-rw-r--r--  2.0 unx    12490 b- defN 24-Mar-15 22:42 diffusers/models/downsampling.py
+-rw-r--r--  2.0 unx     1105 b- defN 24-Mar-15 22:39 diffusers/models/dual_transformer_2d.py
+-rw-r--r--  2.0 unx    34635 b- defN 24-Mar-15 22:42 diffusers/models/embeddings.py
+-rw-r--r--  2.0 unx     3445 b- defN 24-Mar-15 22:39 diffusers/models/embeddings_flax.py
+-rw-r--r--  2.0 unx    18829 b- defN 24-Mar-15 22:42 diffusers/models/lora.py
+-rw-r--r--  2.0 unx     5332 b- defN 24-Mar-15 22:39 diffusers/models/modeling_flax_pytorch_utils.py
+-rw-r--r--  2.0 unx    27368 b- defN 24-Mar-15 22:39 diffusers/models/modeling_flax_utils.py
+-rw-r--r--  2.0 unx      512 b- defN 24-Jan-23 23:01 diffusers/models/modeling_outputs.py
+-rw-r--r--  2.0 unx     6974 b- defN 24-Mar-15 22:39 diffusers/models/modeling_pytorch_flax_utils.py
+-rw-r--r--  2.0 unx    48262 b- defN 24-Mar-15 22:42 diffusers/models/modeling_utils.py
+-rw-r--r--  2.0 unx     9508 b- defN 24-Mar-15 22:39 diffusers/models/normalization.py
+-rw-r--r--  2.0 unx      877 b- defN 24-Mar-15 22:39 diffusers/models/prior_transformer.py
+-rw-r--r--  2.0 unx    32398 b- defN 24-Mar-15 22:42 diffusers/models/resnet.py
+-rw-r--r--  2.0 unx     4021 b- defN 24-Mar-15 22:39 diffusers/models/resnet_flax.py
+-rw-r--r--  2.0 unx     4196 b- defN 24-Mar-15 22:39 diffusers/models/t5_film_transformer.py
+-rw-r--r--  2.0 unx     1492 b- defN 24-Mar-15 22:39 diffusers/models/transformer_2d.py
+-rw-r--r--  2.0 unx     2085 b- defN 24-Mar-19 14:19 diffusers/models/transformer_temporal.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-Mar-15 22:39 diffusers/models/unet_1d.py
+-rw-r--r--  2.0 unx     9963 b- defN 24-Mar-15 22:39 diffusers/models/unet_1d_blocks.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-Mar-15 22:39 diffusers/models/unet_2d.py
+-rw-r--r--  2.0 unx    18582 b- defN 24-Mar-15 22:39 diffusers/models/unet_2d_blocks.py
+-rw-r--r--  2.0 unx     1480 b- defN 24-Mar-15 22:39 diffusers/models/unet_2d_condition.py
+-rw-r--r--  2.0 unx    16731 b- defN 24-Mar-15 22:42 diffusers/models/upsampling.py
+-rw-r--r--  2.0 unx    31942 b- defN 24-Mar-15 22:39 diffusers/models/vae_flax.py
+-rw-r--r--  2.0 unx     7612 b- defN 24-Mar-15 22:39 diffusers/models/vq_model.py
+-rw-r--r--  2.0 unx      278 b- defN 24-Jan-23 23:01 diffusers/models/autoencoders/__init__.py
+-rw-r--r--  2.0 unx     7798 b- defN 24-Mar-15 22:39 diffusers/models/autoencoders/autoencoder_asym_kl.py
+-rw-r--r--  2.0 unx    21391 b- defN 24-Mar-15 22:39 diffusers/models/autoencoders/autoencoder_kl.py
+-rw-r--r--  2.0 unx    16272 b- defN 24-Mar-15 22:39 diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py
+-rw-r--r--  2.0 unx    15991 b- defN 24-Mar-15 22:39 diffusers/models/autoencoders/autoencoder_tiny.py
+-rw-r--r--  2.0 unx    18655 b- defN 24-Mar-15 22:39 diffusers/models/autoencoders/consistency_decoder_vae.py
+-rw-r--r--  2.0 unx    36674 b- defN 24-Mar-15 22:39 diffusers/models/autoencoders/vae.py
+-rw-r--r--  2.0 unx      344 b- defN 24-Mar-15 22:39 diffusers/models/transformers/__init__.py
+-rw-r--r--  2.0 unx     7665 b- defN 24-Mar-15 22:39 diffusers/models/transformers/dual_transformer_2d.py
+-rw-r--r--  2.0 unx    17348 b- defN 24-Mar-15 22:39 diffusers/models/transformers/prior_transformer.py
+-rw-r--r--  2.0 unx    16163 b- defN 24-Mar-15 22:39 diffusers/models/transformers/t5_film_transformer.py
+-rw-r--r--  2.0 unx    23855 b- defN 24-Mar-19 14:19 diffusers/models/transformers/transformer_2d.py
+-rw-r--r--  2.0 unx    16821 b- defN 24-Mar-15 22:39 diffusers/models/transformers/transformer_temporal.py
+-rw-r--r--  2.0 unx      695 b- defN 24-Mar-15 22:42 diffusers/models/unets/__init__.py
+-rw-r--r--  2.0 unx    10794 b- defN 24-Mar-15 22:39 diffusers/models/unets/unet_1d.py
+-rw-r--r--  2.0 unx    27093 b- defN 24-Mar-15 22:39 diffusers/models/unets/unet_1d_blocks.py
+-rw-r--r--  2.0 unx    16581 b- defN 24-Mar-15 22:39 diffusers/models/unets/unet_2d.py
+-rw-r--r--  2.0 unx   147882 b- defN 24-Mar-15 22:42 diffusers/models/unets/unet_2d_blocks.py
+-rw-r--r--  2.0 unx    15572 b- defN 24-Mar-15 22:39 diffusers/models/unets/unet_2d_blocks_flax.py
+-rw-r--r--  2.0 unx    67042 b- defN 24-Mar-19 14:19 diffusers/models/unets/unet_2d_condition.py
+-rw-r--r--  2.0 unx    22269 b- defN 24-Mar-15 22:42 diffusers/models/unets/unet_2d_condition_flax.py
+-rw-r--r--  2.0 unx    89508 b- defN 24-Mar-15 22:42 diffusers/models/unets/unet_3d_blocks.py
+-rw-r--r--  2.0 unx    34691 b- defN 24-Mar-15 22:39 diffusers/models/unets/unet_3d_condition.py
+-rw-r--r--  2.0 unx    32728 b- defN 24-Mar-15 22:42 diffusers/models/unets/unet_i2vgen_xl.py
+-rw-r--r--  2.0 unx    20758 b- defN 24-Mar-15 22:39 diffusers/models/unets/unet_kandinsky3.py
+-rw-r--r--  2.0 unx    42225 b- defN 24-Mar-15 22:39 diffusers/models/unets/unet_motion_model.py
+-rw-r--r--  2.0 unx    22104 b- defN 24-Mar-15 22:42 diffusers/models/unets/unet_spatio_temporal_condition.py
+-rw-r--r--  2.0 unx    28462 b- defN 24-Mar-19 14:19 diffusers/models/unets/unet_stable_cascade.py
+-rw-r--r--  2.0 unx    17338 b- defN 24-Mar-15 22:39 diffusers/models/unets/uvit_2d.py
+-rw-r--r--  2.0 unx    22315 b- defN 24-Mar-15 22:42 diffusers/pipelines/__init__.py
+-rw-r--r--  2.0 unx    50459 b- defN 24-Mar-19 14:19 diffusers/pipelines/auto_pipeline.py
+-rw-r--r--  2.0 unx     7564 b- defN 24-Mar-15 22:39 diffusers/pipelines/free_init_utils.py
+-rw-r--r--  2.0 unx     8329 b- defN 24-Mar-15 22:39 diffusers/pipelines/onnx_utils.py
+-rw-r--r--  2.0 unx    27479 b- defN 24-Mar-15 22:39 diffusers/pipelines/pipeline_flax_utils.py
+-rw-r--r--  2.0 unx    20613 b- defN 24-Mar-15 22:42 diffusers/pipelines/pipeline_loading_utils.py
+-rw-r--r--  2.0 unx    87939 b- defN 24-Mar-15 22:42 diffusers/pipelines/pipeline_utils.py
+-rw-r--r--  2.0 unx     1793 b- defN 24-Jan-23 23:01 diffusers/pipelines/amused/__init__.py
+-rw-r--r--  2.0 unx    15652 b- defN 24-Mar-15 22:39 diffusers/pipelines/amused/pipeline_amused.py
+-rw-r--r--  2.0 unx    17110 b- defN 24-Mar-15 22:42 diffusers/pipelines/amused/pipeline_amused_img2img.py
+-rw-r--r--  2.0 unx    18806 b- defN 24-Mar-15 22:39 diffusers/pipelines/amused/pipeline_amused_inpaint.py
+-rw-r--r--  2.0 unx     1584 b- defN 24-Mar-15 22:39 diffusers/pipelines/animatediff/__init__.py
+-rw-r--r--  2.0 unx    41491 b- defN 24-Mar-15 22:42 diffusers/pipelines/animatediff/pipeline_animatediff.py
+-rw-r--r--  2.0 unx    48805 b- defN 24-Mar-15 22:42 diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py
+-rw-r--r--  2.0 unx      717 b- defN 24-Mar-15 22:39 diffusers/pipelines/animatediff/pipeline_output.py
+-rw-r--r--  2.0 unx     1419 b- defN 24-Jan-23 23:01 diffusers/pipelines/audioldm/__init__.py
+-rw-r--r--  2.0 unx    26054 b- defN 24-Mar-15 22:39 diffusers/pipelines/audioldm/pipeline_audioldm.py
+-rw-r--r--  2.0 unx     1605 b- defN 24-Jan-23 23:01 diffusers/pipelines/audioldm2/__init__.py
+-rw-r--r--  2.0 unx    72277 b- defN 24-Mar-15 22:39 diffusers/pipelines/audioldm2/modeling_audioldm2.py
+-rw-r--r--  2.0 unx    49174 b- defN 24-Mar-15 22:39 diffusers/pipelines/audioldm2/pipeline_audioldm2.py
+-rw-r--r--  2.0 unx      697 b- defN 24-Jan-23 23:01 diffusers/pipelines/blip_diffusion/__init__.py
+-rw-r--r--  2.0 unx    16747 b- defN 24-Mar-15 22:39 diffusers/pipelines/blip_diffusion/blip_image_processing.py
+-rw-r--r--  2.0 unx    27308 b- defN 24-Mar-15 22:39 diffusers/pipelines/blip_diffusion/modeling_blip2.py
+-rw-r--r--  2.0 unx     9007 b- defN 24-Mar-15 22:39 diffusers/pipelines/blip_diffusion/modeling_ctx_clip.py
+-rw-r--r--  2.0 unx    15011 b- defN 24-Mar-15 22:39 diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py
+-rw-r--r--  2.0 unx      484 b- defN 24-Jan-23 23:01 diffusers/pipelines/consistency_models/__init__.py
+-rw-r--r--  2.0 unx    12393 b- defN 24-Mar-15 22:39 diffusers/pipelines/consistency_models/pipeline_consistency_models.py
+-rw-r--r--  2.0 unx     3483 b- defN 24-Jan-23 23:01 diffusers/pipelines/controlnet/__init__.py
+-rw-r--r--  2.0 unx     9510 b- defN 24-Jan-23 23:01 diffusers/pipelines/controlnet/multicontrolnet.py
+-rw-r--r--  2.0 unx    67300 b- defN 24-Mar-15 22:42 diffusers/pipelines/controlnet/pipeline_controlnet.py
+-rw-r--r--  2.0 unx    17373 b- defN 24-Mar-15 22:39 diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py
+-rw-r--r--  2.0 unx    66454 b- defN 24-Mar-15 22:39 diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py
+-rw-r--r--  2.0 unx    81387 b- defN 24-Mar-19 14:19 diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py
+-rw-r--r--  2.0 unx    93309 b- defN 24-Mar-19 14:19 diffusers/pipelines/controlnet/pipeline_controlnet_inpaint_sd_xl.py
+-rw-r--r--  2.0 unx    77568 b- defN 24-Mar-15 22:42 diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py
+-rw-r--r--  2.0 unx    84855 b- defN 24-Mar-15 22:42 diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py
+-rw-r--r--  2.0 unx    22663 b- defN 24-Mar-15 22:42 diffusers/pipelines/controlnet/pipeline_flax_controlnet.py
+-rw-r--r--  2.0 unx      453 b- defN 24-Jan-23 23:01 diffusers/pipelines/dance_diffusion/__init__.py
+-rw-r--r--  2.0 unx     6322 b- defN 24-Mar-15 22:39 diffusers/pipelines/dance_diffusion/pipeline_dance_diffusion.py
+-rw-r--r--  2.0 unx      411 b- defN 24-Jan-23 23:01 diffusers/pipelines/ddim/__init__.py
+-rw-r--r--  2.0 unx     6603 b- defN 24-Mar-15 22:39 diffusers/pipelines/ddim/pipeline_ddim.py
+-rw-r--r--  2.0 unx      425 b- defN 24-Jan-23 23:01 diffusers/pipelines/ddpm/__init__.py
+-rw-r--r--  2.0 unx     4959 b- defN 24-Mar-15 22:39 diffusers/pipelines/ddpm/pipeline_ddpm.py
+-rw-r--r--  2.0 unx     2975 b- defN 24-Jan-23 23:01 diffusers/pipelines/deepfloyd_if/__init__.py
+-rw-r--r--  2.0 unx    35933 b- defN 24-Mar-15 22:42 diffusers/pipelines/deepfloyd_if/pipeline_if.py
+-rw-r--r--  2.0 unx    40257 b- defN 24-Mar-19 14:19 diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py
+-rw-r--r--  2.0 unx    45516 b- defN 24-Mar-19 14:19 diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py
+-rw-r--r--  2.0 unx    45563 b- defN 24-Mar-19 14:19 diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py
+-rw-r--r--  2.0 unx    50379 b- defN 24-Mar-19 14:19 diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py
+-rw-r--r--  2.0 unx    40292 b- defN 24-Mar-15 22:42 diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py
+-rw-r--r--  2.0 unx     1140 b- defN 24-Jan-23 23:01 diffusers/pipelines/deepfloyd_if/pipeline_output.py
+-rw-r--r--  2.0 unx     2117 b- defN 24-Jan-23 23:01 diffusers/pipelines/deepfloyd_if/safety_checker.py
+-rw-r--r--  2.0 unx     5164 b- defN 24-Jan-23 23:01 diffusers/pipelines/deepfloyd_if/timesteps.py
+-rw-r--r--  2.0 unx     1601 b- defN 24-Jan-23 23:01 diffusers/pipelines/deepfloyd_if/watermark.py
+-rw-r--r--  2.0 unx     5470 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/__init__.py
+-rw-r--r--  2.0 unx     1783 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/alt_diffusion/__init__.py
+-rw-r--r--  2.0 unx     5580 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/alt_diffusion/modeling_roberta_series.py
+-rw-r--r--  2.0 unx    48491 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/alt_diffusion/pipeline_alt_diffusion.py
+-rw-r--r--  2.0 unx    51792 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/alt_diffusion/pipeline_alt_diffusion_img2img.py
+-rw-r--r--  2.0 unx      928 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/alt_diffusion/pipeline_output.py
+-rw-r--r--  2.0 unx      507 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/audio_diffusion/__init__.py
+-rw-r--r--  2.0 unx     5764 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/audio_diffusion/mel.py
+-rw-r--r--  2.0 unx    13231 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/audio_diffusion/pipeline_audio_diffusion.py
+-rw-r--r--  2.0 unx      448 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/latent_diffusion_uncond/__init__.py
+-rw-r--r--  2.0 unx     5381 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/latent_diffusion_uncond/pipeline_latent_diffusion_uncond.py
+-rw-r--r--  2.0 unx      412 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/pndm/__init__.py
+-rw-r--r--  2.0 unx     4658 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/pndm/pipeline_pndm.py
+-rw-r--r--  2.0 unx      425 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/repaint/__init__.py
+-rw-r--r--  2.0 unx    10049 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/repaint/pipeline_repaint.py
+-rw-r--r--  2.0 unx      441 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/score_sde_ve/__init__.py
+-rw-r--r--  2.0 unx     4390 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/score_sde_ve/pipeline_score_sde_ve.py
+-rw-r--r--  2.0 unx     2588 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/spectrogram_diffusion/__init__.py
+-rw-r--r--  2.0 unx     3100 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/spectrogram_diffusion/continuous_encoder.py
+-rw-r--r--  2.0 unx    25096 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/spectrogram_diffusion/midi_utils.py
+-rw-r--r--  2.0 unx     2923 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/spectrogram_diffusion/notes_encoder.py
+-rw-r--r--  2.0 unx    11538 b- defN 24-Mar-15 22:42 diffusers/pipelines/deprecated/spectrogram_diffusion/pipeline_spectrogram_diffusion.py
+-rw-r--r--  2.0 unx     2111 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/stable_diffusion_variants/__init__.py
+-rw-r--r--  2.0 unx    47812 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_cycle_diffusion.py
+-rw-r--r--  2.0 unx    27814 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_onnx_stable_diffusion_inpaint_legacy.py
+-rw-r--r--  2.0 unx    42346 b- defN 24-Mar-19 14:19 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_inpaint_legacy.py
+-rw-r--r--  2.0 unx    41304 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_model_editing.py
+-rw-r--r--  2.0 unx    40999 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_paradigms.py
+-rw-r--r--  2.0 unx    63419 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_pix2pix_zero.py
+-rw-r--r--  2.0 unx      453 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/stochastic_karras_ve/__init__.py
+-rw-r--r--  2.0 unx     5277 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/stochastic_karras_ve/pipeline_stochastic_karras_ve.py
+-rw-r--r--  2.0 unx     2838 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/versatile_diffusion/__init__.py
+-rw-r--r--  2.0 unx   115479 b- defN 24-Mar-19 14:19 diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py
+-rw-r--r--  2.0 unx    21916 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion.py
+-rw-r--r--  2.0 unx    27122 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py
+-rw-r--r--  2.0 unx    19641 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py
+-rw-r--r--  2.0 unx    22840 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py
+-rw-r--r--  2.0 unx     1650 b- defN 24-Jan-23 23:01 diffusers/pipelines/deprecated/vq_diffusion/__init__.py
+-rw-r--r--  2.0 unx    15474 b- defN 24-Mar-15 22:39 diffusers/pipelines/deprecated/vq_diffusion/pipeline_vq_diffusion.py
+-rw-r--r--  2.0 unx      408 b- defN 24-Jan-23 23:01 diffusers/pipelines/dit/__init__.py
+-rw-r--r--  2.0 unx     9869 b- defN 24-Mar-15 22:39 diffusers/pipelines/dit/pipeline_dit.py
+-rw-r--r--  2.0 unx     1307 b- defN 24-Mar-15 22:39 diffusers/pipelines/i2vgen_xl/__init__.py
+-rw-r--r--  2.0 unx    37709 b- defN 24-Mar-15 22:42 diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py
+-rw-r--r--  2.0 unx     2312 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky/__init__.py
+-rw-r--r--  2.0 unx    17740 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky/pipeline_kandinsky.py
+-rw-r--r--  2.0 unx    39237 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py
+-rw-r--r--  2.0 unx    21855 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py
+-rw-r--r--  2.0 unx    28573 b- defN 24-Mar-15 22:42 diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py
+-rw-r--r--  2.0 unx    23845 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py
+-rw-r--r--  2.0 unx     1022 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky/text_encoder.py
+-rw-r--r--  2.0 unx     2796 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky2_2/__init__.py
+-rw-r--r--  2.0 unx    14198 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2.py
+-rw-r--r--  2.0 unx    43737 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py
+-rw-r--r--  2.0 unx    14178 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet.py
+-rw-r--r--  2.0 unx    17433 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet_img2img.py
+-rw-r--r--  2.0 unx    18007 b- defN 24-Mar-15 22:39 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_img2img.py
+-rw-r--r--  2.0 unx    24850 b- defN 24-Mar-15 22:42 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_inpainting.py
+-rw-r--r--  2.0 unx    25434 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior.py
+-rw-r--r--  2.0 unx    24981 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior_emb2emb.py
+-rw-r--r--  2.0 unx     1461 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky3/__init__.py
+-rw-r--r--  2.0 unx     3273 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky3/convert_kandinsky3_unet.py
+-rw-r--r--  2.0 unx    28116 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py
+-rw-r--r--  2.0 unx    31349 b- defN 24-Jan-23 23:01 diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py
+-rw-r--r--  2.0 unx     1560 b- defN 24-Jan-23 23:01 diffusers/pipelines/latent_consistency_models/__init__.py
+-rw-r--r--  2.0 unx    48151 b- defN 24-Mar-15 22:42 diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py
+-rw-r--r--  2.0 unx    45070 b- defN 24-Mar-15 22:42 diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py
+-rw-r--r--  2.0 unx     1542 b- defN 24-Jan-23 23:01 diffusers/pipelines/latent_diffusion/__init__.py
+-rw-r--r--  2.0 unx    32819 b- defN 24-Mar-15 22:39 diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py
+-rw-r--r--  2.0 unx     8061 b- defN 24-Jan-23 23:01 diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py
+-rw-r--r--  2.0 unx     1783 b- defN 24-Mar-15 22:42 diffusers/pipelines/ledits_pp/__init__.py
+-rw-r--r--  2.0 unx    74992 b- defN 24-Mar-15 22:42 diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion.py
+-rw-r--r--  2.0 unx    87148 b- defN 24-Mar-15 22:42 diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion_xl.py
+-rw-r--r--  2.0 unx     1579 b- defN 24-Mar-15 22:42 diffusers/pipelines/ledits_pp/pipeline_output.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-Jan-23 23:01 diffusers/pipelines/musicldm/__init__.py
+-rw-r--r--  2.0 unx    30230 b- defN 24-Mar-15 22:39 diffusers/pipelines/musicldm/pipeline_musicldm.py
+-rw-r--r--  2.0 unx     1566 b- defN 24-Jan-23 23:01 diffusers/pipelines/paint_by_example/__init__.py
+-rw-r--r--  2.0 unx     2484 b- defN 24-Mar-15 22:39 diffusers/pipelines/paint_by_example/image_encoder.py
+-rw-r--r--  2.0 unx    30956 b- defN 24-Mar-15 22:39 diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py
+-rw-r--r--  2.0 unx     1299 b- defN 24-Mar-15 22:39 diffusers/pipelines/pia/__init__.py
+-rw-r--r--  2.0 unx    49389 b- defN 24-Mar-15 22:42 diffusers/pipelines/pia/pipeline_pia.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-Jan-23 23:01 diffusers/pipelines/pixart_alpha/__init__.py
+-rw-r--r--  2.0 unx    44691 b- defN 24-Mar-15 22:42 diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py
+-rw-r--r--  2.0 unx     1443 b- defN 24-Jan-23 23:01 diffusers/pipelines/semantic_stable_diffusion/__init__.py
+-rw-r--r--  2.0 unx      822 b- defN 24-Jan-23 23:01 diffusers/pipelines/semantic_stable_diffusion/pipeline_output.py
+-rw-r--r--  2.0 unx    38689 b- defN 24-Mar-15 22:42 diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py
+-rw-r--r--  2.0 unx     2093 b- defN 24-Jan-23 23:01 diffusers/pipelines/shap_e/__init__.py
+-rw-r--r--  2.0 unx     4942 b- defN 24-Mar-15 22:39 diffusers/pipelines/shap_e/camera.py
+-rw-r--r--  2.0 unx    13192 b- defN 24-Mar-15 22:39 diffusers/pipelines/shap_e/pipeline_shap_e.py
+-rw-r--r--  2.0 unx    12951 b- defN 24-Mar-15 22:39 diffusers/pipelines/shap_e/pipeline_shap_e_img2img.py
+-rw-r--r--  2.0 unx    39148 b- defN 24-Mar-15 22:39 diffusers/pipelines/shap_e/renderer.py
+-rw-r--r--  2.0 unx     1672 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_cascade/__init__.py
+-rw-r--r--  2.0 unx    23653 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py
+-rw-r--r--  2.0 unx    17569 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_cascade/pipeline_stable_cascade_combined.py
+-rw-r--r--  2.0 unx    31270 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_cascade/pipeline_stable_cascade_prior.py
+-rw-r--r--  2.0 unx     9314 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion/__init__.py
+-rw-r--r--  2.0 unx     1094 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/clip_image_project_model.py
+-rw-r--r--  2.0 unx    80517 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/convert_from_ckpt.py
+-rw-r--r--  2.0 unx    20576 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py
+-rw-r--r--  2.0 unx    22504 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py
+-rw-r--r--  2.0 unx    25958 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py
+-rw-r--r--  2.0 unx    24324 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py
+-rw-r--r--  2.0 unx    28500 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py
+-rw-r--r--  2.0 unx    29114 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py
+-rw-r--r--  2.0 unx    27955 b- defN 24-Mar-19 14:19 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py
+-rw-r--r--  2.0 unx     1496 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion/pipeline_output.py
+-rw-r--r--  2.0 unx    52676 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py
+-rw-r--r--  2.0 unx    43284 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py
+-rw-r--r--  2.0 unx    22274 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py
+-rw-r--r--  2.0 unx    57238 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py
+-rw-r--r--  2.0 unx    73715 b- defN 24-Mar-19 14:19 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py
+-rw-r--r--  2.0 unx    40591 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py
+-rw-r--r--  2.0 unx    23166 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py
+-rw-r--r--  2.0 unx    39543 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py
+-rw-r--r--  2.0 unx    45135 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py
+-rw-r--r--  2.0 unx    39866 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py
+-rw-r--r--  2.0 unx     5734 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/safety_checker.py
+-rw-r--r--  2.0 unx     4476 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/safety_checker_flax.py
+-rw-r--r--  2.0 unx     1890 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion/stable_unclip_image_normalizer.py
+-rw-r--r--  2.0 unx     1390 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_attend_and_excite/__init__.py
+-rw-r--r--  2.0 unx    50905 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py
+-rw-r--r--  2.0 unx     1358 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_diffedit/__init__.py
+-rw-r--r--  2.0 unx    78053 b- defN 24-Mar-19 14:19 diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py
+-rw-r--r--  2.0 unx     1568 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_gligen/__init__.py
+-rw-r--r--  2.0 unx    43008 b- defN 24-Mar-19 14:19 diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py
+-rw-r--r--  2.0 unx    51194 b- defN 24-Mar-19 14:19 diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py
+-rw-r--r--  2.0 unx     1924 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py
+-rw-r--r--  2.0 unx    33326 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py
+-rw-r--r--  2.0 unx    45167 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py
+-rw-r--r--  2.0 unx     1346 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_ldm3d/__init__.py
+-rw-r--r--  2.0 unx    49473 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py
+-rw-r--r--  2.0 unx     1358 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_panorama/__init__.py
+-rw-r--r--  2.0 unx    48750 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion_panorama/pipeline_stable_diffusion_panorama.py
+-rw-r--r--  2.0 unx     2751 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_safe/__init__.py
+-rw-r--r--  2.0 unx     1459 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_safe/pipeline_output.py
+-rw-r--r--  2.0 unx    39088 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py
+-rw-r--r--  2.0 unx     5049 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion_safe/safety_checker.py
+-rw-r--r--  2.0 unx     1338 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_sag/__init__.py
+-rw-r--r--  2.0 unx    44636 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py
+-rw-r--r--  2.0 unx     3022 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_xl/__init__.py
+-rw-r--r--  2.0 unx    11243 b- defN 24-Mar-15 22:39 diffusers/pipelines/stable_diffusion_xl/pipeline_flax_stable_diffusion_xl.py
+-rw-r--r--  2.0 unx     1037 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_xl/pipeline_output.py
+-rw-r--r--  2.0 unx    65616 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py
+-rw-r--r--  2.0 unx    75253 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py
+-rw-r--r--  2.0 unx    93311 b- defN 24-Mar-19 14:19 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py
+-rw-r--r--  2.0 unx    51914 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py
+-rw-r--r--  2.0 unx     1242 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_diffusion_xl/watermark.py
+-rw-r--r--  2.0 unx     1551 b- defN 24-Jan-23 23:01 diffusers/pipelines/stable_video_diffusion/__init__.py
+-rw-r--r--  2.0 unx    29461 b- defN 24-Mar-15 22:42 diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py
+-rw-r--r--  2.0 unx     1556 b- defN 24-Jan-23 23:01 diffusers/pipelines/t2i_adapter/__init__.py
+-rw-r--r--  2.0 unx    46014 b- defN 24-Mar-15 22:42 diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py
+-rw-r--r--  2.0 unx    67466 b- defN 24-Mar-15 22:42 diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py
+-rw-r--r--  2.0 unx     1979 b- defN 24-Jan-23 23:01 diffusers/pipelines/text_to_video_synthesis/__init__.py
+-rw-r--r--  2.0 unx      723 b- defN 24-Mar-15 22:39 diffusers/pipelines/text_to_video_synthesis/pipeline_output.py
+-rw-r--r--  2.0 unx    32191 b- defN 24-Mar-15 22:42 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py
+-rw-r--r--  2.0 unx    37044 b- defN 24-Mar-15 22:42 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py
+-rw-r--r--  2.0 unx    45008 b- defN 24-Mar-15 22:42 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py
+-rw-r--r--  2.0 unx    63688 b- defN 24-Mar-15 22:39 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py
+-rw-r--r--  2.0 unx     1752 b- defN 24-Jan-23 23:01 diffusers/pipelines/unclip/__init__.py
+-rw-r--r--  2.0 unx    22170 b- defN 24-Mar-15 22:39 diffusers/pipelines/unclip/pipeline_unclip.py
+-rw-r--r--  2.0 unx    19072 b- defN 24-Mar-15 22:39 diffusers/pipelines/unclip/pipeline_unclip_image_variation.py
+-rw-r--r--  2.0 unx     4278 b- defN 24-Mar-15 22:39 diffusers/pipelines/unclip/text_proj.py
+-rw-r--r--  2.0 unx     1814 b- defN 24-Jan-23 23:01 diffusers/pipelines/unidiffuser/__init__.py
+-rw-r--r--  2.0 unx    14113 b- defN 24-Jan-23 23:01 diffusers/pipelines/unidiffuser/modeling_text_decoder.py
+-rw-r--r--  2.0 unx    54328 b- defN 24-Mar-15 22:39 diffusers/pipelines/unidiffuser/modeling_uvit.py
+-rw-r--r--  2.0 unx    68508 b- defN 24-Mar-15 22:39 diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py
+-rw-r--r--  2.0 unx     2100 b- defN 24-Jan-23 23:01 diffusers/pipelines/wuerstchen/__init__.py
+-rw-r--r--  2.0 unx     6955 b- defN 24-Mar-15 22:39 diffusers/pipelines/wuerstchen/modeling_paella_vq_model.py
+-rw-r--r--  2.0 unx     2839 b- defN 24-Mar-15 22:42 diffusers/pipelines/wuerstchen/modeling_wuerstchen_common.py
+-rw-r--r--  2.0 unx    10423 b- defN 24-Mar-15 22:42 diffusers/pipelines/wuerstchen/modeling_wuerstchen_diffnext.py
+-rw-r--r--  2.0 unx     8553 b- defN 24-Mar-15 22:42 diffusers/pipelines/wuerstchen/modeling_wuerstchen_prior.py
+-rw-r--r--  2.0 unx    20583 b- defN 24-Mar-15 22:42 diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py
+-rw-r--r--  2.0 unx    16421 b- defN 24-Mar-15 22:39 diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py
+-rw-r--r--  2.0 unx    23833 b- defN 24-Mar-15 22:42 diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py
+-rw-r--r--  2.0 unx    10029 b- defN 24-Mar-15 22:42 diffusers/schedulers/__init__.py
+-rw-r--r--  2.0 unx     6615 b- defN 24-Jan-23 23:01 diffusers/schedulers/scheduling_amused.py
+-rw-r--r--  2.0 unx     6872 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_consistency_decoder.py
+-rw-r--r--  2.0 unx    18920 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_consistency_models.py
+-rw-r--r--  2.0 unx    24955 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_ddim.py
+-rw-r--r--  2.0 unx    13110 b- defN 24-Mar-15 22:42 diffusers/schedulers/scheduling_ddim_flax.py
+-rw-r--r--  2.0 unx    17848 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_ddim_inverse.py
+-rw-r--r--  2.0 unx    31603 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_ddim_parallel.py
+-rw-r--r--  2.0 unx    26230 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_ddpm.py
+-rw-r--r--  2.0 unx    12463 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_ddpm_flax.py
+-rw-r--r--  2.0 unx    31249 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_ddpm_parallel.py
+-rw-r--r--  2.0 unx     9000 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_ddpm_wuerstchen.py
+-rw-r--r--  2.0 unx    34974 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_deis_multistep.py
+-rw-r--r--  2.0 unx    47987 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_dpmsolver_multistep.py
+-rw-r--r--  2.0 unx    28731 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py
+-rw-r--r--  2.0 unx    43530 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py
+-rw-r--r--  2.0 unx    24337 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_dpmsolver_sde.py
+-rw-r--r--  2.0 unx    44974 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_dpmsolver_singlestep.py
+-rw-r--r--  2.0 unx    30542 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_edm_dpmsolver_multistep.py
+-rw-r--r--  2.0 unx    16047 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_edm_euler.py
+-rw-r--r--  2.0 unx    21230 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_euler_ancestral_discrete.py
+-rw-r--r--  2.0 unx    25388 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_euler_discrete.py
+-rw-r--r--  2.0 unx    10801 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_euler_discrete_flax.py
+-rw-r--r--  2.0 unx    21087 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_heun_discrete.py
+-rw-r--r--  2.0 unx     8783 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_ipndm.py
+-rw-r--r--  2.0 unx    22552 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py
+-rw-r--r--  2.0 unx    21240 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_k_dpm_2_discrete.py
+-rw-r--r--  2.0 unx     9636 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_karras_ve_flax.py
+-rw-r--r--  2.0 unx    32343 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_lcm.py
+-rw-r--r--  2.0 unx    20746 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_lms_discrete.py
+-rw-r--r--  2.0 unx    11077 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_lms_discrete_flax.py
+-rw-r--r--  2.0 unx    21815 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_pndm.py
+-rw-r--r--  2.0 unx    21539 b- defN 24-Mar-15 22:42 diffusers/schedulers/scheduling_pndm_flax.py
+-rw-r--r--  2.0 unx    15343 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_repaint.py
+-rw-r--r--  2.0 unx    50359 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_sasolver.py
+-rw-r--r--  2.0 unx    13421 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_sde_ve.py
+-rw-r--r--  2.0 unx    12134 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_sde_ve_flax.py
+-rw-r--r--  2.0 unx    35066 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_tcd.py
+-rw-r--r--  2.0 unx    15047 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_unclip.py
+-rw-r--r--  2.0 unx    37536 b- defN 24-Mar-19 14:19 diffusers/schedulers/scheduling_unipc_multistep.py
+-rw-r--r--  2.0 unx     8407 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_utils.py
+-rw-r--r--  2.0 unx    12368 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_utils_flax.py
+-rw-r--r--  2.0 unx    23009 b- defN 24-Mar-15 22:39 diffusers/schedulers/scheduling_vq_diffusion.py
+-rw-r--r--  2.0 unx     1348 b- defN 24-Jan-23 23:01 diffusers/schedulers/deprecated/__init__.py
+-rw-r--r--  2.0 unx     9842 b- defN 24-Mar-15 22:39 diffusers/schedulers/deprecated/scheduling_karras_ve.py
+-rw-r--r--  2.0 unx     4294 b- defN 24-Mar-15 22:39 diffusers/schedulers/deprecated/scheduling_sde_vp.py
+-rw-r--r--  2.0 unx     3693 b- defN 24-Mar-15 22:42 diffusers/utils/__init__.py
+-rw-r--r--  2.0 unx     1839 b- defN 24-Mar-15 22:39 diffusers/utils/accelerate_utils.py
+-rw-r--r--  2.0 unx     2442 b- defN 24-Mar-15 22:39 diffusers/utils/constants.py
+-rw-r--r--  2.0 unx     2101 b- defN 24-Jan-23 23:01 diffusers/utils/deprecation_utils.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-Mar-15 22:39 diffusers/utils/doc_utils.py
+-rw-r--r--  2.0 unx     2358 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_flax_and_transformers_objects.py
+-rw-r--r--  2.0 unx     5316 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_flax_objects.py
+-rw-r--r--  2.0 unx      506 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_note_seq_objects.py
+-rw-r--r--  2.0 unx      493 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_onnx_objects.py
+-rw-r--r--  2.0 unx    29293 b- defN 24-Mar-15 22:42 diffusers/utils/dummy_pt_objects.py
+-rw-r--r--  2.0 unx      948 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_torch_and_librosa_objects.py
+-rw-r--r--  2.0 unx      537 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_torch_and_scipy_objects.py
+-rw-r--r--  2.0 unx      550 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_torch_and_torchsde_objects.py
+-rw-r--r--  2.0 unx     1151 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py
+-rw-r--r--  2.0 unx     3023 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py
+-rw-r--r--  2.0 unx    47757 b- defN 24-Mar-15 22:42 diffusers/utils/dummy_torch_and_transformers_objects.py
+-rw-r--r--  2.0 unx      621 b- defN 24-Jan-23 23:01 diffusers/utils/dummy_transformers_and_torch_and_note_seq_objects.py
+-rw-r--r--  2.0 unx    19130 b- defN 24-Mar-15 22:39 diffusers/utils/dynamic_modules_utils.py
+-rw-r--r--  2.0 unx     4425 b- defN 24-Mar-15 22:42 diffusers/utils/export_utils.py
+-rw-r--r--  2.0 unx    21214 b- defN 24-Mar-15 22:39 diffusers/utils/hub_utils.py
+-rw-r--r--  2.0 unx    26041 b- defN 24-Mar-15 22:42 diffusers/utils/import_utils.py
+-rw-r--r--  2.0 unx     1546 b- defN 24-Mar-15 22:39 diffusers/utils/loading_utils.py
+-rw-r--r--  2.0 unx     9396 b- defN 24-Mar-15 22:39 diffusers/utils/logging.py
+-rw-r--r--  2.0 unx      550 b- defN 24-Mar-15 22:39 diffusers/utils/model_card_template.md
+-rw-r--r--  2.0 unx     4953 b- defN 24-Mar-15 22:39 diffusers/utils/outputs.py
+-rw-r--r--  2.0 unx    10180 b- defN 24-Mar-15 22:39 diffusers/utils/peft_utils.py
+-rw-r--r--  2.0 unx     1979 b- defN 24-Jan-23 23:01 diffusers/utils/pil_utils.py
+-rw-r--r--  2.0 unx    12954 b- defN 24-Mar-15 22:39 diffusers/utils/state_dict_utils.py
+-rw-r--r--  2.0 unx    34708 b- defN 24-Mar-15 22:39 diffusers/utils/testing_utils.py
+-rw-r--r--  2.0 unx     6233 b- defN 24-Mar-15 22:39 diffusers/utils/torch_utils.py
+-rw-r--r--  2.0 unx     4333 b- defN 24-Jan-23 23:01 diffusers/utils/versions.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-May-26 00:47 souJpg_diffusers-0.28.0.dev1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    19101 b- defN 24-May-26 00:47 souJpg_diffusers-0.28.0.dev1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-26 00:47 souJpg_diffusers-0.28.0.dev1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       72 b- defN 24-May-26 00:47 souJpg_diffusers-0.28.0.dev1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       10 b- defN 24-May-26 00:47 souJpg_diffusers-0.28.0.dev1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    43654 b- defN 24-May-26 00:47 souJpg_diffusers-0.28.0.dev1.dist-info/RECORD
+399 files, 8415694 bytes uncompressed, 1954648 bytes compressed:  76.8%
```

## zipnote {}

```diff
@@ -1173,26 +1173,26 @@
 
 Filename: diffusers/utils/torch_utils.py
 Comment: 
 
 Filename: diffusers/utils/versions.py
 Comment: 
 
-Filename: souJpg_diffusers-0.27.2.dist-info/LICENSE
+Filename: souJpg_diffusers-0.28.0.dev1.dist-info/LICENSE
 Comment: 
 
-Filename: souJpg_diffusers-0.27.2.dist-info/METADATA
+Filename: souJpg_diffusers-0.28.0.dev1.dist-info/METADATA
 Comment: 
 
-Filename: souJpg_diffusers-0.27.2.dist-info/WHEEL
+Filename: souJpg_diffusers-0.28.0.dev1.dist-info/WHEEL
 Comment: 
 
-Filename: souJpg_diffusers-0.27.2.dist-info/entry_points.txt
+Filename: souJpg_diffusers-0.28.0.dev1.dist-info/entry_points.txt
 Comment: 
 
-Filename: souJpg_diffusers-0.27.2.dist-info/top_level.txt
+Filename: souJpg_diffusers-0.28.0.dev1.dist-info/top_level.txt
 Comment: 
 
-Filename: souJpg_diffusers-0.27.2.dist-info/RECORD
+Filename: souJpg_diffusers-0.28.0.dev1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## diffusers/__init__.py

```diff
@@ -1,8 +1,8 @@
-__version__ = "0.27.2"
+__version__ = "0.28.0.dev0"
 
 from typing import TYPE_CHECKING
 
 from .utils import (
     DIFFUSERS_SLOW_IMPORT,
     OptionalDependencyNotAvailable,
     _LazyModule,
```

## diffusers/loaders/lora_conversion_utils.py

```diff
@@ -194,69 +194,44 @@
             elif any(key in diffusers_name for key in ("proj_in", "proj_out")):
                 unet_state_dict[diffusers_name] = state_dict.pop(key)
                 unet_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
             else:
                 unet_state_dict[diffusers_name] = state_dict.pop(key)
                 unet_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
 
-        elif lora_name.startswith("lora_te_"):
-            diffusers_name = key.replace("lora_te_", "").replace("_", ".")
-            diffusers_name = diffusers_name.replace("text.model", "text_model")
-            diffusers_name = diffusers_name.replace("self.attn", "self_attn")
-            diffusers_name = diffusers_name.replace("q.proj.lora", "to_q_lora")
-            diffusers_name = diffusers_name.replace("k.proj.lora", "to_k_lora")
-            diffusers_name = diffusers_name.replace("v.proj.lora", "to_v_lora")
-            diffusers_name = diffusers_name.replace("out.proj.lora", "to_out_lora")
-            if "self_attn" in diffusers_name:
-                te_state_dict[diffusers_name] = state_dict.pop(key)
-                te_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
-            elif "mlp" in diffusers_name:
-                # Be aware that this is the new diffusers convention and the rest of the code might
-                # not utilize it yet.
-                diffusers_name = diffusers_name.replace(".lora.", ".lora_linear_layer.")
-                te_state_dict[diffusers_name] = state_dict.pop(key)
-                te_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
-
-        # (sayakpaul): Duplicate code. Needs to be cleaned.
-        elif lora_name.startswith("lora_te1_"):
-            diffusers_name = key.replace("lora_te1_", "").replace("_", ".")
-            diffusers_name = diffusers_name.replace("text.model", "text_model")
-            diffusers_name = diffusers_name.replace("self.attn", "self_attn")
-            diffusers_name = diffusers_name.replace("q.proj.lora", "to_q_lora")
-            diffusers_name = diffusers_name.replace("k.proj.lora", "to_k_lora")
-            diffusers_name = diffusers_name.replace("v.proj.lora", "to_v_lora")
-            diffusers_name = diffusers_name.replace("out.proj.lora", "to_out_lora")
-            if "self_attn" in diffusers_name:
-                te_state_dict[diffusers_name] = state_dict.pop(key)
-                te_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
-            elif "mlp" in diffusers_name:
-                # Be aware that this is the new diffusers convention and the rest of the code might
-                # not utilize it yet.
-                diffusers_name = diffusers_name.replace(".lora.", ".lora_linear_layer.")
-                te_state_dict[diffusers_name] = state_dict.pop(key)
-                te_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
+        elif lora_name.startswith(("lora_te_", "lora_te1_", "lora_te2_")):
+            if lora_name.startswith(("lora_te_", "lora_te1_")):
+                key_to_replace = "lora_te_" if lora_name.startswith("lora_te_") else "lora_te1_"
+            else:
+                key_to_replace = "lora_te2_"
 
-        # (sayakpaul): Duplicate code. Needs to be cleaned.
-        elif lora_name.startswith("lora_te2_"):
-            diffusers_name = key.replace("lora_te2_", "").replace("_", ".")
+            diffusers_name = key.replace(key_to_replace, "").replace("_", ".")
             diffusers_name = diffusers_name.replace("text.model", "text_model")
             diffusers_name = diffusers_name.replace("self.attn", "self_attn")
             diffusers_name = diffusers_name.replace("q.proj.lora", "to_q_lora")
             diffusers_name = diffusers_name.replace("k.proj.lora", "to_k_lora")
             diffusers_name = diffusers_name.replace("v.proj.lora", "to_v_lora")
             diffusers_name = diffusers_name.replace("out.proj.lora", "to_out_lora")
             if "self_attn" in diffusers_name:
-                te2_state_dict[diffusers_name] = state_dict.pop(key)
-                te2_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
+                if lora_name.startswith(("lora_te_", "lora_te1_")):
+                    te_state_dict[diffusers_name] = state_dict.pop(key)
+                    te_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
+                else:
+                    te2_state_dict[diffusers_name] = state_dict.pop(key)
+                    te2_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
             elif "mlp" in diffusers_name:
                 # Be aware that this is the new diffusers convention and the rest of the code might
                 # not utilize it yet.
                 diffusers_name = diffusers_name.replace(".lora.", ".lora_linear_layer.")
-                te2_state_dict[diffusers_name] = state_dict.pop(key)
-                te2_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
+                if lora_name.startswith(("lora_te_", "lora_te1_")):
+                    te_state_dict[diffusers_name] = state_dict.pop(key)
+                    te_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
+                else:
+                    te2_state_dict[diffusers_name] = state_dict.pop(key)
+                    te2_state_dict[diffusers_name.replace(".down.", ".up.")] = state_dict.pop(lora_name_up)
 
         # Rename the alphas so that they can be mapped appropriately.
         if lora_name_alpha in state_dict:
             alpha = state_dict.pop(lora_name_alpha).item()
             if lora_name_alpha.startswith("lora_unet_"):
                 prefix = "unet."
             elif lora_name_alpha.startswith(("lora_te_", "lora_te1_")):
```

## diffusers/models/attention.py

```diff
@@ -289,15 +289,15 @@
         timestep: Optional[torch.LongTensor] = None,
         cross_attention_kwargs: Dict[str, Any] = None,
         class_labels: Optional[torch.LongTensor] = None,
         added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,
     ) -> torch.FloatTensor:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         # Notice that normalization is always applied before the real computation in the following blocks.
         # 0. Self-Attention
         batch_size = hidden_states.shape[0]
 
         if self.norm_type == "ada_norm":
             norm_hidden_states = self.norm1(hidden_states, timestep)
```

## diffusers/models/attention_processor.py

```diff
@@ -420,15 +420,15 @@
         # 1. if no layer has a LoRA activated we can return the processor as usual
         if not any(is_lora_activated.values()):
             return self.processor
 
         # If doesn't apply LoRA do `add_k_proj` or `add_v_proj`
         is_lora_activated.pop("add_k_proj", None)
         is_lora_activated.pop("add_v_proj", None)
-        # 2. else it is not posssible that only some layers have LoRA activated
+        # 2. else it is not possible that only some layers have LoRA activated
         if not all(is_lora_activated.values()):
             raise ValueError(
                 f"Make sure that either all layers or no layers have LoRA activated, but have {is_lora_activated}"
             )
 
         # 3. And we need to merge the current LoRA layers into the corresponding LoRA attention processor
         non_lora_processor_cls_name = self.processor.__class__.__name__
@@ -2094,15 +2094,15 @@
         attn._modules.pop("processor")
         attn.processor = AttnAddedKVProcessor()
         return attn.processor(attn, hidden_states, **kwargs)
 
 
 class IPAdapterAttnProcessor(nn.Module):
     r"""
-    Attention processor for Multiple IP-Adapater.
+    Attention processor for Multiple IP-Adapters.
 
     Args:
         hidden_size (`int`):
             The hidden size of the attention layer.
         cross_attention_dim (`int`):
             The number of channels in the `encoder_hidden_states`.
         num_tokens (`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`):
@@ -2148,16 +2148,16 @@
 
         # separate ip_hidden_states from encoder_hidden_states
         if encoder_hidden_states is not None:
             if isinstance(encoder_hidden_states, tuple):
                 encoder_hidden_states, ip_hidden_states = encoder_hidden_states
             else:
                 deprecation_message = (
-                    "You have passed a tensor as `encoder_hidden_states`.This is deprecated and will be removed in a future release."
-                    " Please make sure to update your script to pass `encoder_hidden_states` as a tuple to supress this warning."
+                    "You have passed a tensor as `encoder_hidden_states`. This is deprecated and will be removed in a future release."
+                    " Please make sure to update your script to pass `encoder_hidden_states` as a tuple to suppress this warning."
                 )
                 deprecate("encoder_hidden_states not a tuple", "1.0.0", deprecation_message, standard_warn=False)
                 end_pos = encoder_hidden_states.shape[1] - self.num_tokens[0]
                 encoder_hidden_states, ip_hidden_states = (
                     encoder_hidden_states[:, :end_pos, :],
                     [encoder_hidden_states[:, end_pos:, :]],
                 )
@@ -2249,15 +2249,15 @@
         hidden_states = hidden_states / attn.rescale_output_factor
 
         return hidden_states
 
 
 class IPAdapterAttnProcessor2_0(torch.nn.Module):
     r"""
-    Attention processor for IP-Adapater for PyTorch 2.0.
+    Attention processor for IP-Adapter for PyTorch 2.0.
 
     Args:
         hidden_size (`int`):
             The hidden size of the attention layer.
         cross_attention_dim (`int`):
             The number of channels in the `encoder_hidden_states`.
         num_tokens (`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`):
@@ -2308,16 +2308,16 @@
 
         # separate ip_hidden_states from encoder_hidden_states
         if encoder_hidden_states is not None:
             if isinstance(encoder_hidden_states, tuple):
                 encoder_hidden_states, ip_hidden_states = encoder_hidden_states
             else:
                 deprecation_message = (
-                    "You have passed a tensor as `encoder_hidden_states`.This is deprecated and will be removed in a future release."
-                    " Please make sure to update your script to pass `encoder_hidden_states` as a tuple to supress this warning."
+                    "You have passed a tensor as `encoder_hidden_states`. This is deprecated and will be removed in a future release."
+                    " Please make sure to update your script to pass `encoder_hidden_states` as a tuple to suppress this warning."
                 )
                 deprecate("encoder_hidden_states not a tuple", "1.0.0", deprecation_message, standard_warn=False)
                 end_pos = encoder_hidden_states.shape[1] - self.num_tokens[0]
                 encoder_hidden_states, ip_hidden_states = (
                     encoder_hidden_states[:, :end_pos, :],
                     [encoder_hidden_states[:, end_pos:, :]],
                 )
```

## diffusers/models/controlnet.py

```diff
@@ -277,15 +277,15 @@
             )
 
         if encoder_hid_dim_type == "text_proj":
             self.encoder_hid_proj = nn.Linear(encoder_hid_dim, cross_attention_dim)
         elif encoder_hid_dim_type == "text_image_proj":
             # image_embed_dim DOESN'T have to be `cross_attention_dim`. To not clutter the __init__ too much
             # they are set to `cross_attention_dim` here as this is exactly the required dimension for the currently only use
-            # case when `addition_embed_type == "text_image_proj"` (Kadinsky 2.1)`
+            # case when `addition_embed_type == "text_image_proj"` (Kandinsky 2.1)`
             self.encoder_hid_proj = TextImageProjection(
                 text_embed_dim=encoder_hid_dim,
                 image_embed_dim=cross_attention_dim,
                 cross_attention_dim=cross_attention_dim,
             )
 
         elif encoder_hid_dim_type is not None:
@@ -326,15 +326,15 @@
 
             self.add_embedding = TextTimeEmbedding(
                 text_time_embedding_from_dim, time_embed_dim, num_heads=addition_embed_type_num_heads
             )
         elif addition_embed_type == "text_image":
             # text_embed_dim and image_embed_dim DON'T have to be `cross_attention_dim`. To not clutter the __init__ too much
             # they are set to `cross_attention_dim` here as this is exactly the required dimension for the currently only use
-            # case when `addition_embed_type == "text_image"` (Kadinsky 2.1)`
+            # case when `addition_embed_type == "text_image"` (Kandinsky 2.1)`
             self.add_embedding = TextImageTimeEmbedding(
                 text_embed_dim=cross_attention_dim, image_embed_dim=cross_attention_dim, time_embed_dim=time_embed_dim
             )
         elif addition_embed_type == "text_time":
             self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos, freq_shift)
             self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)
 
@@ -669,14 +669,15 @@
         conditioning_scale: float = 1.0,
         class_labels: Optional[torch.Tensor] = None,
         timestep_cond: Optional[torch.Tensor] = None,
         attention_mask: Optional[torch.Tensor] = None,
         added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         guess_mode: bool = False,
+        promptsMoreImportant: bool = False,
         return_dict: bool = True,
     ) -> Union[ControlNetOutput, Tuple[Tuple[torch.FloatTensor, ...], torch.FloatTensor]]:
         """
         The [`ControlNetModel`] forward method.
 
         Args:
             sample (`torch.FloatTensor`):
@@ -840,14 +841,19 @@
 
         # 6. scaling
         if guess_mode and not self.config.global_pool_conditions:
             scales = torch.logspace(-1, 0, len(down_block_res_samples) + 1, device=sample.device)  # 0.1 to 1.0
             scales = scales * conditioning_scale
             down_block_res_samples = [sample * scale for sample, scale in zip(down_block_res_samples, scales)]
             mid_block_res_sample = mid_block_res_sample * scales[-1]  # last one
+        elif promptsMoreImportant and not self.config.global_pool_conditions:
+            scales = [conditioning_scale * (0.825 ** float(12-i)) for i in range(13)]
+            down_block_res_samples = [sample * scale for sample, scale in zip(down_block_res_samples, scales)]
+            mid_block_res_sample = mid_block_res_sample * scales[-1]  # last one
+            
         else:
             down_block_res_samples = [sample * conditioning_scale for sample in down_block_res_samples]
             mid_block_res_sample = mid_block_res_sample * conditioning_scale
 
         if self.config.global_pool_conditions:
             down_block_res_samples = [
                 torch.mean(sample, dim=(2, 3), keepdim=True) for sample in down_block_res_samples
```

## diffusers/models/transformer_temporal.py

```diff
@@ -16,19 +16,19 @@
     TransformerSpatioTemporalModel,
     TransformerTemporalModel,
     TransformerTemporalModelOutput,
 )
 
 
 class TransformerTemporalModelOutput(TransformerTemporalModelOutput):
-    deprecation_message = "Importing `TransformerTemporalModelOutput` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.tranformer_temporal import TransformerTemporalModelOutput`, instead."
+    deprecation_message = "Importing `TransformerTemporalModelOutput` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_temporal import TransformerTemporalModelOutput`, instead."
     deprecate("TransformerTemporalModelOutput", "0.29", deprecation_message)
 
 
 class TransformerTemporalModel(TransformerTemporalModel):
-    deprecation_message = "Importing `TransformerTemporalModel` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.tranformer_temporal import TransformerTemporalModel`, instead."
+    deprecation_message = "Importing `TransformerTemporalModel` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_temporal import TransformerTemporalModel`, instead."
     deprecate("TransformerTemporalModel", "0.29", deprecation_message)
 
 
 class TransformerSpatioTemporalModel(TransformerSpatioTemporalModel):
-    deprecation_message = "Importing `TransformerSpatioTemporalModel` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.tranformer_temporal import TransformerSpatioTemporalModel`, instead."
+    deprecation_message = "Importing `TransformerSpatioTemporalModel` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_temporal import TransformerSpatioTemporalModel`, instead."
     deprecate("TransformerTemporalModelOutput", "0.29", deprecation_message)
```

## diffusers/models/transformers/transformer_2d.py

```diff
@@ -125,15 +125,15 @@
         self.is_input_continuous = (in_channels is not None) and (patch_size is None)
         self.is_input_vectorized = num_vector_embeds is not None
         self.is_input_patches = in_channels is not None and patch_size is not None
 
         if norm_type == "layer_norm" and num_embeds_ada_norm is not None:
             deprecation_message = (
                 f"The configuration file of this model: {self.__class__} is outdated. `norm_type` is either not set or"
-                " incorrectly set to `'layer_norm'`.Make sure to set `norm_type` to `'ada_norm'` in the config."
+                " incorrectly set to `'layer_norm'`. Make sure to set `norm_type` to `'ada_norm'` in the config."
                 " Please make sure to update the config accordingly as leaving `norm_type` might led to incorrect"
                 " results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it"
                 " would be very nice if you could open a Pull request for the `transformer/config.json` file"
             )
             deprecate("norm_type!=num_embeds_ada_norm", "1.0.0", deprecation_message, standard_warn=False)
             norm_type = "ada_norm"
 
@@ -304,15 +304,15 @@
 
         Returns:
             If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a
             `tuple` where the first element is the sample tensor.
         """
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
         # ensure attention_mask is a bias, and give it a singleton query_tokens dimension.
         #   we may have done this conversion already, e.g. if we came here via UNet2DConditionModel#forward.
         #   we can tell by counting dims; if ndim == 2: it's a mask rather than a bias.
         # expects mask of shape:
         #   [batch, key_tokens]
         # adds singleton query_tokens dimension:
         #   [batch,                    1, key_tokens]
```

## diffusers/models/unets/unet_2d_blocks.py

```diff
@@ -842,15 +842,15 @@
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         hidden_states = self.resnets[0](hidden_states, temb)
         for attn, resnet in zip(self.attentions, self.resnets[1:]):
             if self.training and self.gradient_checkpointing:
 
                 def create_custom_forward(module, return_dict=None):
                     def custom_forward(*inputs):
@@ -982,15 +982,15 @@
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
         if cross_attention_kwargs.get("scale", None) is not None:
-            logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+            logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         if attention_mask is None:
             # if encoder_hidden_states is defined: we are doing cross-attn, so we should use cross-attn mask.
             mask = None if encoder_hidden_states is None else encoder_attention_mask
         else:
             # when attention_mask is defined: we don't even check for encoder_attention_mask.
             # this is to maintain compatibility with UnCLIP, which uses 'attention_mask' param for cross-attn masks.
@@ -1112,15 +1112,15 @@
         hidden_states: torch.FloatTensor,
         temb: Optional[torch.FloatTensor] = None,
         upsample_size: Optional[int] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
     ) -> Tuple[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]:
         cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
         if cross_attention_kwargs.get("scale", None) is not None:
-            logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+            logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         output_states = ()
 
         for resnet, attn in zip(self.resnets, self.attentions):
             hidden_states = resnet(hidden_states, temb)
             hidden_states = attn(hidden_states, **cross_attention_kwargs)
             output_states = output_states + (hidden_states,)
@@ -1237,15 +1237,15 @@
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
         additional_residuals: Optional[torch.FloatTensor] = None,
     ) -> Tuple[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         output_states = ()
 
         blocks = list(zip(self.resnets, self.attentions))
 
         for i, (resnet, attn) in enumerate(blocks):
             if self.training and self.gradient_checkpointing:
@@ -1982,15 +1982,15 @@
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> Tuple[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]:
         cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
         if cross_attention_kwargs.get("scale", None) is not None:
-            logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+            logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         output_states = ()
 
         if attention_mask is None:
             # if encoder_hidden_states is defined: we are doing cross-attn, so we should use cross-attn mask.
             mask = None if encoder_hidden_states is None else encoder_attention_mask
         else:
@@ -2197,15 +2197,15 @@
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> Tuple[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]:
         cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
         if cross_attention_kwargs.get("scale", None) is not None:
-            logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+            logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         output_states = ()
 
         for resnet, attn in zip(self.resnets, self.attentions):
             if self.training and self.gradient_checkpointing:
 
                 def create_custom_forward(module, return_dict=None):
@@ -2479,15 +2479,15 @@
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         upsample_size: Optional[int] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         is_freeu_enabled = (
             getattr(self, "s1", None)
             and getattr(self, "s2", None)
             and getattr(self, "b1", None)
             and getattr(self, "b2", None)
         )
@@ -3308,15 +3308,15 @@
         upsample_size: Optional[int] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
         if cross_attention_kwargs.get("scale", None) is not None:
-            logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+            logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         if attention_mask is None:
             # if encoder_hidden_states is defined: we are doing cross-attn, so we should use cross-attn mask.
             mask = None if encoder_hidden_states is None else encoder_attention_mask
         else:
             # when attention_mask is defined: we don't even check for encoder_attention_mask.
             # this is to maintain compatibility with UnCLIP, which uses 'attention_mask' param for cross-attn masks.
@@ -3690,15 +3690,15 @@
         emb: Optional[torch.FloatTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
         if cross_attention_kwargs.get("scale", None) is not None:
-            logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+            logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         # 1. Self-Attention
         if self.add_self_attention:
             norm_hidden_states = self.norm1(hidden_states, emb)
 
             height, weight = norm_hidden_states.shape[2:]
             norm_hidden_states = self._to_3d(norm_hidden_states, height, weight)
```

## diffusers/models/unets/unet_2d_condition.py

```diff
@@ -576,15 +576,15 @@
             )
 
         if encoder_hid_dim_type == "text_proj":
             self.encoder_hid_proj = nn.Linear(encoder_hid_dim, cross_attention_dim)
         elif encoder_hid_dim_type == "text_image_proj":
             # image_embed_dim DOESN'T have to be `cross_attention_dim`. To not clutter the __init__ too much
             # they are set to `cross_attention_dim` here as this is exactly the required dimension for the currently only use
-            # case when `addition_embed_type == "text_image_proj"` (Kadinsky 2.1)`
+            # case when `addition_embed_type == "text_image_proj"` (Kandinsky 2.1)`
             self.encoder_hid_proj = TextImageProjection(
                 text_embed_dim=encoder_hid_dim,
                 image_embed_dim=cross_attention_dim,
                 cross_attention_dim=cross_attention_dim,
             )
         elif encoder_hid_dim_type == "image_proj":
             # Kandinsky 2.2
@@ -656,15 +656,15 @@
 
             self.add_embedding = TextTimeEmbedding(
                 text_time_embedding_from_dim, time_embed_dim, num_heads=addition_embed_type_num_heads
             )
         elif addition_embed_type == "text_image":
             # text_embed_dim and image_embed_dim DON'T have to be `cross_attention_dim`. To not clutter the __init__ too much
             # they are set to `cross_attention_dim` here as this is exactly the required dimension for the currently only use
-            # case when `addition_embed_type == "text_image"` (Kadinsky 2.1)`
+            # case when `addition_embed_type == "text_image"` (Kandinsky 2.1)`
             self.add_embedding = TextImageTimeEmbedding(
                 text_embed_dim=cross_attention_dim, image_embed_dim=cross_attention_dim, time_embed_dim=time_embed_dim
             )
         elif addition_embed_type == "text_time":
             self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos, freq_shift)
             self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)
         elif addition_embed_type == "image":
@@ -1006,15 +1006,15 @@
 
     def process_encoder_hidden_states(
         self, encoder_hidden_states: torch.Tensor, added_cond_kwargs: Dict[str, Any]
     ) -> torch.Tensor:
         if self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_proj":
             encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states)
         elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_image_proj":
-            # Kadinsky 2.1 - style
+            # Kandinsky 2.1 - style
             if "image_embeds" not in added_cond_kwargs:
                 raise ValueError(
                     f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'text_image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`"
                 )
 
             image_embeds = added_cond_kwargs.get("image_embeds")
             encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states, image_embeds)
```

## diffusers/models/unets/unet_3d_blocks.py

```diff
@@ -1179,15 +1179,15 @@
         num_frames: int = 1,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         additional_residuals: Optional[torch.FloatTensor] = None,
     ):
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         output_states = ()
 
         blocks = list(zip(self.resnets, self.attentions, self.motion_modules))
         for i, (resnet, attn, motion_module) in enumerate(blocks):
             if self.training and self.gradient_checkpointing:
 
@@ -1363,15 +1363,15 @@
         upsample_size: Optional[int] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
         num_frames: int = 1,
     ) -> torch.FloatTensor:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         is_freeu_enabled = (
             getattr(self, "s1", None)
             and getattr(self, "s2", None)
             and getattr(self, "b1", None)
             and getattr(self, "b2", None)
         )
@@ -1703,15 +1703,15 @@
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
         num_frames: int = 1,
     ) -> torch.FloatTensor:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         hidden_states = self.resnets[0](hidden_states, temb)
 
         blocks = zip(self.attentions, self.resnets[1:], self.motion_modules)
         for attn, resnet, motion_module in blocks:
             if self.training and self.gradient_checkpointing:
```

## diffusers/models/unets/unet_stable_cascade.py

```diff
@@ -517,17 +517,19 @@
 
             for i, (up_block, upscaler, repmap) in enumerate(block_group):
                 for j in range(len(repmap) + 1):
                     for k, block in enumerate(up_block):
                         if isinstance(block, SDCascadeResBlock):
                             skip = level_outputs[i] if k == 0 and i > 0 else None
                             if skip is not None and (x.size(-1) != skip.size(-1) or x.size(-2) != skip.size(-2)):
+                                orig_type = x.dtype
                                 x = torch.nn.functional.interpolate(
                                     x.float(), skip.shape[-2:], mode="bilinear", align_corners=True
                                 )
+                                x = x.to(orig_type)
                             x = torch.utils.checkpoint.checkpoint(
                                 create_custom_forward(block), x, skip, use_reentrant=False
                             )
                         elif isinstance(block, SDCascadeAttnBlock):
                             x = torch.utils.checkpoint.checkpoint(
                                 create_custom_forward(block), x, clip, use_reentrant=False
                             )
@@ -543,17 +545,19 @@
         else:
             for i, (up_block, upscaler, repmap) in enumerate(block_group):
                 for j in range(len(repmap) + 1):
                     for k, block in enumerate(up_block):
                         if isinstance(block, SDCascadeResBlock):
                             skip = level_outputs[i] if k == 0 and i > 0 else None
                             if skip is not None and (x.size(-1) != skip.size(-1) or x.size(-2) != skip.size(-2)):
+                                orig_type = x.dtype
                                 x = torch.nn.functional.interpolate(
                                     x.float(), skip.shape[-2:], mode="bilinear", align_corners=True
                                 )
+                                x = x.to(orig_type)
                             x = block(x, skip)
                         elif isinstance(block, SDCascadeAttnBlock):
                             x = block(x, clip)
                         elif isinstance(block, SDCascadeTimestepBlock):
                             x = block(x, r_embed)
                         else:
                             x = block(x)
```

## diffusers/pipelines/auto_pipeline.py

```diff
@@ -42,14 +42,15 @@
     KandinskyV22InpaintCombinedPipeline,
     KandinskyV22InpaintPipeline,
     KandinskyV22Pipeline,
 )
 from .kandinsky3 import Kandinsky3Img2ImgPipeline, Kandinsky3Pipeline
 from .latent_consistency_models import LatentConsistencyModelImg2ImgPipeline, LatentConsistencyModelPipeline
 from .pixart_alpha import PixArtAlphaPipeline
+from .stable_cascade import StableCascadeCombinedPipeline, StableCascadeDecoderPipeline
 from .stable_diffusion import (
     StableDiffusionImg2ImgPipeline,
     StableDiffusionInpaintPipeline,
     StableDiffusionPipeline,
 )
 from .stable_diffusion_xl import (
     StableDiffusionXLImg2ImgPipeline,
@@ -66,14 +67,15 @@
         ("if", IFPipeline),
         ("kandinsky", KandinskyCombinedPipeline),
         ("kandinsky22", KandinskyV22CombinedPipeline),
         ("kandinsky3", Kandinsky3Pipeline),
         ("stable-diffusion-controlnet", StableDiffusionControlNetPipeline),
         ("stable-diffusion-xl-controlnet", StableDiffusionXLControlNetPipeline),
         ("wuerstchen", WuerstchenCombinedPipeline),
+        ("cascade", StableCascadeCombinedPipeline),
         ("lcm", LatentConsistencyModelPipeline),
         ("pixart", PixArtAlphaPipeline),
     ]
 )
 
 AUTO_IMAGE2IMAGE_PIPELINES_MAPPING = OrderedDict(
     [
@@ -102,14 +104,15 @@
 )
 
 _AUTO_TEXT2IMAGE_DECODER_PIPELINES_MAPPING = OrderedDict(
     [
         ("kandinsky", KandinskyPipeline),
         ("kandinsky22", KandinskyV22Pipeline),
         ("wuerstchen", WuerstchenDecoderPipeline),
+        ("cascade", StableCascadeDecoderPipeline),
     ]
 )
 _AUTO_IMAGE2IMAGE_DECODER_PIPELINES_MAPPING = OrderedDict(
     [
         ("kandinsky", KandinskyImg2ImgPipeline),
         ("kandinsky22", KandinskyV22Img2ImgPipeline),
     ]
```

## diffusers/pipelines/amused/pipeline_amused_img2img.py

```diff
@@ -123,15 +123,15 @@
                 latents as `image`, but if passing latents directly it is not encoded again.
             strength (`float`, *optional*, defaults to 0.5):
                 Indicates extent to transform the reference `image`. Must be between 0 and 1. `image` is used as a
                 starting point and more noise is added the higher the `strength`. The number of denoising steps depends
                 on the amount of noise initially added. When `strength` is 1, added noise is maximum and the denoising
                 process runs for the full number of iterations specified in `num_inference_steps`. A value of 1
                 essentially ignores `image`.
-            num_inference_steps (`int`, *optional*, defaults to 16):
+            num_inference_steps (`int`, *optional*, defaults to 12):
                 The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                 expense of slower inference.
             guidance_scale (`float`, *optional*, defaults to 10.0):
                 A higher guidance scale value encourages the model to generate images closely linked to the text
                 `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.
             negative_prompt (`str` or `List[str]`, *optional*):
                 The prompt or prompts to guide what to not include in image generation. If not defined, you need to
@@ -187,15 +187,15 @@
         ):
             raise ValueError("pass either both `prompt_embeds` and `encoder_hidden_states` or neither")
 
         if (negative_prompt_embeds is not None and negative_encoder_hidden_states is None) or (
             negative_prompt_embeds is None and negative_encoder_hidden_states is not None
         ):
             raise ValueError(
-                "pass either both `negatve_prompt_embeds` and `negative_encoder_hidden_states` or neither"
+                "pass either both `negative_prompt_embeds` and `negative_encoder_hidden_states` or neither"
             )
 
         if (prompt is None and prompt_embeds is None) or (prompt is not None and prompt_embeds is not None):
             raise ValueError("pass only one of `prompt` or `prompt_embeds`")
 
         if isinstance(prompt, str):
             prompt = [prompt]
```

## diffusers/pipelines/controlnet/pipeline_controlnet.py

```diff
@@ -820,28 +820,30 @@
             latents = latents.to(device)
 
         # scale the initial noise by the standard deviation required by the scheduler
         latents = latents * self.scheduler.init_noise_sigma
         return latents
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py

```diff
@@ -1167,15 +1167,15 @@
             width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                 The width in pixels of the generated image.
             padding_mask_crop (`int`, *optional*, defaults to `None`):
                 The size of margin in the crop to be applied to the image and masking. If `None`, no crop is applied to image and mask_image. If
                 `padding_mask_crop` is not `None`, it will first find a rectangular region with the same aspect ration of the image and
                 contains all masked area, and then expand that area based on `padding_mask_crop`. The image and mask_image will then be cropped based on
                 the expanded area before resizing to the original image size for inpainting. This is useful when the masked area is small while the image is large
-                and contain information inreleant for inpainging, such as background.
+                and contain information irrelevant for inpainting, such as background.
             strength (`float`, *optional*, defaults to 1.0):
                 Indicates extent to transform the reference `image`. Must be between 0 and 1. `image` is used as a
                 starting point and more noise is added the higher the `strength`. The number of denoising steps depends
                 on the amount of noise initially added. When `strength` is 1, added noise is maximum and the denoising
                 process runs for the full number of iterations specified in `num_inference_steps`. A value of 1
                 essentially ignores `image`.
             num_inference_steps (`int`, *optional*, defaults to 50):
```

## diffusers/pipelines/controlnet/pipeline_controlnet_inpaint_sd_xl.py

```diff
@@ -1194,15 +1194,15 @@
             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                 The width in pixels of the generated image.
             padding_mask_crop (`int`, *optional*, defaults to `None`):
                 The size of margin in the crop to be applied to the image and masking. If `None`, no crop is applied to image and mask_image. If
                 `padding_mask_crop` is not `None`, it will first find a rectangular region with the same aspect ration of the image and
                 contains all masked area, and then expand that area based on `padding_mask_crop`. The image and mask_image will then be cropped based on
                 the expanded area before resizing to the original image size for inpainting. This is useful when the masked area is small while the image is large
-                and contain information inreleant for inpainging, such as background.
+                and contain information irrelevant for inpainting, such as background.
             strength (`float`, *optional*, defaults to 0.9999):
                 Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be
                 between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the
                 `strength`. The number of denoising steps depends on the amount of noise initially added. When
                 `strength` is 1, added noise will be maximum and the denoising process will run for the full number of
                 iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked
                 portion of the reference `image`. Note that in the case of `denoising_start` being declared as an
```

## diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py

```diff
@@ -865,28 +865,30 @@
         # to be in float32 which can save lots of memory
         if use_torch_2_0_or_xformers:
             self.vae.post_quant_conv.to(dtype)
             self.vae.decoder.conv_in.to(dtype)
             self.vae.decoder.mid_block.to(dtype)
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py

```diff
@@ -609,15 +609,15 @@
             return images
 
         if isinstance(image[0], PIL.Image.Image):
             new_image = []
 
             for image_ in image:
                 image_ = image_.convert("RGB")
-                image_ = resize(image_, self.unet.sample_size)
+                image_ = resize(image_, self.unet.config.sample_size)
                 image_ = np.array(image_)
                 image_ = image_.astype(np.float32)
                 image_ = image_ / 127.5 - 1
                 new_image.append(image_)
 
             image = new_image
```

## diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py

```diff
@@ -658,15 +658,15 @@
             return images
 
         if isinstance(image[0], PIL.Image.Image):
             new_image = []
 
             for image_ in image:
                 image_ = image_.convert("RGB")
-                image_ = resize(image_, self.unet.sample_size)
+                image_ = resize(image_, self.unet.config.sample_size)
                 image_ = np.array(image_)
                 image_ = image_.astype(np.float32)
                 image_ = image_ / 127.5 - 1
                 new_image.append(image_)
 
             image = new_image
```

## diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py

```diff
@@ -650,15 +650,15 @@
             return images
 
         if isinstance(image[0], PIL.Image.Image):
             new_image = []
 
             for image_ in image:
                 image_ = image_.convert("RGB")
-                image_ = resize(image_, self.unet.sample_size)
+                image_ = resize(image_, self.unet.config.sample_size)
                 image_ = np.array(image_)
                 image_ = image_.astype(np.float32)
                 image_ = image_ / 127.5 - 1
                 new_image.append(image_)
 
             image = new_image
 
@@ -697,15 +697,15 @@
             mask_image[mask_image >= 0.5] = 1
 
         elif isinstance(mask_image[0], PIL.Image.Image):
             new_mask_image = []
 
             for mask_image_ in mask_image:
                 mask_image_ = mask_image_.convert("L")
-                mask_image_ = resize(mask_image_, self.unet.sample_size)
+                mask_image_ = resize(mask_image_, self.unet.config.sample_size)
                 mask_image_ = np.array(mask_image_)
                 mask_image_ = mask_image_[None, None, :]
                 new_mask_image.append(mask_image_)
 
             mask_image = new_mask_image
 
             mask_image = np.concatenate(mask_image, axis=0)
```

## diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py

```diff
@@ -694,15 +694,15 @@
             return images
 
         if isinstance(image[0], PIL.Image.Image):
             new_image = []
 
             for image_ in image:
                 image_ = image_.convert("RGB")
-                image_ = resize(image_, self.unet.sample_size)
+                image_ = resize(image_, self.unet.config.sample_size)
                 image_ = np.array(image_)
                 image_ = image_.astype(np.float32)
                 image_ = image_ / 127.5 - 1
                 new_image.append(image_)
 
             image = new_image
 
@@ -774,15 +774,15 @@
             mask_image[mask_image >= 0.5] = 1
 
         elif isinstance(mask_image[0], PIL.Image.Image):
             new_mask_image = []
 
             for mask_image_ in mask_image:
                 mask_image_ = mask_image_.convert("L")
-                mask_image_ = resize(mask_image_, self.unet.sample_size)
+                mask_image_ = resize(mask_image_, self.unet.config.sample_size)
                 mask_image_ = np.array(mask_image_)
                 mask_image_ = mask_image_[None, None, :]
                 new_mask_image.append(mask_image_)
 
             mask_image = new_mask_image
 
             mask_image = np.concatenate(mask_image, axis=0)
```

## diffusers/pipelines/deprecated/spectrogram_diffusion/pipeline_spectrogram_diffusion.py

```diff
@@ -129,15 +129,15 @@
     @torch.no_grad()
     def __call__(
         self,
         input_tokens: List[List[int]],
         generator: Optional[torch.Generator] = None,
         num_inference_steps: int = 100,
         return_dict: bool = True,
-        output_type: str = "numpy",
+        output_type: str = "np",
         callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
         callback_steps: int = 1,
     ) -> Union[AudioPipelineOutput, Tuple]:
         if (callback_steps is None) or (
             callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)
         ):
             raise ValueError(
@@ -153,15 +153,15 @@
                 A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make
                 generation deterministic.
             num_inference_steps (`int`, *optional*, defaults to 100):
                 The number of denoising steps. More denoising steps usually lead to a higher quality audio at the
                 expense of slower inference.
             return_dict (`bool`, *optional*, defaults to `True`):
                 Whether or not to return a [`~pipelines.AudioPipelineOutput`] instead of a plain tuple.
-            output_type (`str`, *optional*, defaults to `"numpy"`):
+            output_type (`str`, *optional*, defaults to `"np"`):
                 The output format of the generated audio.
             callback (`Callable`, *optional*):
                 A function that calls every `callback_steps` steps during inference. The function is called with the
                 following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.
             callback_steps (`int`, *optional*, defaults to 1):
                 The frequency at which the `callback` function is called. If not specified, the callback is called at
                 every step.
@@ -245,24 +245,24 @@
 
             # call the callback, if provided
             if callback is not None and i % callback_steps == 0:
                 callback(i, full_pred_mel)
 
             logger.info("Generated segment", i)
 
-        if output_type == "numpy" and not is_onnx_available():
+        if output_type == "np" and not is_onnx_available():
             raise ValueError(
                 "Cannot return output in 'np' format if ONNX is not available. Make sure to have ONNX installed or set 'output_type' to 'mel'."
             )
-        elif output_type == "numpy" and self.melgan is None:
+        elif output_type == "np" and self.melgan is None:
             raise ValueError(
                 "Cannot return output in 'np' format if melgan component is not defined. Make sure to define `self.melgan` or set 'output_type' to 'mel'."
             )
 
-        if output_type == "numpy":
+        if output_type == "np":
             output = self.melgan(input_features=full_pred_mel.astype(np.float32))
         else:
             output = full_pred_mel
 
         if not return_dict:
             return (output,)
```

## diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py

```diff
@@ -527,15 +527,15 @@
             )
 
         if encoder_hid_dim_type == "text_proj":
             self.encoder_hid_proj = nn.Linear(encoder_hid_dim, cross_attention_dim)
         elif encoder_hid_dim_type == "text_image_proj":
             # image_embed_dim DOESN'T have to be `cross_attention_dim`. To not clutter the __init__ too much
             # they are set to `cross_attention_dim` here as this is exactly the required dimension for the currently only use
-            # case when `addition_embed_type == "text_image_proj"` (Kadinsky 2.1)`
+            # case when `addition_embed_type == "text_image_proj"` (Kandinsky 2.1)`
             self.encoder_hid_proj = TextImageProjection(
                 text_embed_dim=encoder_hid_dim,
                 image_embed_dim=cross_attention_dim,
                 cross_attention_dim=cross_attention_dim,
             )
         elif encoder_hid_dim_type == "image_proj":
             # Kandinsky 2.2
@@ -587,15 +587,15 @@
 
             self.add_embedding = TextTimeEmbedding(
                 text_time_embedding_from_dim, time_embed_dim, num_heads=addition_embed_type_num_heads
             )
         elif addition_embed_type == "text_image":
             # text_embed_dim and image_embed_dim DON'T have to be `cross_attention_dim`. To not clutter the __init__ too much
             # they are set to `cross_attention_dim` here as this is exactly the required dimension for the currently only use
-            # case when `addition_embed_type == "text_image"` (Kadinsky 2.1)`
+            # case when `addition_embed_type == "text_image"` (Kandinsky 2.1)`
             self.add_embedding = TextImageTimeEmbedding(
                 text_embed_dim=cross_attention_dim, image_embed_dim=cross_attention_dim, time_embed_dim=time_embed_dim
             )
         elif addition_embed_type == "text_time":
             self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos, freq_shift)
             self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)
         elif addition_embed_type == "image":
@@ -1253,15 +1253,15 @@
 
         if self.time_embed_act is not None:
             emb = self.time_embed_act(emb)
 
         if self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_proj":
             encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states)
         elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_image_proj":
-            # Kadinsky 2.1 - style
+            # Kandinsky 2.1 - style
             if "image_embeds" not in added_cond_kwargs:
                 raise ValueError(
                     f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'text_image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`"
                 )
 
             image_embeds = added_cond_kwargs.get("image_embeds")
             encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states, image_embeds)
@@ -2000,15 +2000,15 @@
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         upsample_size: Optional[int] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         is_freeu_enabled = (
             getattr(self, "s1", None)
             and getattr(self, "s2", None)
             and getattr(self, "b1", None)
             and getattr(self, "b2", None)
         )
@@ -2334,15 +2334,15 @@
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         if cross_attention_kwargs is not None:
             if cross_attention_kwargs.get("scale", None) is not None:
-                logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         hidden_states = self.resnets[0](hidden_states, temb)
         for attn, resnet in zip(self.attentions, self.resnets[1:]):
             if self.training and self.gradient_checkpointing:
 
                 def create_custom_forward(module, return_dict=None):
                     def custom_forward(*inputs):
@@ -2475,15 +2475,15 @@
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
         if cross_attention_kwargs.get("scale", None) is not None:
-            logger.warning("Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.")
+            logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
 
         if attention_mask is None:
             # if encoder_hidden_states is defined: we are doing cross-attn, so we should use cross-attn mask.
             mask = None if encoder_hidden_states is None else encoder_attention_mask
         else:
             # when attention_mask is defined: we don't even check for encoder_attention_mask.
             # this is to maintain compatibility with UnCLIP, which uses 'attention_mask' param for cross-attn masks.
```

## diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py

```diff
@@ -544,28 +544,30 @@
         # get latents
         init_latents = self.scheduler.add_noise(init_latents, noise, timestep)
         latents = init_latents
 
         return latents
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py

```diff
@@ -486,28 +486,30 @@
         else:
             latents = latents.to(device)
 
         # scale the initial noise by the standard deviation required by the scheduler
         latents = latents * self.scheduler.init_noise_sigma
         return latents
 
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion_xl.py

```diff
@@ -709,28 +709,30 @@
         # to be in float32 which can save lots of memory
         if use_torch_2_0_or_xformers:
             self.vae.post_quant_conv.to(dtype)
             self.vae.decoder.conv_in.to(dtype)
             self.vae.decoder.mid_block.to(dtype)
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py

```diff
@@ -96,18 +96,16 @@
             tokenizer=tokenizer,
             text_encoder=text_encoder,
             scheduler=scheduler,
             vqgan=vqgan,
         )
         self.register_to_config(latent_dim_scale=latent_dim_scale)
 
-    def prepare_latents(
-        self, batch_size, image_embeddings, num_images_per_prompt, dtype, device, generator, latents, scheduler
-    ):
-        _, channels, height, width = image_embeddings.shape
+    def prepare_latents(self, image_embeddings, num_images_per_prompt, dtype, device, generator, latents, scheduler):
+        batch_size, channels, height, width = image_embeddings.shape
         latents_shape = (
             batch_size * num_images_per_prompt,
             4,
             int(height * self.config.latent_dim_scale),
             int(width * self.config.latent_dim_scale),
         )
 
@@ -381,27 +379,15 @@
             negative_prompt=negative_prompt,
             prompt_embeds=prompt_embeds,
             negative_prompt_embeds=negative_prompt_embeds,
             callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,
         )
         if isinstance(image_embeddings, list):
             image_embeddings = torch.cat(image_embeddings, dim=0)
-
-        if prompt is not None and isinstance(prompt, str):
-            batch_size = 1
-        elif prompt is not None and isinstance(prompt, list):
-            batch_size = len(prompt)
-        else:
-            batch_size = prompt_embeds.shape[0]
-
-        # Compute the effective number of images per prompt
-        # We must account for the fact that the image embeddings from the prior can be generated with num_images_per_prompt > 1
-        # This results in a case where a single prompt is associated with multiple image embeddings
-        # Divide the number of image embeddings by the batch size to determine if this is the case.
-        num_images_per_prompt = num_images_per_prompt * (image_embeddings.shape[0] // batch_size)
+        batch_size = image_embeddings.shape[0]
 
         # 2. Encode caption
         if prompt_embeds is None and negative_prompt_embeds is None:
             _, prompt_embeds_pooled, _, negative_prompt_embeds_pooled = self.encode_prompt(
                 prompt=prompt,
                 device=device,
                 batch_size=batch_size,
@@ -427,15 +413,15 @@
         )
 
         self.scheduler.set_timesteps(num_inference_steps, device=device)
         timesteps = self.scheduler.timesteps
 
         # 5. Prepare latents
         latents = self.prepare_latents(
-            batch_size, image_embeddings, num_images_per_prompt, dtype, device, generator, latents, self.scheduler
+            image_embeddings, num_images_per_prompt, dtype, device, generator, latents, self.scheduler
         )
 
         # 6. Run denoising loop
         self._num_timesteps = len(timesteps[:-1])
         for i, t in enumerate(self.progress_bar(timesteps[:-1])):
             timestep_ratio = t.expand(latents.size(0)).to(dtype)
```

## diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py

```diff
@@ -465,15 +465,15 @@
 
         latents_dtype = prompt_embeds.dtype
         image = preprocess(image).cpu().numpy()
         height, width = image.shape[2:]
 
         latents = self.prepare_latents(
             batch_size * num_images_per_prompt,
-            self.num_latent_channels,
+            self.config.num_latent_channels,
             height,
             width,
             latents_dtype,
             generator,
         )
         image = image.astype(latents_dtype)
 
@@ -494,20 +494,20 @@
 
         batch_multiplier = 2 if do_classifier_free_guidance else 1
         image = np.concatenate([image] * batch_multiplier * num_images_per_prompt)
         noise_level = np.concatenate([noise_level] * image.shape[0])
 
         # 7. Check that sizes of image and latents match
         num_channels_image = image.shape[1]
-        if self.num_latent_channels + num_channels_image != self.num_unet_input_channels:
+        if self.config.num_latent_channels + num_channels_image != self.config.num_unet_input_channels:
             raise ValueError(
                 "Incorrect configuration settings! The config of `pipeline.unet` expects"
-                f" {self.num_unet_input_channels} but received `num_channels_latents`: {self.num_latent_channels} +"
+                f" {self.config.num_unet_input_channels} but received `num_channels_latents`: {self.config.num_latent_channels} +"
                 f" `num_channels_image`: {num_channels_image} "
-                f" = {self.num_latent_channels + num_channels_image}. Please verify the config of"
+                f" = {self.config.num_latent_channels + num_channels_image}. Please verify the config of"
                 " `pipeline.unet` or your `image` input."
             )
 
         # 8. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
         accepts_eta = "eta" in set(inspect.signature(self.scheduler.step).parameters.keys())
         extra_step_kwargs = {}
         if accepts_eta:
```

## diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py

```diff
@@ -665,28 +665,30 @@
             latents = latents.to(device)
 
         # scale the initial noise by the standard deviation required by the scheduler
         latents = latents * self.scheduler.init_noise_sigma
         return latents
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py

```diff
@@ -763,28 +763,30 @@
         # get latents
         init_latents = self.scheduler.add_noise(init_latents, noise, timestep)
         latents = init_latents
 
         return latents
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py

```diff
@@ -905,28 +905,30 @@
         timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]
         if hasattr(self.scheduler, "set_begin_index"):
             self.scheduler.set_begin_index(t_start * self.scheduler.order)
 
         return timesteps, num_inference_steps - t_start
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
@@ -1020,15 +1022,15 @@
             width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                 The width in pixels of the generated image.
             padding_mask_crop (`int`, *optional*, defaults to `None`):
                 The size of margin in the crop to be applied to the image and masking. If `None`, no crop is applied to image and mask_image. If
                 `padding_mask_crop` is not `None`, it will first find a rectangular region with the same aspect ration of the image and
                 contains all masked area, and then expand that area based on `padding_mask_crop`. The image and mask_image will then be cropped based on
                 the expanded area before resizing to the original image size for inpainting. This is useful when the masked area is small while the image is large
-                and contain information inreleant for inpainging, such as background.
+                and contain information irrelevant for inpainting, such as background.
             strength (`float`, *optional*, defaults to 1.0):
                 Indicates extent to transform the reference `image`. Must be between 0 and 1. `image` is used as a
                 starting point and more noise is added the higher the `strength`. The number of denoising steps depends
                 on the amount of noise initially added. When `strength` is 1, added noise is maximum and the denoising
                 process runs for the full number of iterations specified in `num_inference_steps`. A value of 1
                 essentially ignores `image`.
             num_inference_steps (`int`, *optional*, defaults to 50):
```

## diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py

```diff
@@ -1297,15 +1297,15 @@
         prompt_embeds: Optional[torch.FloatTensor] = None,
         negative_prompt_embeds: Optional[torch.FloatTensor] = None,
         output_type: Optional[str] = "pil",
         return_dict: bool = True,
         callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
         callback_steps: int = 1,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
-        clip_ckip: int = None,
+        clip_skip: int = None,
     ):
         r"""
         The call function to the pipeline for generation.
 
         Args:
             prompt (`str` or `List[str]`, *optional*):
                 The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.
@@ -1419,15 +1419,15 @@
             device,
             num_images_per_prompt,
             do_classifier_free_guidance,
             negative_prompt,
             prompt_embeds=prompt_embeds,
             negative_prompt_embeds=negative_prompt_embeds,
             lora_scale=text_encoder_lora_scale,
-            clip_skip=clip_ckip,
+            clip_skip=clip_skip,
         )
         # For classifier free guidance, we need to do two forward passes.
         # Here we concatenate the unconditional and text embeddings into a single batch
         # to avoid doing two forward passes
         if do_classifier_free_guidance:
             prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
```

## diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py

```diff
@@ -676,15 +676,15 @@
             prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
 
         # 4. Prepare timesteps
         self.scheduler.set_timesteps(num_inference_steps, device=device)
         timesteps = self.scheduler.timesteps
 
         # 5. Prepare latent variables
-        num_channels_latents = self.unet.in_channels
+        num_channels_latents = self.unet.config.in_channels
         latents = self.prepare_latents(
             batch_size * num_images_per_prompt,
             num_channels_latents,
             height,
             width,
             prompt_embeds.dtype,
             device,
@@ -709,15 +709,15 @@
         _text_embeddings = self.text_encoder(**tokenizer_inputs).pooler_output
         n_objs = len(gligen_boxes)
         # For each entity, described in phrases, is denoted with a bounding box,
         # we represent the location information as (xmin,ymin,xmax,ymax)
         boxes = torch.zeros(max_objs, 4, device=device, dtype=self.text_encoder.dtype)
         boxes[:n_objs] = torch.tensor(gligen_boxes)
         text_embeddings = torch.zeros(
-            max_objs, self.unet.cross_attention_dim, device=device, dtype=self.text_encoder.dtype
+            max_objs, self.unet.config.cross_attention_dim, device=device, dtype=self.text_encoder.dtype
         )
         text_embeddings[:n_objs] = _text_embeddings
         # Generate a mask for each object that is entity described by phrases
         masks = torch.zeros(max_objs, device=device, dtype=self.text_encoder.dtype)
         masks[:n_objs] = 1
 
         repeat_batch = batch_size * num_images_per_prompt
```

## diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py

```diff
@@ -843,15 +843,15 @@
             prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
 
         # 4. Prepare timesteps
         self.scheduler.set_timesteps(num_inference_steps, device=device)
         timesteps = self.scheduler.timesteps
 
         # 5. Prepare latent variables
-        num_channels_latents = self.unet.in_channels
+        num_channels_latents = self.unet.config.in_channels
         latents = self.prepare_latents(
             batch_size * num_images_per_prompt,
             num_channels_latents,
             height,
             width,
             prompt_embeds.dtype,
             device,
```

## diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py

```diff
@@ -640,28 +640,30 @@
             latents = latents.to(device)
 
         # scale the initial noise by the standard deviation required by the scheduler
         latents = latents * self.scheduler.init_noise_sigma
         return latents
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py

```diff
@@ -628,15 +628,15 @@
 
         device = self._execution_device
         # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
         # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
         # corresponds to doing no classifier free guidance.
         do_classifier_free_guidance = guidance_scale > 1.0
         # and `sag_scale` is` `s` of equation (16)
-        # of the self-attentnion guidance paper: https://arxiv.org/pdf/2210.00939.pdf
+        # of the self-attention guidance paper: https://arxiv.org/pdf/2210.00939.pdf
         # `sag_scale = 0` means no self-attention guidance
         do_self_attention_guidance = sag_scale > 0.0
 
         if ip_adapter_image is not None:
             output_hidden_state = False if isinstance(self.unet.encoder_hid_proj, ImageProjection) else True
             image_embeds, negative_image_embeds = self.encode_image(
                 ip_adapter_image, device, num_images_per_prompt, output_hidden_state
@@ -663,15 +663,15 @@
 
         # 4. Prepare timesteps
         self.scheduler.set_timesteps(num_inference_steps, device=device)
         timesteps = self.scheduler.timesteps
 
         if timesteps.dtype not in [torch.int16, torch.int32, torch.int64]:
             raise ValueError(
-                f"{self.__class__.__name__} does not support using a scheduler of type {self.scheduler.__class__.__name__}. Please make sure to use one of 'DDIMScheduler, PNDMScheduler, DDPMScheduler, DEISMultistepScheduler, UniPCMultistepScheduler, DPMSolverMultistepScheduler, DPMSolverSinlgestepScheduler'."
+                f"{self.__class__.__name__} does not support using a scheduler of type {self.scheduler.__class__.__name__}. Please make sure to use one of 'DDIMScheduler, PNDMScheduler, DDPMScheduler, DEISMultistepScheduler, UniPCMultistepScheduler, DPMSolverMultistepScheduler, DPMSolverSinglestepScheduler'."
             )
 
         # 5. Prepare latent variables
         num_channels_latents = self.unet.config.in_channels
         latents = self.prepare_latents(
             batch_size * num_images_per_prompt,
             num_channels_latents,
@@ -719,15 +719,15 @@
                     ).sample
 
                     # perform guidance
                     if do_classifier_free_guidance:
                         noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                         noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
 
-                    # perform self-attention guidance with the stored self-attentnion map
+                    # perform self-attention guidance with the stored self-attention map
                     if do_self_attention_guidance:
                         # classifier-free guidance produces two chunks of attention map
                         # and we only use unconditional one according to equation (25)
                         # in https://arxiv.org/pdf/2210.00939.pdf
                         if do_classifier_free_guidance:
                             # DDIM-like prediction of x0
                             pred_x0 = self.pred_x0(latents, noise_pred_uncond, t)
```

## diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py

```diff
@@ -736,28 +736,30 @@
         # to be in float32 which can save lots of memory
         if use_torch_2_0_or_xformers:
             self.vae.post_quant_conv.to(dtype)
             self.vae.decoder.conv_in.to(dtype)
             self.vae.decoder.mid_block.to(dtype)
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py

```diff
@@ -870,28 +870,30 @@
         # to be in float32 which can save lots of memory
         if use_torch_2_0_or_xformers:
             self.vae.post_quant_conv.to(dtype)
             self.vae.decoder.conv_in.to(dtype)
             self.vae.decoder.mid_block.to(dtype)
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py

```diff
@@ -1106,28 +1106,30 @@
         # to be in float32 which can save lots of memory
         if use_torch_2_0_or_xformers:
             self.vae.post_quant_conv.to(dtype)
             self.vae.decoder.conv_in.to(dtype)
             self.vae.decoder.mid_block.to(dtype)
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
@@ -1253,15 +1255,15 @@
                 [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
                 and checkpoints that are not specifically fine-tuned on low resolutions.
             padding_mask_crop (`int`, *optional*, defaults to `None`):
                 The size of margin in the crop to be applied to the image and masking. If `None`, no crop is applied to image and mask_image. If
                 `padding_mask_crop` is not `None`, it will first find a rectangular region with the same aspect ration of the image and
                 contains all masked area, and then expand that area based on `padding_mask_crop`. The image and mask_image will then be cropped based on
                 the expanded area before resizing to the original image size for inpainting. This is useful when the masked area is small while the image is large
-                and contain information inreleant for inpainging, such as background.
+                and contain information irrelevant for inpainting, such as background.
             strength (`float`, *optional*, defaults to 0.9999):
                 Conceptually, indicates how much to transform the masked portion of the reference `image`. Must be
                 between 0 and 1. `image` will be used as a starting point, adding more noise to it the larger the
                 `strength`. The number of denoising steps depends on the amount of noise initially added. When
                 `strength` is 1, added noise will be maximum and the denoising process will run for the full number of
                 iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores the masked
                 portion of the reference `image`. Note that in the case of `denoising_start` being declared as an
```

## diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py

```diff
@@ -609,28 +609,30 @@
 
             # round down to nearest multiple of `self.adapter.downscale_factor`
             width = (width // self.adapter.downscale_factor) * self.adapter.downscale_factor
 
         return height, width
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py

```diff
@@ -780,28 +780,30 @@
 
             # round down to nearest multiple of `self.adapter.downscale_factor`
             width = (width // self.adapter.downscale_factor) * self.adapter.downscale_factor
 
         return height, width
 
     # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
-    def get_guidance_scale_embedding(self, w, embedding_dim=512, dtype=torch.float32):
+    def get_guidance_scale_embedding(
+        self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32
+    ) -> torch.FloatTensor:
         """
         See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
 
         Args:
-            timesteps (`torch.Tensor`):
-                generate embedding vectors at these timesteps
+            w (`torch.Tensor`):
+                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
             embedding_dim (`int`, *optional*, defaults to 512):
-                dimension of the embeddings to generate
-            dtype:
-                data type of the generated embeddings
+                Dimension of the embeddings to generate.
+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
+                Data type of the generated embeddings.
 
         Returns:
-            `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
+            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
         """
         assert len(w.shape) == 1
         w = w * 1000.0
 
         half_dim = embedding_dim // 2
         emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
         emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
```

## diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py

```diff
@@ -571,16 +571,16 @@
             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                 A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make
                 generation deterministic.
             latents (`torch.FloatTensor`, *optional*):
                 Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for video
                 generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                 tensor is generated by sampling using the supplied random `generator`.
-            output_type (`str`, *optional*, defaults to `"numpy"`):
-                The output format of the generated video. Choose between `"latent"` and `"numpy"`.
+            output_type (`str`, *optional*, defaults to `"np"`):
+                The output format of the generated video. Choose between `"latent"` and `"np"`.
             return_dict (`bool`, *optional*, defaults to `True`):
                 Whether or not to return a
                 [`~pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput`] instead of
                 a plain tuple.
             callback (`Callable`, *optional*):
                 A function that calls every `callback_steps` steps during inference. The function is called with the
                 following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.
```

## diffusers/schedulers/scheduling_consistency_decoder.py

```diff
@@ -41,15 +41,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_consistency_models.py

```diff
@@ -100,15 +100,15 @@
         self._step_index = None
         self._begin_index = None
         self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
@@ -229,15 +229,15 @@
         # See https://github.com/openai/consistency_models/blob/main/cm/karras_diffusion.py#L675
         num_train_timesteps = self.config.num_train_timesteps
         ramp = timesteps[::-1].copy()
         ramp = ramp / (num_train_timesteps - 1)
         sigmas = self._convert_to_karras(ramp)
         timesteps = self.sigma_to_t(sigmas)
 
-        sigmas = np.concatenate([sigmas, [self.sigma_min]]).astype(np.float32)
+        sigmas = np.concatenate([sigmas, [self.config.sigma_min]]).astype(np.float32)
         self.sigmas = torch.from_numpy(sigmas).to(device=device)
 
         if str(device).startswith("mps"):
             # mps does not support float64
             self.timesteps = torch.from_numpy(timesteps).to(device, dtype=torch.float32)
         else:
             self.timesteps = torch.from_numpy(timesteps).to(device=device)
```

## diffusers/schedulers/scheduling_ddim.py

```diff
@@ -78,15 +78,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_ddim_inverse.py

```diff
@@ -76,15 +76,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_ddim_parallel.py

```diff
@@ -78,15 +78,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_ddpm.py

```diff
@@ -75,15 +75,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_ddpm_parallel.py

```diff
@@ -77,15 +77,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_ddpm_wuerstchen.py

```diff
@@ -71,15 +71,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_deis_multistep.py

```diff
@@ -57,15 +57,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -187,15 +187,15 @@
         self._step_index = None
         self._begin_index = None
         self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_dpmsolver_multistep.py

```diff
@@ -57,15 +57,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -278,15 +278,15 @@
         self._step_index = None
         self._begin_index = None
         self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py

```diff
@@ -57,15 +57,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -213,15 +213,15 @@
         self._step_index = None
         self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication
         self.use_karras_sigmas = use_karras_sigmas
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     def set_timesteps(self, num_inference_steps: int = None, device: Union[str, torch.device] = None):
         """
         Sets the discrete timesteps used for the diffusion chain (to be run before inference).
 
@@ -229,15 +229,15 @@
             num_inference_steps (`int`):
                 The number of diffusion steps used when generating samples with a pre-trained model.
             device (`str` or `torch.device`, *optional*):
                 The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
         """
         # Clipping the minimum of all lambda(t) for numerical stability.
         # This is critical for cosine (squaredcos_cap_v2) noise schedule.
-        clipped_idx = torch.searchsorted(torch.flip(self.lambda_t, [0]), self.lambda_min_clipped).item()
+        clipped_idx = torch.searchsorted(torch.flip(self.lambda_t, [0]), self.config.lambda_min_clipped).item()
         self.noisiest_timestep = self.config.num_train_timesteps - 1 - clipped_idx
 
         # "linspace", "leading", "trailing" corresponds to annotation of Table 2. of https://arxiv.org/abs/2305.08891
         if self.config.timestep_spacing == "linspace":
             timesteps = (
                 np.linspace(0, self.noisiest_timestep, num_inference_steps + 1).round()[:-1].copy().astype(np.int64)
             )
```

## diffusers/schedulers/scheduling_dpmsolver_sde.py

```diff
@@ -106,15 +106,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -229,15 +229,15 @@
             return self.sigmas.max()
 
         return (self.sigmas.max() ** 2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
@@ -321,15 +321,15 @@
                 f"{self.config.timestep_spacing} is not supported. Please make sure to choose one of 'linspace', 'leading' or 'trailing'."
             )
 
         sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)
         log_sigmas = np.log(sigmas)
         sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)
 
-        if self.use_karras_sigmas:
+        if self.config.use_karras_sigmas:
             sigmas = self._convert_to_karras(in_sigmas=sigmas)
             timesteps = np.array([self._sigma_to_t(sigma, log_sigmas) for sigma in sigmas])
 
         second_order_timesteps = self._second_order_timesteps(sigmas, log_sigmas)
 
         sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)
         sigmas = torch.from_numpy(sigmas).to(device=device)
```

## diffusers/schedulers/scheduling_dpmsolver_singlestep.py

```diff
@@ -59,15 +59,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -248,15 +248,15 @@
             elif order == 1:
                 orders = [1] * steps
         return orders
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_edm_dpmsolver_multistep.py

```diff
@@ -139,15 +139,15 @@
     def init_noise_sigma(self):
         # standard deviation of the initial noise distribution
         return (self.config.sigma_max**2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_edm_euler.py

```diff
@@ -107,15 +107,15 @@
     def init_noise_sigma(self):
         # standard deviation of the initial noise distribution
         return (self.config.sigma_max**2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_euler_ancestral_discrete.py

```diff
@@ -78,15 +78,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -224,15 +224,15 @@
             return self.sigmas.max()
 
         return (self.sigmas.max() ** 2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_euler_discrete.py

```diff
@@ -78,15 +78,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -244,15 +244,15 @@
             return max_sigma
 
         return (max_sigma**2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
@@ -339,15 +339,15 @@
             sigmas = torch.linspace(np.log(sigmas[-1]), np.log(sigmas[0]), num_inference_steps + 1).exp().numpy()
         else:
             raise ValueError(
                 f"{self.config.interpolation_type} is not implemented. Please specify interpolation_type to either"
                 " 'linear' or 'log_linear'"
             )
 
-        if self.use_karras_sigmas:
+        if self.config.use_karras_sigmas:
             sigmas = self._convert_to_karras(in_sigmas=sigmas, num_inference_steps=self.num_inference_steps)
             timesteps = np.array([self._sigma_to_t(sigma, log_sigmas) for sigma in sigmas])
 
         sigmas = torch.from_numpy(sigmas).to(dtype=torch.float32, device=device)
 
         # TODO: Support the full EDM scalings for all prediction types and timestep types
         if self.config.timestep_type == "continuous" and self.config.prediction_type == "v_prediction":
```

## diffusers/schedulers/scheduling_heun_discrete.py

```diff
@@ -53,15 +53,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -170,15 +170,15 @@
             return self.sigmas.max()
 
         return (self.sigmas.max() ** 2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_ipndm.py

```diff
@@ -57,15 +57,15 @@
         self.ets = []
         self._step_index = None
         self._begin_index = None
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py

```diff
@@ -54,15 +54,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -147,15 +147,15 @@
             return self.sigmas.max()
 
         return (self.sigmas.max() ** 2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_k_dpm_2_discrete.py

```diff
@@ -53,15 +53,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -147,15 +147,15 @@
             return self.sigmas.max()
 
         return (self.sigmas.max() ** 2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_lcm.py

```diff
@@ -80,15 +80,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_lms_discrete.py

```diff
@@ -75,15 +75,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -176,15 +176,15 @@
             return self.sigmas.max()
 
         return (self.sigmas.max() ** 2 + 1) ** 0.5
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
@@ -284,15 +284,15 @@
                 f"{self.config.timestep_spacing} is not supported. Please make sure to choose one of 'linspace', 'leading' or 'trailing'."
             )
 
         sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)
         log_sigmas = np.log(sigmas)
         sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)
 
-        if self.use_karras_sigmas:
+        if self.config.use_karras_sigmas:
             sigmas = self._convert_to_karras(in_sigmas=sigmas)
             timesteps = np.array([self._sigma_to_t(sigma, log_sigmas) for sigma in sigmas])
 
         sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)
 
         self.sigmas = torch.from_numpy(sigmas).to(device=device)
         self.timesteps = torch.from_numpy(timesteps).to(device=device)
```

## diffusers/schedulers/scheduling_pndm.py

```diff
@@ -55,15 +55,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_repaint.py

```diff
@@ -74,15 +74,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_sasolver.py

```diff
@@ -58,15 +58,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -212,15 +212,15 @@
         self._step_index = None
         self._begin_index = None
         self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## diffusers/schedulers/scheduling_tcd.py

```diff
@@ -79,15 +79,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -303,14 +303,15 @@
         current timestep.
 
         Args:
             sample (`torch.FloatTensor`):
                 The input sample.
             timestep (`int`, *optional*):
                 The current timestep in the diffusion chain.
+
         Returns:
             `torch.FloatTensor`:
                 A scaled input sample.
         """
         return sample
 
     # Copied from diffusers.schedulers.scheduling_ddim.DDIMScheduler._get_variance
@@ -360,15 +361,15 @@
 
     def set_timesteps(
         self,
         num_inference_steps: Optional[int] = None,
         device: Union[str, torch.device] = None,
         original_inference_steps: Optional[int] = None,
         timesteps: Optional[List[int]] = None,
-        strength: int = 1.0,
+        strength: float = 1.0,
     ):
         """
         Sets the discrete timesteps used for the diffusion chain (to be run before inference).
 
         Args:
             num_inference_steps (`int`, *optional*):
                 The number of diffusion steps used when generating samples with a pre-trained model. If used,
@@ -380,14 +381,16 @@
                 schedule (which is different from the standard `diffusers` implementation). We will then take
                 `num_inference_steps` timesteps from this schedule, evenly spaced in terms of indices, and use that as
                 our final timestep schedule. If not set, this will default to the `original_inference_steps` attribute.
             timesteps (`List[int]`, *optional*):
                 Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default
                 timestep spacing strategy of equal spacing between timesteps on the training/distillation timestep
                 schedule is used. If `timesteps` is passed, `num_inference_steps` must be `None`.
+            strength (`float`, *optional*, defaults to 1.0):
+                Used to determine the number of timesteps used for inference when using img2img, inpaint, etc.
         """
         # 0. Check inputs
         if num_inference_steps is None and timesteps is None:
             raise ValueError("Must pass exactly one of `num_inference_steps` or `custom_timesteps`.")
 
         if num_inference_steps is not None and timesteps is not None:
             raise ValueError("Can only pass one of `num_inference_steps` or `custom_timesteps`.")
@@ -620,22 +623,26 @@
         self._step_index += 1
 
         if not return_dict:
             return (prev_sample, pred_noised_sample)
 
         return TCDSchedulerOutput(prev_sample=prev_sample, pred_noised_sample=pred_noised_sample)
 
+    # Copied from diffusers.schedulers.scheduling_ddpm.DDPMScheduler.add_noise
     def add_noise(
         self,
         original_samples: torch.FloatTensor,
         noise: torch.FloatTensor,
         timesteps: torch.IntTensor,
     ) -> torch.FloatTensor:
         # Make sure alphas_cumprod and timestep have same device and dtype as original_samples
-        alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)
+        # Move the self.alphas_cumprod to device to avoid redundant CPU to GPU data movement
+        # for the subsequent add_noise calls
+        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device)
+        alphas_cumprod = self.alphas_cumprod.to(dtype=original_samples.dtype)
         timesteps = timesteps.to(original_samples.device)
 
         sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5
         sqrt_alpha_prod = sqrt_alpha_prod.flatten()
         while len(sqrt_alpha_prod.shape) < len(original_samples.shape):
             sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)
 
@@ -643,19 +650,21 @@
         sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()
         while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):
             sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)
 
         noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise
         return noisy_samples
 
+    # Copied from diffusers.schedulers.scheduling_ddpm.DDPMScheduler.get_velocity
     def get_velocity(
         self, sample: torch.FloatTensor, noise: torch.FloatTensor, timesteps: torch.IntTensor
     ) -> torch.FloatTensor:
         # Make sure alphas_cumprod and timestep have same device and dtype as sample
-        alphas_cumprod = self.alphas_cumprod.to(device=sample.device, dtype=sample.dtype)
+        self.alphas_cumprod = self.alphas_cumprod.to(device=sample.device)
+        alphas_cumprod = self.alphas_cumprod.to(dtype=sample.dtype)
         timesteps = timesteps.to(sample.device)
 
         sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5
         sqrt_alpha_prod = sqrt_alpha_prod.flatten()
         while len(sqrt_alpha_prod.shape) < len(sample.shape):
             sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)
 
@@ -666,14 +675,15 @@
 
         velocity = sqrt_alpha_prod * noise - sqrt_one_minus_alpha_prod * sample
         return velocity
 
     def __len__(self):
         return self.config.num_train_timesteps
 
+    # Copied from diffusers.schedulers.scheduling_ddpm.DDPMScheduler.previous_timestep
     def previous_timestep(self, timestep):
         if self.custom_timesteps:
             index = (self.timesteps == timestep).nonzero(as_tuple=True)[0][0]
             if index == self.timesteps.shape[0] - 1:
                 prev_t = torch.tensor(-1)
             else:
                 prev_t = self.timesteps[index + 1]
```

## diffusers/schedulers/scheduling_unclip.py

```diff
@@ -75,15 +75,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
```

## diffusers/schedulers/scheduling_unipc_multistep.py

```diff
@@ -57,15 +57,15 @@
 
     elif alpha_transform_type == "exp":
 
         def alpha_bar_fn(t):
             return math.exp(t * -12.0)
 
     else:
-        raise ValueError(f"Unsupported alpha_tranform_type: {alpha_transform_type}")
+        raise ValueError(f"Unsupported alpha_transform_type: {alpha_transform_type}")
 
     betas = []
     for i in range(num_diffusion_timesteps):
         t1 = i / num_diffusion_timesteps
         t2 = (i + 1) / num_diffusion_timesteps
         betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))
     return torch.tensor(betas, dtype=torch.float32)
@@ -198,15 +198,15 @@
         self._step_index = None
         self._begin_index = None
         self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication
 
     @property
     def step_index(self):
         """
-        The index counter for current timestep. It will increae 1 after each scheduler step.
+        The index counter for current timestep. It will increase 1 after each scheduler step.
         """
         return self._step_index
 
     @property
     def begin_index(self):
         """
         The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
```

## Comparing `souJpg_diffusers-0.27.2.dist-info/LICENSE` & `souJpg_diffusers-0.28.0.dev1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `souJpg_diffusers-0.27.2.dist-info/METADATA` & `souJpg_diffusers-0.28.0.dev1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: souJpg-diffusers
-Version: 0.27.2
+Version: 0.28.0.dev1
 Summary: State-of-the-art diffusion in PyTorch and JAX.
 Home-page: https://github.com/huggingface/diffusers
 Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/diffusers/graphs/contributors)
 Author-email: patrick@huggingface.co
 License: Apache 2.0 License
 Keywords: deep learning diffusion jax pytorch stable diffusion audioldm
 Classifier: Development Status :: 5 - Production/Stable
@@ -175,15 +175,15 @@
 
 ### Apple Silicon (M1/M2) support
 
 Please refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide.
 
 ## Quickstart
 
-Generating outputs is super easy with 🤗 Diffusers. To generate an image from text, use the `from_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 19000+ checkpoints):
+Generating outputs is super easy with 🤗 Diffusers. To generate an image from text, use the `from_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 22000+ checkpoints):
 
 ```python
 from diffusers import DiffusionPipeline
 import torch
 
 pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
 pipeline.to("cuda")
@@ -317,15 +317,15 @@
 - https://github.com/apple/ml-stable-diffusion
 - https://github.com/Sanster/lama-cleaner
 - https://github.com/IDEA-Research/Grounded-Segment-Anything
 - https://github.com/ashawkey/stable-dreamfusion
 - https://github.com/deep-floyd/IF
 - https://github.com/bentoml/BentoML
 - https://github.com/bmaltais/kohya_ss
-- +8000 other amazing GitHub repositories 💪
+- +9000 other amazing GitHub repositories 💪
 
 Thank you for using us ❤️.
 
 ## Credits
 
 This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:
```

## Comparing `souJpg_diffusers-0.27.2.dist-info/RECORD` & `souJpg_diffusers-0.28.0.dev1.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-diffusers/__init__.py,sha256=VBEUBZoiU245pY8e-ZqkKOoWpBy2xKg8KjUO9I3B_v4,28672
+diffusers/__init__.py,sha256=mvl3guMa1y37rQBg9wYYOu3pc2hfbDbopzI0jmrILsE,28677
 diffusers/configuration_utils.py,sha256=LiREW2fL18a184byyg3mZBuIm8IPEQAwrLJUmWtcGkM,31927
 diffusers/dependency_versions_check.py,sha256=J_ZAEhVN6uLWAOUZCJrcGJ7PYxUek4f_nwGTFM7LTk8,1271
 diffusers/dependency_versions_table.py,sha256=P-wcoeAnOJBNuNj175VsgHxYyeQlaL1ocF3mkUgS68s,1485
 diffusers/image_processor.py,sha256=mhEStv4t4URzuB6XX80w8zODZuN3JHKy6rb0KHw6wJk,40698
 diffusers/optimization.py,sha256=jpSY6io5iZ52aOniErG75M2nVIyAzq26ebmc-Z-eKIc,14743
 diffusers/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 diffusers/training_utils.py,sha256=LkhtcLztTTecMTCzqMoNm29wCbGoPAD8Hf3VgKEmsAY,18932
@@ -14,28 +14,28 @@
 diffusers/experimental/rl/__init__.py,sha256=Gcoznw9rYjfMvswH0seXekKYDAAN1YXXxZ-RWMdzvrE,57
 diffusers/experimental/rl/value_guided_sampling.py,sha256=gnUDVNx5nIVJDWxhHBlga4j7VQTxSTkUI1QaCnpiWAM,6033
 diffusers/loaders/__init__.py,sha256=iy4fYB0jCS069y8Fe1dO3jufAQ_OZ5f2e54sQ1tlALc,3705
 diffusers/loaders/autoencoder.py,sha256=b5Z9UnpaYgbYXRUk3dpw127fKhWKGkh6gWpuI9L7Spk,7664
 diffusers/loaders/controlnet.py,sha256=9B12Gv7fMS7PPk_U4eKycVmkTpxnEhX5e47SXnzKoAM,7107
 diffusers/loaders/ip_adapter.py,sha256=6hm-cuzkSaBUZqS4e_hOnw-QRxv5CQ6oDBR2muIy5SA,14330
 diffusers/loaders/lora.py,sha256=fUvdQFVTJT0kxHDsFKhMOt7F7ylXL2jxI1k-aZOTloc,62835
-diffusers/loaders/lora_conversion_utils.py,sha256=ogNfjlxakH5z74e5Jtmid-8K0E_-_8_Nb9tBumvJkzA,14365
+diffusers/loaders/lora_conversion_utils.py,sha256=ftQBjBq38umc9QFxZ2s6xkJjZIN9OFINbT8SWr8sF48,12528
 diffusers/loaders/peft.py,sha256=4zQdvh7I1fmA721cIJlxGV0lcKmtGsEVkLfoSwxSn7c,8385
 diffusers/loaders/single_file.py,sha256=JM7-0nO7Bx6oOxS-PRkVF1l6ZQlaKxxg2Dxra2DY5u4,13517
 diffusers/loaders/single_file_utils.py,sha256=XY4ZXMk93vZz1zGGP6emP3h187hRE8aQkX0LKaew3h4,67296
 diffusers/loaders/textual_inversion.py,sha256=Q7vh8HuPTk4bkE0JLXfL2Jh6Em-w2e9-mnK9ScrZjpk,26563
 diffusers/loaders/unet.py,sha256=lsMRxkIejZ5eSN0I8R2rHvZueu68NTcq8V1LrWw0bHw,46429
 diffusers/loaders/utils.py,sha256=IgI-rwNZ-xRx_jIgp61xHkeLAvqm3FSpJ674s5LzE_k,2423
 diffusers/models/__init__.py,sha256=BJtenj7qyEPdpFnGfg1uahxG7UvB8ao7rqIK4oXRrnE,4307
 diffusers/models/activations.py,sha256=6q9MDYWsxSD3RxodJD05gxEBuXXpcAoyYvPzA54u0UM,4490
 diffusers/models/adapter.py,sha256=XuVoUbhLkPEOeClqYiTm8KGMQrXgn2WZU3I73-jkcew,24723
-diffusers/models/attention.py,sha256=MnW_N1U5E3J5Re-y4HKVfs1x6UZqgeKexzznVCudUAM,27962
+diffusers/models/attention.py,sha256=SPQHxzN77Tj4zAZEdklpS5ujYgCuyvnQxhNONUaDCc0,27961
 diffusers/models/attention_flax.py,sha256=Ju2KJCqsx_LIiz0i1pBcek7RMKTmVOIF4SvEcOqgJ1c,20250
-diffusers/models/attention_processor.py,sha256=-AKF0vKW1NjWXEpB4NQi9utE_ODMBa3KYPyuAWdmAlo,108049
-diffusers/models/controlnet.py,sha256=53XHrhr--gEmKv5XijPg5ri1EcOgtJJfMGqnAq2lbIk,43152
+diffusers/models/attention_processor.py,sha256=sTfirOFuMzamceNonaj7BtPdjiaVR9oiFEhIs7o6Du0,108051
+diffusers/models/controlnet.py,sha256=JFcibKELJJmovgEp82QVW9mp2fb0pZLZ2IcQ5Rm_Vw8,43567
 diffusers/models/controlnet_flax.py,sha256=fHFtk18NGmS6LQd_T7Ueo9b6WyzJOzBO6oLWBT0l4hg,16710
 diffusers/models/downsampling.py,sha256=pDBx7tR3hGyz9RMlxyCj5RGo_GGMQuGSBMHV2onWsWs,12490
 diffusers/models/dual_transformer_2d.py,sha256=kcyBXJB16ge1TVuBHeCb_vySJInCDn_7X304ztGT5vI,1105
 diffusers/models/embeddings.py,sha256=VN5pU89HAjt8S_zirRS2thD9oIaQZ1jyUgT-YliA4-Q,34635
 diffusers/models/embeddings_flax.py,sha256=hNN63qtv9Ht6VEeidrIJdrdKWm0o8mumcgzxMNaR2H0,3445
 diffusers/models/lora.py,sha256=7LbI7bj8yk9GptoOnOLrhzarFcVSQX47LuGoZ1MBK0A,18829
 diffusers/models/modeling_flax_pytorch_utils.py,sha256=2mIT_b1K_ehzhh5zRCy84DMxkOG8U4cOIO2ZL6QYZ1E,5332
@@ -45,15 +45,15 @@
 diffusers/models/modeling_utils.py,sha256=uKbnNT5UUAnNr71Z_Rx2PYd9ePk1BzUNRAOAgvXEC3g,48262
 diffusers/models/normalization.py,sha256=u_kJqs_MTuXovGvSoBQ1SJvj6QY2rrHXG9ZVktsF17g,9508
 diffusers/models/prior_transformer.py,sha256=oihNB0R9OM2WGvPMLvDbUE3M--bcsJ6B_JwM6dOqCeo,877
 diffusers/models/resnet.py,sha256=m7TN51pi8S1kZD_fGkJ7q5cz1_OE5EK_gev73afA1f0,32398
 diffusers/models/resnet_flax.py,sha256=tqRZQCZIq7NlXex3eGldyhRpZjr_EXWl1l2eVflFV7c,4021
 diffusers/models/t5_film_transformer.py,sha256=HDoaHS3oLNbNzagVepheFHgvfvcDoKiyKpiPoffww0Q,4196
 diffusers/models/transformer_2d.py,sha256=ufNZeXmj1EjLWqUVevfFIQpcux28RSyFpYIltOQ-7u4,1492
-diffusers/models/transformer_temporal.py,sha256=wzv5GKYO4AAwNsyndubHKAyMAYpA18zqnyGh_B51aa4,2082
+diffusers/models/transformer_temporal.py,sha256=cm1M_o-OBJE_7Gb4uXXrLE865jyWXeSvFfUceLBOmLg,2085
 diffusers/models/unet_1d.py,sha256=xMTvJORijQ7Qu9ujmx3Y2gX74CFke3jTITYwIpRhLvU,1323
 diffusers/models/unet_1d_blocks.py,sha256=vfF0ydu_VEsNCzxVe11EpVnRjgyJCSbcB2DsrUhtZ1k,9963
 diffusers/models/unet_2d.py,sha256=Qkp6VR2DjeTyt0F3C9x4AI4vd5Tu-S-6WhQUtAOfkwg,1324
 diffusers/models/unet_2d_blocks.py,sha256=8Y7d1RaKm7sblh76b-kiKhywkljzqPxgyy9NLzmAShQ,18582
 diffusers/models/unet_2d_condition.py,sha256=A8lUVtTaZWLVySQlVzADJvQC1DBtP53t7i1qwIV0pvo,1480
 diffusers/models/upsampling.py,sha256=u4XJP5y_LSiw3Locj8y7RdDOvTHtfmHlNWx5Y5gB6tQ,16731
 diffusers/models/vae_flax.py,sha256=H-fZdwSllq2TPPm2wR2W5bN0v7zRIXP3mLpLBbQI7Rg,31942
@@ -65,42 +65,42 @@
 diffusers/models/autoencoders/autoencoder_tiny.py,sha256=ib7B-UaM5KUrqEqG6oJiHqRB6VREJNjpeg2_e5hnhy0,15991
 diffusers/models/autoencoders/consistency_decoder_vae.py,sha256=RHt9IgtTRh4G5VStLfweut7fdAVLPKHOWjIxAM0sDuU,18655
 diffusers/models/autoencoders/vae.py,sha256=ivXSxHVOfwuzIDHv-21p2aLVCnG7yjKFuNc__Izob2Q,36674
 diffusers/models/transformers/__init__.py,sha256=tdzJmrQQY8uCOSepCUQSaU81tB6OfLRVgKX4MRrkbrg,344
 diffusers/models/transformers/dual_transformer_2d.py,sha256=8hVyS0dLpsmlJmCT3K2Bk2ik_SvUcoh1zdPEZSrINmM,7665
 diffusers/models/transformers/prior_transformer.py,sha256=fJd8AEmuEBacCwukHYOXDVSFG6PEKI0ev_cJqUlAUvI,17348
 diffusers/models/transformers/t5_film_transformer.py,sha256=vCaG0vr5zvaBCHIZk5_095A2mjB7QZ32gHYJ_R4cG7g,16163
-diffusers/models/transformers/transformer_2d.py,sha256=RDV-KLu8ges2a_sAbx9mSyiRmAXBqu9Jxmyr7-bv8yM,23855
+diffusers/models/transformers/transformer_2d.py,sha256=QkdqIHFwKgDw4BXJ5pLK0Ms0VFkzyXV8v9Hs6Wa59vI,23855
 diffusers/models/transformers/transformer_temporal.py,sha256=Pqu26_EBaEhAPiRfq0kzCGbUaSqqm3O_xPvvbrRcXhU,16821
 diffusers/models/unets/__init__.py,sha256=srYFA7zEcDY7LxyUB2jz3TdRgsLz8elrWCpT6Y4YXuU,695
 diffusers/models/unets/unet_1d.py,sha256=wygq61UscCi66RaWJds1L6rwM2pm7AR2SxJAPLuX_Zc,10794
 diffusers/models/unets/unet_1d_blocks.py,sha256=dS9aNLkQxTUNb0TUaXyyyeUFuNY2gusLY4r5TR0cw6c,27093
 diffusers/models/unets/unet_2d.py,sha256=1n6idSaeb99NFAjUYmg2sVTfdE_g--3TYHazjlWUEl0,16581
-diffusers/models/unets/unet_2d_blocks.py,sha256=dpMrkQPJuBY2Ax7vYQUJumK9OOrT5al_IxkBIuK2Y3A,147891
+diffusers/models/unets/unet_2d_blocks.py,sha256=6V-03CB7v5bGdNHtSJurtj5B3ZUlrxHoEcQ2kQT4wxc,147882
 diffusers/models/unets/unet_2d_blocks_flax.py,sha256=k6IGsqPXIm_WczW_HQzVqlu0UuKdmeC7JQGBk21gTVM,15572
-diffusers/models/unets/unet_2d_condition.py,sha256=omXUwCuhQfjv1afejFqfO7XxOPiljXMk0F8wZHz_UrY,67039
+diffusers/models/unets/unet_2d_condition.py,sha256=TJ4QzakIgEZjVkbA5t2z-PdXJCwRMJpfbyB831ni2FU,67042
 diffusers/models/unets/unet_2d_condition_flax.py,sha256=foKH3jZM6iRZpt2E1uTclvNp9du8tAmkrSVyOmav4bI,22269
-diffusers/models/unets/unet_3d_blocks.py,sha256=cyUbQjEl-5hPupcgD6FlL9JTn_Mv9wLcVyM_OJ8c7TM,89511
+diffusers/models/unets/unet_3d_blocks.py,sha256=9SD8U8UB9U0coJx6zl-VUITSP0ATazWPHTxtUzZzilk,89508
 diffusers/models/unets/unet_3d_condition.py,sha256=rSXChafZKifbDPPo4Cyam_RyCab52MLexIGpO94w7qQ,34691
 diffusers/models/unets/unet_i2vgen_xl.py,sha256=AQm9aBqynct8mzN_fBQoHfcPok-EoaOsyz_YrH4S2SE,32728
 diffusers/models/unets/unet_kandinsky3.py,sha256=RkCets7EQZqrxuG9_ubezgQOfWdOCnc3pnb2XSDSBE4,20758
 diffusers/models/unets/unet_motion_model.py,sha256=3Uz9yQgHPiGi4rdnPOGXIRtPJAOL3swVsaNFwFiphKw,42225
 diffusers/models/unets/unet_spatio_temporal_condition.py,sha256=IQ3VPTFEKvrI5rdOarXcgck1BgsTbErSPmfyYC7B1kg,22104
-diffusers/models/unets/unet_stable_cascade.py,sha256=_sZvlWCCsRxxnhZB6Osr7qCDYrMYl1qzsz73si38DTM,28254
+diffusers/models/unets/unet_stable_cascade.py,sha256=vXhPG8y1KAf1TzSxvs_beGpyqtgfpJ8_Ya9PXKVKwtA,28462
 diffusers/models/unets/uvit_2d.py,sha256=ScLWI09Wref-vU25gWYao4DojlfGxMRz7cmCQUKV01A,17338
 diffusers/pipelines/__init__.py,sha256=cPrPShXX6grj1JF6vKG7F64NePrUV6mC0pYE_QZrGnE,22315
-diffusers/pipelines/auto_pipeline.py,sha256=rsGGmwrgKWm4Ax0vFgxBtTpcwN87aVB32muvND3mHiU,50268
+diffusers/pipelines/auto_pipeline.py,sha256=buKro99eNHlXr5JM64rnUEd4uAhD2hANgIKmeXy3cfw,50459
 diffusers/pipelines/free_init_utils.py,sha256=KiLxHZHZrXnJXTadasPUfwtqwIZAWFVicoYshGmr_JA,7564
 diffusers/pipelines/onnx_utils.py,sha256=6TK_wddhFsKqPejOrAHL-cavB45j30Sd8bMOfJGprms,8329
 diffusers/pipelines/pipeline_flax_utils.py,sha256=E8pk4PitJrWGwnloWxUsqDZwqt3hUN3wc9FhOYovfbM,27479
 diffusers/pipelines/pipeline_loading_utils.py,sha256=DUr6niS22qWFaWfE2qhaAI3gjhkbJjJpHHOyw0umEYI,20613
 diffusers/pipelines/pipeline_utils.py,sha256=WdXz2a6HRETVaT99vE8Zh4iewsM0FXn_6ssvT6KeK54,87939
 diffusers/pipelines/amused/__init__.py,sha256=pzqLeLosNQ29prMLhTxvPpmoIDPB3OFMQMlErOIRkmI,1793
 diffusers/pipelines/amused/pipeline_amused.py,sha256=vJH1JQI8YXi7_IeNSBxGJw7QjDLUWCFcPEahtm8fblo,15652
-diffusers/pipelines/amused/pipeline_amused_img2img.py,sha256=bsGz2FkAG9IUK7g5CKbrL38j3Ec7sNODaxU9PP1cz_M,17109
+diffusers/pipelines/amused/pipeline_amused_img2img.py,sha256=l6mCohOcuMW0xxBG3A-N2kyJlOZ6snWjYKSLK7Mg2fQ,17110
 diffusers/pipelines/amused/pipeline_amused_inpaint.py,sha256=XVOg0jgJLBp9euW65WJdfp-hhkXJzWPRCEC5tiED-Bc,18806
 diffusers/pipelines/animatediff/__init__.py,sha256=u0e7GhmyU8OIhokFGN-492ovi4L7ECBAxwLBDdrVvmY,1584
 diffusers/pipelines/animatediff/pipeline_animatediff.py,sha256=zF51umGYsKJzrmCZ9dsANnmlccitZ51lpsNPUUYMRD8,41491
 diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py,sha256=xjLCO1C0QDyi8R5u8hmpT5J8Hbm4EtJizDsyGZefez4,48805
 diffusers/pipelines/animatediff/pipeline_output.py,sha256=o8bKHfibKH79Va7qjspBCUfnSsfSlWQJTSH_tWNgQNM,717
 diffusers/pipelines/audioldm/__init__.py,sha256=HMUjKqEf7OAtgIeV2CQoGIoDE6oY7b26N55yn4qCIpU,1419
 diffusers/pipelines/audioldm/pipeline_audioldm.py,sha256=ozq1aCwIbdY5l94GcwZ3UieMfqdCEj-HFJZlKYhZjcQ,26054
@@ -112,34 +112,34 @@
 diffusers/pipelines/blip_diffusion/modeling_blip2.py,sha256=iMX2Yg48BI5_DLi9ImHIerHWvAK0aKUDyS971yV-iuw,27308
 diffusers/pipelines/blip_diffusion/modeling_ctx_clip.py,sha256=Yl1pTY8vlL7r2lfN6H9kT6DfZxndys4-BZhLSyZ22hM,9007
 diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py,sha256=op6137oMNzj3TB8GzWlJ2_Vx3K1bsQ1czLy3g0qZeTE,15011
 diffusers/pipelines/consistency_models/__init__.py,sha256=q_nrLK9DH0_kLcLmRIvgvLP-vDVwloC3lBus776596c,484
 diffusers/pipelines/consistency_models/pipeline_consistency_models.py,sha256=kcd4lv8ln4JWFzRdGEUImYbcTk7cQzVnDOfle-Lh1Hg,12393
 diffusers/pipelines/controlnet/__init__.py,sha256=V5lvvD6DALNY3InsdwVmRz65_ZlWM-mPoATmu-yxCtk,3483
 diffusers/pipelines/controlnet/multicontrolnet.py,sha256=BzfW0-0b85lvbMm2aZD5DwOhRUHDQ261_dfXJPTqihI,9510
-diffusers/pipelines/controlnet/pipeline_controlnet.py,sha256=i9ZwTHbVIWRSflysS_jNinAPnOFMYPvM4d0_RDej5rg,67128
+diffusers/pipelines/controlnet/pipeline_controlnet.py,sha256=9vFydLPsZ2spVoAipUmeU8cl86GhhVKP9O9V_zEy3WU,67300
 diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py,sha256=BSderBzJ6exY7nNE4Rw3JVP5zsulNgwqboZuGJ5_aa4,17373
 diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py,sha256=_YEGGEI4olKQ6e7nFmnes5YPjFhLrCEOeHIbFHO5FOY,66454
-diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py,sha256=kmwG2Z9xeiHRJ_PSjXNrgk-V3AObbLqJxCj2OEv9k3s,81386
-diffusers/pipelines/controlnet/pipeline_controlnet_inpaint_sd_xl.py,sha256=CyeRCrGDX1UPM74SxyhXSewGxcaZIezEY8lKd5K7IDY,93308
-diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py,sha256=I9fOcXsLfeVuwcFJ4YSkaVskUMFeIuCpd0_9IgWPB3U,77396
+diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py,sha256=fS3LUdWETtKRTaWgs7xmxJiKIZqbxfVt_XOPjJYMLxQ,81387
+diffusers/pipelines/controlnet/pipeline_controlnet_inpaint_sd_xl.py,sha256=kJNdl_DdY0ivdBPgL9ol3dcD3dr9ekykacpLFM3mjaY,93309
+diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py,sha256=paQkoS--ICZP55C0nh8c_AKkyd6uS3WQ1LTMGygeH84,77568
 diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py,sha256=JFU8piqj0ymkkgZHmOpo4zHIzLv0hHVRgfq4IiPuRA0,84855
 diffusers/pipelines/controlnet/pipeline_flax_controlnet.py,sha256=1spdtZVcZJFmMXoTRn11Ag7SzeVnRFMLN9pXtEQJehQ,22663
 diffusers/pipelines/dance_diffusion/__init__.py,sha256=SOwr8mpuw34oKEUuy4uVLlhjfHuLRCP0kpMjoSPXADU,453
 diffusers/pipelines/dance_diffusion/pipeline_dance_diffusion.py,sha256=qwHC2RLpZL_37Z39nx7BpIoESJurLLrf3SNMCNoqeqI,6322
 diffusers/pipelines/ddim/__init__.py,sha256=-zCVlqBSKWZdwY5HSsoiRT4nUEuT6dckiD_KIFen3bs,411
 diffusers/pipelines/ddim/pipeline_ddim.py,sha256=OPhDDQYe0X3-yYNsqClXy_or_g0UFQ-B2cdgSZhmqnk,6603
 diffusers/pipelines/ddpm/__init__.py,sha256=DAj0i0-iba7KACShx0bzGa9gqAV7yxGgf9sy_Hf095Q,425
 diffusers/pipelines/ddpm/pipeline_ddpm.py,sha256=VLJQY8I-BDcYOY-ukuYHPY0JwCSFHlYufh6gK4Dm8Rg,4959
 diffusers/pipelines/deepfloyd_if/__init__.py,sha256=gh1fQ5u6q0d-o3XGExCGD0jPaUK-gWCturfHU-TYIi8,2975
 diffusers/pipelines/deepfloyd_if/pipeline_if.py,sha256=10p2yPcx5oqnPjEQVvUzbJxo8H_lZVoNjKfiZKMnLMw,35933
-diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py,sha256=MGNlsMu1o6Urpz16vyavec8hTL2mm-6T0ObArm7vH6g,40250
-diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py,sha256=tPZZeZ4wUQ77wi9fl9gqUVnaidnaWZc-cCBGEptRtSs,45509
-diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py,sha256=Xb1Gbmmdn2naPAESVW2WM3XRTVY9aXFuZD_LKf3L3-8,45549
-diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py,sha256=HckGKuTOcOTHdYg9CR0N2SI8D43WOTU0xeSg8Dx2rsg,50365
+diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py,sha256=9E6VA6koKObV7JsSn-Ugx_wsUFy1AYk9CSIRAZcEvf4,40257
+diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py,sha256=rkwxI4GBXBLCXmTMF-FH8ut_bk1AZPxt_NUlXhiXvLU,45516
+diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py,sha256=9UnLMNAeyqz1JszuhtaPUbqlpIwHBeRLlTj7CHsULeY,45563
+diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py,sha256=Lr4RYa309O4yZ_srzC2BDggqJR6jWuWZwOATYRyou24,50379
 diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py,sha256=YvZY37mmaQu4VDXcB6n4NoqASugPHrsIFXSEPSvna54,40292
 diffusers/pipelines/deepfloyd_if/pipeline_output.py,sha256=YYPrm5jLziKPwvE0Xgx9bk1YhKC3TLivvR7W_wXzu4U,1140
 diffusers/pipelines/deepfloyd_if/safety_checker.py,sha256=zqN0z4Mvf7AtrxlUb6qAoiw_QuxGdDk-6js5YuarxTo,2117
 diffusers/pipelines/deepfloyd_if/timesteps.py,sha256=JO8b-8zlcvk_Tb6s6GGY7MgRPRADs35y0KBcSkqmNDM,5164
 diffusers/pipelines/deepfloyd_if/watermark.py,sha256=XSMfepkqMttxL05MGccFNLcs_blmjeWgEr2ipT3T7MA,1601
 diffusers/pipelines/deprecated/__init__.py,sha256=mXBnea22TkkUdiGxUpZDXTSb1RlURczuRcGeIzn9DcQ,5470
 diffusers/pipelines/deprecated/alt_diffusion/__init__.py,sha256=1SiGoNJytgnMwGmR48q8erVnU9JP5uz5E6XgHvlFDTc,1783
@@ -158,26 +158,26 @@
 diffusers/pipelines/deprecated/repaint/pipeline_repaint.py,sha256=qK5yT5rTUYm18V4wjD8FbDnZNECt8HP0gVwNzto4Kug,10049
 diffusers/pipelines/deprecated/score_sde_ve/__init__.py,sha256=7CLXxU1JqmMFbdm0bLwCHxGUjGJFvS64xueOQdD2X7s,441
 diffusers/pipelines/deprecated/score_sde_ve/pipeline_score_sde_ve.py,sha256=gkc03tCs1JWn8BwSd6MkR1rwJkGakcR3kldWU_rgdcY,4390
 diffusers/pipelines/deprecated/spectrogram_diffusion/__init__.py,sha256=lOJEU-CHJhv0N2BCEM9-dzKmm1Y-HPt1FuF9lGBgIpg,2588
 diffusers/pipelines/deprecated/spectrogram_diffusion/continuous_encoder.py,sha256=ymaMR3S9Xn3WXwIFoHAWbY89WKlOj603Myr2giqJUn4,3100
 diffusers/pipelines/deprecated/spectrogram_diffusion/midi_utils.py,sha256=4KYCUCTbnoS5x5YO2YCIFHGcYRbdSLH62_PD0eHh5XM,25096
 diffusers/pipelines/deprecated/spectrogram_diffusion/notes_encoder.py,sha256=TZsEASnZusL-9JK_v3GMR_kWNZZ8YDK3ATDmOBYKTq8,2923
-diffusers/pipelines/deprecated/spectrogram_diffusion/pipeline_spectrogram_diffusion.py,sha256=elihz9AgHvwe_DNDcuQ9G4Wam2G9Ma1aWZev7UFPkjI,11553
+diffusers/pipelines/deprecated/spectrogram_diffusion/pipeline_spectrogram_diffusion.py,sha256=HQ8YXonftzvdqVCXo3VOHT5DEX8Ap6inLzmV8jTb9m4,11538
 diffusers/pipelines/deprecated/stable_diffusion_variants/__init__.py,sha256=mnIQupN59oc3JmKGaQZia7MO92E08wswJrP9QITzWQs,2111
 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_cycle_diffusion.py,sha256=UTaPKyxt7PsLzt3-FtXkE91YQSgBBrbOPbfhbxJimss,47812
 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_onnx_stable_diffusion_inpaint_legacy.py,sha256=MtocKuS-glZL1ubF_Tk5iKVXjXIZb5blTOrcInv0Ax0,27814
 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_inpaint_legacy.py,sha256=iu8EoeWyzatIc_6Hmf358CvUgDN79NoU8yC5XvLu3oU,42346
 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_model_editing.py,sha256=EmTmyjms3t5POXCCue5ky0Hc4bVABvnZpSG4ax6kUQA,41304
 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_paradigms.py,sha256=rk_HndXcW5y_9RcfBStE0H7XmkoIkUNVRpgPFL8p4HE,40999
 diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_pix2pix_zero.py,sha256=sFEGJraThOwdOTBnGVSuSXnmjF4HJiAmoGJLAXCbVxg,63419
 diffusers/pipelines/deprecated/stochastic_karras_ve/__init__.py,sha256=WOKqWaBgVgNkDUUf4ZL1--TauXKeaPqtGf3P2fTFYMw,453
 diffusers/pipelines/deprecated/stochastic_karras_ve/pipeline_stochastic_karras_ve.py,sha256=hALFduQcJ6FEJW6XlYcZ8wA_iqNGkm7CMpcbT-VHxVQ,5277
 diffusers/pipelines/deprecated/versatile_diffusion/__init__.py,sha256=_CRp2PIJD6loFlES3hMcPigZNOUMf2OgTaRFgoit7hc,2838
-diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py,sha256=7VyOcBfSyikSLYd5PN48L4LfCG1BqPja_9JF6a7BnpE,115479
+diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py,sha256=J5nB-j_CS1DYzNllG-zaqLdUaENq1AWUWBSDP3hFPZM,115479
 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion.py,sha256=DP0cqDy9iW30X2Bm6zxF0u1RMO3fZRgmWyP4U7n09Uk,21916
 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py,sha256=oJq0FKhjn0p5IRKeYrWDf7jlMJXKRX6RKZQt9dcqFVI,27122
 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py,sha256=auaYrCfhQw3J8EwO806zdbt08E7WFytO5ewso9h6FkM,19641
 diffusers/pipelines/deprecated/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py,sha256=0A96_GQziIgm91uohIxnCD3IeDvq1txtNTQJ3XbjfRE,22840
 diffusers/pipelines/deprecated/vq_diffusion/__init__.py,sha256=CD0X20a3_61pBaOzDxgU_33PLjxN1W8V46TCAwykUgE,1650
 diffusers/pipelines/deprecated/vq_diffusion/pipeline_vq_diffusion.py,sha256=ntOfJzy0d-9oCI9B8T3VT_jC2wxuPSxHuNu4mjE1ZeU,15474
 diffusers/pipelines/dit/__init__.py,sha256=w6yUFMbGzaUGPKpLfEfvHlYmrKD0UErczwsHDaDtLuQ,408
@@ -201,22 +201,22 @@
 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior.py,sha256=rP0GvZL-vwZ4d494woYQMm9OYngohIK1gGgKKI4o5fk,25434
 diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior_emb2emb.py,sha256=4wPZaVHCi3ZBmdhc3PJ9petZbHQ_Q2hVy3azw7q5L3c,24981
 diffusers/pipelines/kandinsky3/__init__.py,sha256=7Mv8Ov-XstHMLmRQU7psdheFn_e_qXJWWTYV7z7uj4U,1461
 diffusers/pipelines/kandinsky3/convert_kandinsky3_unet.py,sha256=JiGoSQbu19tYwb_yam7rbxtudWm_fsHvIb-HNulb4SA,3273
 diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py,sha256=TyRlNsZUsrr_-ry3C18mB04JVuy4iKZiEc9iHD61Hek,28116
 diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py,sha256=DNNvT78eXtbgohpkngFtnp_QwoQ5TN3pdCUrJ6U0PtM,31349
 diffusers/pipelines/latent_consistency_models/__init__.py,sha256=SfUylLTTBCs_wlGOPpW899lgE1E0GOLGu4GhDPFx-Ls,1560
-diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py,sha256=EKFaQ5F0ITVJILK1dsiS5IpN-AhmexNekUic8dlQZi8,47979
-diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py,sha256=oGYDieYIGiZJFkCpr8O3ESsZPS9qq8ebgEM8St_6MQk,44898
+diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py,sha256=AMYw5Lrq_9idSh8DwJ6d6dcoDzIhZ0aJV9gUvWZhymc,48151
+diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py,sha256=qI9zDZBBcouGp6sta6NULtO4azKB4HNVWpyI1yiEzf8,45070
 diffusers/pipelines/latent_diffusion/__init__.py,sha256=iUkMRZY-pteRsvsROOz2Pacm7t02Q6QvbsgQedJt6-E,1542
 diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py,sha256=IOlFKw0q-uFBJsWfwyGBSq824xWEhGFLlXMIwkVNrLw,32819
 diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py,sha256=cJxtFEapCNDHxmaiNuifF6bAhounGQtfYfgeMvIyu0c,8061
 diffusers/pipelines/ledits_pp/__init__.py,sha256=3VaqGS1d39iC5flUifb4vAD_bDJ-sIUFaLIYhBuHbwE,1783
 diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion.py,sha256=iNiiqrqAxkUSYoWfz5nSpwQzxFPFKz6dKG4gpLHJ8XA,74992
-diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion_xl.py,sha256=T0dJiTgOcooR1CWQ_li-2lPPEqS9RghC7M-PLvZMNGQ,86976
+diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion_xl.py,sha256=ix2-95kh2wKoADcdHwZPfW3_z6-7jE3i6Kn4hyOp5a0,87148
 diffusers/pipelines/ledits_pp/pipeline_output.py,sha256=w2bNTYVkbNCRaujmziScVBE0lE7-JHDHi_uK9pop1nA,1579
 diffusers/pipelines/musicldm/__init__.py,sha256=l1I5QzvTwMOOltJkcwpTb6nNcr93bWiP_ErHbDdwz6Y,1411
 diffusers/pipelines/musicldm/pipeline_musicldm.py,sha256=KMLzGCJBzyv7faZgDKnuUAr5St9piPVLpgqr_b4i7nQ,30230
 diffusers/pipelines/paint_by_example/__init__.py,sha256=EL3EGhjCG7CMzwloJRauSDHc6oArjVsETUCj8mOauRs,1566
 diffusers/pipelines/paint_by_example/image_encoder.py,sha256=tWrFvICx9coL55Mo01JVv4J014gEvfNHzLarKbNtIs0,2484
 diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py,sha256=SZJHd4q12btoLv7OfKFxNgCQw8X4fbd52WKh0rsVmqo,30956
 diffusers/pipelines/pia/__init__.py,sha256=md5F8G279iZg4WGSmLP7N8apWkuHkfssjLQFzv6c2zI,1299
@@ -228,79 +228,79 @@
 diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py,sha256=V1-uLh-beOA-FwPkwj9MjQdF4KfCpeV1XjZrtbR5Ieo,38689
 diffusers/pipelines/shap_e/__init__.py,sha256=LGToZwsVeVBEsE5eveY0Hc2GgI6UgDz6H_6cB_Snn0Y,2093
 diffusers/pipelines/shap_e/camera.py,sha256=O35wgvHgwbcf_QbnP1m5MBYhsXyw_jMZZyMXNFnW0RY,4942
 diffusers/pipelines/shap_e/pipeline_shap_e.py,sha256=hHIzOdsftfqOBbcrwJqADtA2B8umrnJjnJRmn_KhmYM,13192
 diffusers/pipelines/shap_e/pipeline_shap_e_img2img.py,sha256=sTEEugcsYVBltWjfZWwp3K6-7-m8uG6turYKYirBK84,12951
 diffusers/pipelines/shap_e/renderer.py,sha256=7Q2F48rD4lAI-idP5x66dsBmd0TQSUUPcF8xZ-6reuQ,39148
 diffusers/pipelines/stable_cascade/__init__.py,sha256=buKExLbA-qdePd19JSEF29AhOCIaDgqFfLajEmo-Kg4,1672
-diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py,sha256=muD14xWY-QQzd_JjuwsB-m5hVKO933vn3lRibr6w170,24370
+diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py,sha256=XHBz_sElWMj6ODnp1x_9oGDx9H7ki0MHjuavsF3d37M,23653
 diffusers/pipelines/stable_cascade/pipeline_stable_cascade_combined.py,sha256=-eTFBpzXDIc1Rag5r--V1cg6rJfUr-Db0NLJI2zm8c0,17569
 diffusers/pipelines/stable_cascade/pipeline_stable_cascade_prior.py,sha256=cWCJzeIptg5Wa2ECP7DJfIQuYkboGprbwJZm_fmWP8o,31270
 diffusers/pipelines/stable_diffusion/__init__.py,sha256=1f_Sh21VZhxvaeODVJbQMc_WD0qEWjyDHSrk9boZrGc,9314
 diffusers/pipelines/stable_diffusion/clip_image_project_model.py,sha256=AyR2S1ueZGcWzZC0L7Zli4qA88iGiYqd8NAdwYqDStA,1094
 diffusers/pipelines/stable_diffusion/convert_from_ckpt.py,sha256=fSINoVYJ2p2n19anMrfCrTvMRL53vCpQIGvTAOrm9Gw,80517
 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py,sha256=6Ej3vXGfogBfem_M3jQZyCkSRuUqgn8T4IzohnJ53v4,20576
 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py,sha256=I3q5iMuo1xG03NAUmMqDcMvdKHhGGL79XoT59TwO8bU,22504
 diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py,sha256=LVunJp_s0krmN-FYO0-jDTvv8rUe6_w2rFpdYLzPeaI,25958
 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py,sha256=MrkQriLbTAhqNDoSXzMtkA_PgV2_CGnVQCGS_Pweac0,24324
 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py,sha256=NUlgwV4uDWt95L9NWpAOEdXYJMwf5rVRbIxN2H6H7_I,28500
 diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py,sha256=wp6aDJHGtiSZAaa3T7wIAmNvTZ7X8ZpIPNIRjRfmMzQ,29114
-diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py,sha256=QRU9fto-kuRLqXW3yVTiL4Gl0TeiQAhsVPeO0rrpRzo,27913
+diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py,sha256=04yTaZXyShbxYSpD2AQ1C9y2cuONdtLDflYxlCmfThQ,27955
 diffusers/pipelines/stable_diffusion/pipeline_output.py,sha256=Io-12AumvYjBOKP4Qq1BJ2rak5pKdtMO-ACcorY6hFE,1496
-diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py,sha256=YJciBpfx9nYlJ5Gf3K5NVyb033CEgNqHK0Z6aHRPOgo,52504
+diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py,sha256=pM2QX2VvCjP6C7T1rq8SPnbHk0qYnvUZaHWpLTUjQ9U,52676
 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py,sha256=bf_H0F-8S7NvMQd1m0MS0iC6KDoDd_vo91pHrDPRZC4,43284
 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py,sha256=3oi1Ls3qleWsju1l2qUg_Z3uaogy_jTM2-lb1mZPdos,22274
-diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py,sha256=yAYq77LMvgqbvtNR12kwPEDmhn-2i2qxq1hPMcQlPPo,57066
-diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py,sha256=JgWd8nSsTlqPEOKfgdscg17odO7SCLbR0GBIECSnDxs,73542
+diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py,sha256=7xVhNsEjWLthRWLRPtArQaUPliaqGyVgcOSMfLmzCZw,57238
+diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py,sha256=NY8Xt1tOIPKxE1wIKyjN-wtDIG-iZQYAG9tjb9Z0AME,73715
 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py,sha256=x65JLGd4SqnYxssjiOyB3iTz5pikVxluWbP41IjGSHQ,40591
 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py,sha256=ulGpwAPChQ7UeMSykAvUtRPnsotfz8NAtr4YXBAPxXA,23166
 diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py,sha256=J6AdX_iTIIO3tfQk--JaFKLBH2GTqwff_WSa-OY0niw,39543
 diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py,sha256=3PQ9Kaek4y_lbBkZ0CSZozXVVyjjqRxk5O1tl36Iv5s,45135
 diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py,sha256=1SfqDMe4yvUxHMDRsQPV_4v44WK-IJd-ok0aKdVQJME,39866
 diffusers/pipelines/stable_diffusion/safety_checker.py,sha256=DnbsnVctFGvItiuk_PLBdkgBjZwFCtk6JMZXfV9GDHs,5734
 diffusers/pipelines/stable_diffusion/safety_checker_flax.py,sha256=8VrTsmMmbKJE3BhXzsUxMEnLYUbKFiKxksGgV2oikhc,4476
 diffusers/pipelines/stable_diffusion/stable_unclip_image_normalizer.py,sha256=PULQ_c3li4FD8Rn-3q5qCoHoE4Iknx3eZ2_XLy1DbA4,1890
 diffusers/pipelines/stable_diffusion_attend_and_excite/__init__.py,sha256=VpZ5FPx9ACTOT4qiEqun2QYeUtx9Rp0YVDwqhYe28QM,1390
 diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py,sha256=WzxjC0nLW4U9wy-lCQuwc921xMhy8GjeX8UhJRmWkTc,50905
 diffusers/pipelines/stable_diffusion_diffedit/__init__.py,sha256=JlcUNahRBm0uaPzappogqfjyLDsNW6IeyOfuLs4af5M,1358
-diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py,sha256=UOGWHJHOCOS0wHOrgqaiG40ffuZ5Np98BsqKqIcm8i0,78053
+diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py,sha256=2fk5uJlf0JnA34wAeJh8g5wKqfqORGrlHXh5xh34Z-E,78053
 diffusers/pipelines/stable_diffusion_gligen/__init__.py,sha256=b4dZB5bUuZmEAcg7MmCyWZpyxNmMrlrByEQW_xwGGgI,1568
-diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py,sha256=iLCgxXdcZoACp-nXeKicadA1-P4aahz8bD8o_q7wdNA,42994
-diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py,sha256=mDDFTr7D4H7i0h3ex98Hu4qBIX0wPIt7KTTYrOIsWtU,51187
+diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py,sha256=d-0jBQAlwK6KyUcI-2szZYirOdlsQgokXRPFlpGUFac,43008
+diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py,sha256=uv81E9jQvoS-7BgHtrfkb7-7b5LGoiGpT7drtEJS8WQ,51194
 diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py,sha256=EBpyQedEN-jfJ0qeLCFg9t28cFPNbNaniKIGM4ZMF14,1924
 diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py,sha256=AcWpgnvLvbw9Fg-vlAkE-IwTtThr0X5pv6seUnbbXTo,33326
 diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py,sha256=5cZtYzbfydHfS0Qmht7hptNZqM_xCjeQxbv-bkpPJa8,45167
 diffusers/pipelines/stable_diffusion_ldm3d/__init__.py,sha256=8p2npGKPPJbPaTa4swOWRMd24x36E563Bhc_mM29va0,1346
-diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py,sha256=XsQID02ey3kUp0jAvMPEttMLN4xN9BfmAGj2EOEIHqg,49301
+diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py,sha256=lBXk5ypuInYw6J31zMalwMafd4dpuGZACGhSSJnKEqA,49473
 diffusers/pipelines/stable_diffusion_panorama/__init__.py,sha256=af52eZSYshuw1d6kqKwx0C5Teopkx8UpO9ph_A4WI0Q,1358
 diffusers/pipelines/stable_diffusion_panorama/pipeline_stable_diffusion_panorama.py,sha256=tSa8UI8GnUZqqA5KQEUSs7ovc671qP7BvYkTWKkYkko,48750
 diffusers/pipelines/stable_diffusion_safe/__init__.py,sha256=rRKtzOjuaHLDqSLSavcy2W8sEljso9MLhmEwrNiJFJ0,2751
 diffusers/pipelines/stable_diffusion_safe/pipeline_output.py,sha256=WGQS6-k9dPH0hYBj_dZMlHFkOvUUti9fjVv0Sf8LCjQ,1459
 diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py,sha256=0EE-mrcf7QR-THc6sLqrzIPbKUR-fCYzrBlmrfEDySg,39088
 diffusers/pipelines/stable_diffusion_safe/safety_checker.py,sha256=fzco9KgjLVUJHldtOOK4KvkpMaS-nSQlCcnS8Qodv7s,5049
 diffusers/pipelines/stable_diffusion_sag/__init__.py,sha256=06vnWbASiG3o4sQ7CDlDrqEm6dSCerKdLODz1FS-EFE,1338
-diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py,sha256=TR2tJoZUoVMrACweyaa6qPGt09qk1YqTinqFMT14__w,44638
+diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py,sha256=ZaU6442db0SpjwlhDQZhHW_AO_LF9Kzfxtzqlx-lP3s,44636
 diffusers/pipelines/stable_diffusion_xl/__init__.py,sha256=6lTMI458kVDLzQDeZxEBacdFxpj4xAY9CSZ6Xr_FWoY,3022
 diffusers/pipelines/stable_diffusion_xl/pipeline_flax_stable_diffusion_xl.py,sha256=AHlpNWvIvO38Dp2bpXOfYw_-oxuLb7lsz9WETsQmbjk,11243
 diffusers/pipelines/stable_diffusion_xl/pipeline_output.py,sha256=Isy1wE8hgoScXXHWVel5jRAzgPTelP-aZieugTOTgUc,1037
-diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py,sha256=Y9FKBbFswfMPKcQrIvBpddlUcBw5DT5HvZUPKc_xKEI,65444
-diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py,sha256=YpsEGITuF2D-Ceo2n80AK-UcU9hwkbHEQyXgw-h_UEI,75081
-diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py,sha256=ccNycUXpiaOTxlsKhRYJvpKOx7q1q4glspEPaIQLUvs,93138
+diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py,sha256=DfL8L-5Ba481z2GJbb9RItaarZ0YSh8zfrpbSenvaNY,65616
+diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py,sha256=njLHA2dpIC4_5bLmqa0BpATpGgtYUcognGUxQSeW6Do,75253
+diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py,sha256=-tfzGZ8zryY1PEK21qvey5uR7pcGvMMMxZ6TgM2Hyug,93311
 diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py,sha256=I0FRdOn0g-iT-hCPriJbOfR3AQaxaf9eo3jD9jEA-PQ,51914
 diffusers/pipelines/stable_diffusion_xl/watermark.py,sha256=yEVuusMoOiSYa7EhZTe-M5_1QMeGBKxICIJjT1I0hM0,1242
 diffusers/pipelines/stable_video_diffusion/__init__.py,sha256=QtcDxzfLJ7loCDspiulKyKU6kd-l3twJyWBDPraD_94,1551
 diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py,sha256=Jg61yuUnN00SnCi1vHB-jBAZgUUN3qcTKHnRnWHkZT4,29461
 diffusers/pipelines/t2i_adapter/__init__.py,sha256=PgIg_SzwFAqWOML5BLHvuCTmu4p06MPT66xBpDShx8c,1556
-diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py,sha256=HIdm4djbwZcP_BP7GhOHKIHZebuix9jcXri1J2gHbL4,45842
-diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py,sha256=qDJEzkPwRFyfvHokKDRWyXYRcMIKgPxKTCMoWMIigDU,67294
+diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py,sha256=k_dAy8fXcQRFbPXytVVtFb1PbUHzl4dY3NjltF4ains,46014
+diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py,sha256=njFRKjO424zrHTMP4aKqHvMYhqQha_2mTsduiOEZ1bs,67466
 diffusers/pipelines/text_to_video_synthesis/__init__.py,sha256=7-NplGtgnp5GUu4XN_STE9fqAtFCAc6FF3lphjbDBhs,1979
 diffusers/pipelines/text_to_video_synthesis/pipeline_output.py,sha256=hwdMeM0ae7gmpEWorXlvB2EfXShbNsc9AO-mIp-JlC4,723
 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py,sha256=6vsN9pZtr87Ytgmg6iW2LyYf_QJD5Di7OjkFum4_W54,32191
 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py,sha256=slUN-KQd6Omv3ALKzmpiH-mt4VBqX5CHh4BJ-MuVPH0,37044
-diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py,sha256=7yhGMWV0DqdUoarTo-tFVoxoRtf8XrDvuASKI77aBg0,45014
+diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py,sha256=61tr4jk5-jjXMqnWBcsFwTt2NmxLdhdcZJZ_tRop9ws,45008
 diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py,sha256=-qIQB-yfnZqk4fTB27guE5Xrx0gCoje3HbJbfuk_ECo,63688
 diffusers/pipelines/unclip/__init__.py,sha256=jBYZIN7NhTKM_Oq7ipJ4JaMXO-GtdchmFWe07gDerfA,1752
 diffusers/pipelines/unclip/pipeline_unclip.py,sha256=mieEfWUZ1ezybLQ2nBHXFO_uF_MPnIfxkPV590OI2Vk,22170
 diffusers/pipelines/unclip/pipeline_unclip_image_variation.py,sha256=1cJXvIrA3pRN3o-8mt3QH_aVXifT-4emgKo0KcUZbZY,19072
 diffusers/pipelines/unclip/text_proj.py,sha256=ZvkD9D4ijlPE2uiaoiDiS1gFvEiNcQMOTtKTyRPhpSU,4278
 diffusers/pipelines/unidiffuser/__init__.py,sha256=GvGtf-AToJXNHxv3RAo5_I_9zPQjDFbMTAHICCt-4xY,1814
 diffusers/pipelines/unidiffuser/modeling_text_decoder.py,sha256=JjarQJKGvq84bxECj7FATlY2-JV7R71O6tfkeWr0H4k,14113
@@ -312,52 +312,52 @@
 diffusers/pipelines/wuerstchen/modeling_wuerstchen_diffnext.py,sha256=zo77mi0f53A82NfaE4TaHiN7gdLrLPEGubUDdRFU_ks,10423
 diffusers/pipelines/wuerstchen/modeling_wuerstchen_prior.py,sha256=Rop39GTBI1oXkxx1QwaPdXrhVbHgXJtB_uawwZCxhrE,8553
 diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py,sha256=kKCOMfJTg9zgnmD3xjxAYzQMPPdFoWpnPQ3tvHG9Zb8,20583
 diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py,sha256=17b93CwEK-pQLQ_4BvTxYm5vROY2wshrEH-FNL63TTs,16421
 diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py,sha256=N2iabh9Sm9jhlquEfS-P3ATp6Qq0bXzZARWNhe0yKrw,23833
 diffusers/schedulers/__init__.py,sha256=QhyrpbHn7BntT-jNMNsCUkh8-rVpjgL3ExvSNXoYtK4,10029
 diffusers/schedulers/scheduling_amused.py,sha256=z9WvPx0FKP4eXns9k8yj15FyJKuHDnBBBXcaxQ0mUNM,6615
-diffusers/schedulers/scheduling_consistency_decoder.py,sha256=hKIGNVDy7-mFmvp0L1GTaF-YSQ6hFi_xcnv9cMuHaG0,6871
-diffusers/schedulers/scheduling_consistency_models.py,sha256=AbKznEkhWwz5rn-skJMZb5DTSuen4P_ylMAKf8O6EdA,18912
-diffusers/schedulers/scheduling_ddim.py,sha256=RNvEVUd9T0RFCo9tNco9NLQJFv10S_6TBAIbOsomHuo,24954
+diffusers/schedulers/scheduling_consistency_decoder.py,sha256=gHu-MCb_yv9m89Ym9eQ5aCeKiE8FhhmTMwFN3PRO1oA,6872
+diffusers/schedulers/scheduling_consistency_models.py,sha256=vkSHK6pOYdmiorPD_spifssZQy8aHnE_l1DCMq7fnZ0,18920
+diffusers/schedulers/scheduling_ddim.py,sha256=CquykCldmpSTecSlA4M7Ie6y1RuPDIV_q--YAQwrzVY,24955
 diffusers/schedulers/scheduling_ddim_flax.py,sha256=rfQR1qrp9n8f4pp2Md19wkELk5Xm2QTGwvLauOk9Gn4,13110
-diffusers/schedulers/scheduling_ddim_inverse.py,sha256=QzOwi115Sn3Ilmr0f3wHXvXRQRcNsLUiWEf4GtjSEH4,17847
-diffusers/schedulers/scheduling_ddim_parallel.py,sha256=vpXLhFbHvxUD0ynhajtBOskMYcjlaaL1qaIRcjroi_o,31602
-diffusers/schedulers/scheduling_ddpm.py,sha256=EPFRnOA-3bZ1mRc3tl_3C3R-X-WozInc_VDU5A9Beqw,26229
+diffusers/schedulers/scheduling_ddim_inverse.py,sha256=LLVwnZ-WSRJYeJ6tpkIjiPyHDLyJgJzjc45FPsmPYik,17848
+diffusers/schedulers/scheduling_ddim_parallel.py,sha256=edIH1hSLI98r9FYnuWjpzyslhem_3-460brGOHEIUxE,31603
+diffusers/schedulers/scheduling_ddpm.py,sha256=IN1Wm6YCC007rnuONKelDvl4zUAt__yWe4ShFcVdzwY,26230
 diffusers/schedulers/scheduling_ddpm_flax.py,sha256=4Ewhe0kE9l73oENsLYADBJktuEThYGaZLHhTi7yyq3o,12463
-diffusers/schedulers/scheduling_ddpm_parallel.py,sha256=-EGmiJaTbfgp_Fj0UtYaX7DvZ11bFNQpO2rL6HdVob0,31248
-diffusers/schedulers/scheduling_ddpm_wuerstchen.py,sha256=TOs2SXvgbD2aYrTEUGIBQ1hu5GQ_HV_ROGS5n1IQX_c,8999
-diffusers/schedulers/scheduling_deis_multistep.py,sha256=9FJYhHOFYOeY8CADqmlsrXiKelVaArd_y9_IcWzplp4,34972
-diffusers/schedulers/scheduling_dpmsolver_multistep.py,sha256=3Qb1bhvh-lh-xXb8ibJKHCGW1RMdFmtiW3GABTYkp0I,47985
+diffusers/schedulers/scheduling_ddpm_parallel.py,sha256=uTeVj-o-UFp_jway6A3YB_cvN5bDgX0BZbU5uFYp25w,31249
+diffusers/schedulers/scheduling_ddpm_wuerstchen.py,sha256=CNit1DazKmSl1WkJuyTxg2qsJ_2pG7opYI_So-WxIVs,9000
+diffusers/schedulers/scheduling_deis_multistep.py,sha256=cD-EnKugVCrimAS1HzFV9zBSfQb1xR6qFBq1SQPyKz8,34974
+diffusers/schedulers/scheduling_dpmsolver_multistep.py,sha256=WM6S8RVmL69oDQ8tAYx7kK-yqHhVahNPIWAzTL2JL7U,47987
 diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py,sha256=WkTcIei0fhpO1nnXW-zN4NKoi350A2Ztg6obyJ1pI_4,28731
-diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py,sha256=53DK70CqXwxD8FHU92KTBYM2eiEOAO7xVFvwPZYAhiw,43521
-diffusers/schedulers/scheduling_dpmsolver_sde.py,sha256=NTQffFEzD7lf0Ntc5nJjG4ZmIg41jK96XTNl2CVuSWM,24328
-diffusers/schedulers/scheduling_dpmsolver_singlestep.py,sha256=TsclOP3lhNnHpnMQU0bDGDYKQ9T1RMHvRrdOFJDX_1Q,44972
-diffusers/schedulers/scheduling_edm_dpmsolver_multistep.py,sha256=nFm9fXjvMBXNL_R_PAiFeAO3UdOeSruhWYgwAEhkOj8,30541
-diffusers/schedulers/scheduling_edm_euler.py,sha256=NMqkM-XihuGFG1gs-4xfCfmZZKCiTAX4QNDAX-gKDCA,16046
-diffusers/schedulers/scheduling_euler_ancestral_discrete.py,sha256=AQmNlKm5fEl1Ax5PjJBmqWTwgpa3lDF3xY3-2GMilhE,21228
-diffusers/schedulers/scheduling_euler_discrete.py,sha256=GNXLKaQmnMOz2vhhInQMK8qxiewcIeT0y8paSBoH-uo,25379
+diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py,sha256=Sv9lcLxXxYokgiAaoDgqcMe2KjMNalyUG2DbgygZ0gc,43530
+diffusers/schedulers/scheduling_dpmsolver_sde.py,sha256=r_kUBKAbhYEn9idp22JFCZ1lO7vybNONO6eznBl2Was,24337
+diffusers/schedulers/scheduling_dpmsolver_singlestep.py,sha256=_KAtrYCtQMvYpCVdsjXqUo3L2-X38BI1kD200cMS6cg,44974
+diffusers/schedulers/scheduling_edm_dpmsolver_multistep.py,sha256=DX8suVqrOEmlIhIWxE8yG84UUyGBgtBoV-wm4h-xv6w,30542
+diffusers/schedulers/scheduling_edm_euler.py,sha256=YBl3HMUAYBwnhZSVp2sJJKbpZAyQu-Ld2cFNgowdyjA,16047
+diffusers/schedulers/scheduling_euler_ancestral_discrete.py,sha256=QbGuwrynfAOCO9XqRsewS-kuFs5qPWXjwyd-teirhyM,21230
+diffusers/schedulers/scheduling_euler_discrete.py,sha256=P9CO0b7ZolGF8fSvAZbnrNuuAPAb6VtmYQDkYDXCyxw,25388
 diffusers/schedulers/scheduling_euler_discrete_flax.py,sha256=Bsi5Yz6QiTS77pSMJdkoB8IhemE8h_xVkru9MUb4GHM,10801
-diffusers/schedulers/scheduling_heun_discrete.py,sha256=TGv1ef8yGD11UpoTlqRPRl_csyrpQf-X2Y_BYwtJHpI,21085
-diffusers/schedulers/scheduling_ipndm.py,sha256=ZMdXOgzcJDNmYjTN7Op0syLcFUMQQo8RD6zk6v_dLo0,8782
-diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py,sha256=xNHvEVSJBtYoJXCaUrtnF04nerckBU5t1ytl7II56SU,22550
-diffusers/schedulers/scheduling_k_dpm_2_discrete.py,sha256=jm3ujNVcakpxlUVeUMiPj5z4AoySNI1WoZ91zOQO0LE,21238
+diffusers/schedulers/scheduling_heun_discrete.py,sha256=JDkPvx7fqsIxBEAPfOI2Y46LA2YyX5xY15nVF8GFjqc,21087
+diffusers/schedulers/scheduling_ipndm.py,sha256=LvEaSMta3X0o7ufyv6iFgya_gvXCkx0PLKrO3grLJuY,8783
+diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py,sha256=DSnutwMjpCrjBp92Giz4fdajjiVFoCrXAQZ9Yn7UOxY,22552
+diffusers/schedulers/scheduling_k_dpm_2_discrete.py,sha256=Az7t5HNaCbXJ8UNr5GPz2xPnspu67Z1ldrx76RG_eO4,21240
 diffusers/schedulers/scheduling_karras_ve_flax.py,sha256=frO1wwwWo1WnC7ojS8B5CTvK0EFx1DBWzcr_MwMZGsM,9636
-diffusers/schedulers/scheduling_lcm.py,sha256=ZSJd7oomjQmX6JrbDmyH_JaxdaknSLrtzM3E649-3m0,32342
-diffusers/schedulers/scheduling_lms_discrete.py,sha256=A7T3QHLNAZOF5ImsW0PmCexNvlWXKymUAG8RbWQQfA8,20737
+diffusers/schedulers/scheduling_lcm.py,sha256=TdwXvusMMAlvOvI89_EmSBjSHEmRbxO0viwU7Eafysk,32343
+diffusers/schedulers/scheduling_lms_discrete.py,sha256=sci8tBoauH7joRFXXvN_QzytOqxGxLOuWeXI6xKT_SE,20746
 diffusers/schedulers/scheduling_lms_discrete_flax.py,sha256=OoO3816H2AWXGLpA3AeREFffh4yhDa-qbPHrtStfCGo,11077
-diffusers/schedulers/scheduling_pndm.py,sha256=lRvjFygTdAey9l6sGEWWfQHxshYCAaWAAOdrm9MD0LI,21814
+diffusers/schedulers/scheduling_pndm.py,sha256=UOZOSYtkaPrqEKx9AshI23e-Hr-BpWkQXpL7fPx5CeM,21815
 diffusers/schedulers/scheduling_pndm_flax.py,sha256=Yzsf1yQH1ycnyIOzWAbDJw1g-oBgjGY3ejgD1gdaZ48,21539
-diffusers/schedulers/scheduling_repaint.py,sha256=CaBoC8SNijsHlHCa-RFRg8QovwmfV0ru2TlMoCKhKKQ,15342
-diffusers/schedulers/scheduling_sasolver.py,sha256=z9n1q5YZhvG_G8cE5Tpn-1xYZ4xir1FrjEBNHOlicls,50357
+diffusers/schedulers/scheduling_repaint.py,sha256=00H7LCjey7fpN_1UBlJNI2-k-oN__0npvmeld8XAXeA,15343
+diffusers/schedulers/scheduling_sasolver.py,sha256=fgE_4i0xzmDIXWJ8-j0S_BGeAl4qLOoaj0FAVHt194w,50359
 diffusers/schedulers/scheduling_sde_ve.py,sha256=5S6UDX4Seo9Rgyq49gyHj2XfttJOz54eaj9mt4bwRb0,13421
 diffusers/schedulers/scheduling_sde_ve_flax.py,sha256=8nZWyB7tUTtXafpQpiAOFGVHGPK9KNNdPHX71XtZsVo,12134
-diffusers/schedulers/scheduling_tcd.py,sha256=aGv-975qTaYw-2C74Lu_8CrDaQlz5QECyZ-cM9cyo1M,34398
-diffusers/schedulers/scheduling_unclip.py,sha256=MLqcpFMhQuKoS5QWrIvDSmR0jYnP_rbVy4Y-xBBpLwQ,15046
-diffusers/schedulers/scheduling_unipc_multistep.py,sha256=TeL0GWEnsRAgmwCbQWgRy_OMb-sWse_iBeabNCncg_c,37534
+diffusers/schedulers/scheduling_tcd.py,sha256=2JzspRxWR-I7x-4nAlyO8riOus8mp2_W5j81YfU9jPI,35066
+diffusers/schedulers/scheduling_unclip.py,sha256=WIuvURuFOG4Ee6J_VEJbCTW4LNQTY78y97eJoQoZptA,15047
+diffusers/schedulers/scheduling_unipc_multistep.py,sha256=v7wTUzdGCdV1VuiHVuFi4z1mKiKkewdquqW4dHjzmjc,37536
 diffusers/schedulers/scheduling_utils.py,sha256=gXqWv_kWyGd0sKhQ67FnPo4pZvBIi93Cz9rjuNsdR6Y,8407
 diffusers/schedulers/scheduling_utils_flax.py,sha256=x8cJYTp0OskUa8ciCqK6J51gLZRWfg08zwf8hGi03NU,12368
 diffusers/schedulers/scheduling_vq_diffusion.py,sha256=4F-Rb8I03PRtyVDLkW3G5IpR60p5QSVL39vf0lkEXxM,23009
 diffusers/schedulers/deprecated/__init__.py,sha256=j5t9z89-IHxvsBrCdcxMgKtmXI16O3YyPjkt3ILnAEc,1348
 diffusers/schedulers/deprecated/scheduling_karras_ve.py,sha256=p7VjMb-mDs-PwQNnqdU8hl9QssTKCZ6AvDLbO04XbKk,9842
 diffusers/schedulers/deprecated/scheduling_sde_vp.py,sha256=pHAFBV372CD-1KRRwtmNab2uUKgmkD2F23rvj3d_M54,4294
 diffusers/utils/__init__.py,sha256=Zml0x_dukTKfhMHrHj6tKExzO2x48iovxdxezr0xh3U,3693
@@ -387,13 +387,13 @@
 diffusers/utils/outputs.py,sha256=hL1yMK86ota2SdZhyiWSzvfANjaO8pU7Hz1w81_Vmr8,4953
 diffusers/utils/peft_utils.py,sha256=QKJ0hnEAiwfDS66tO6pfbDOWtSpFkLIGN2aZlenqccw,10180
 diffusers/utils/pil_utils.py,sha256=mNv1FfHvtdW-lpOemxwe-dNoSfSF_sptgpYELP-bA20,1979
 diffusers/utils/state_dict_utils.py,sha256=JXLPkeJcmC6b-68oAhFkq2k6sdmUeVrlnhahucbsdrI,12954
 diffusers/utils/testing_utils.py,sha256=3hpnggU5-5_m6k1eiqorHNwq7lonLzLSkyb4mMwaF6A,34708
 diffusers/utils/torch_utils.py,sha256=waQgGB87DZ5EqL8ZjwYYtsXtZfPj8xGNB7Fg_7JGtvQ,6233
 diffusers/utils/versions.py,sha256=-e7XW1TzZ-tsRo9PMQHp-hNGYHuVDFzLtwg3uAJzqdI,4333
-souJpg_diffusers-0.27.2.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-souJpg_diffusers-0.27.2.dist-info/METADATA,sha256=rdpN88OBERbW27vQfi47nMu38e41UUAqzDJ7tOrXy7c,19096
-souJpg_diffusers-0.27.2.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-souJpg_diffusers-0.27.2.dist-info/entry_points.txt,sha256=_1bvshKV_6_b63_FAkcUs9W6tUKGeIoQ3SHEZsovEWs,72
-souJpg_diffusers-0.27.2.dist-info/top_level.txt,sha256=axJl2884vMSvhzrFrSoht36QXA_6gZN9cKtg4xOO72o,10
-souJpg_diffusers-0.27.2.dist-info/RECORD,,
+souJpg_diffusers-0.28.0.dev1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+souJpg_diffusers-0.28.0.dev1.dist-info/METADATA,sha256=v9k4pNjh7Otf9Ct89_6SV4QoaggupWb0vZ1E5a8KvGM,19101
+souJpg_diffusers-0.28.0.dev1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+souJpg_diffusers-0.28.0.dev1.dist-info/entry_points.txt,sha256=_1bvshKV_6_b63_FAkcUs9W6tUKGeIoQ3SHEZsovEWs,72
+souJpg_diffusers-0.28.0.dev1.dist-info/top_level.txt,sha256=axJl2884vMSvhzrFrSoht36QXA_6gZN9cKtg4xOO72o,10
+souJpg_diffusers-0.28.0.dev1.dist-info/RECORD,,
```

