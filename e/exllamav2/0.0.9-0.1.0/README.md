# Comparing `tmp/exllamav2-0.0.9-py3-none-any.whl.zip` & `tmp/exllamav2-0.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,75 +1,159 @@
-Zip file size: 91417 bytes, number of entries: 73
--rw-r--r--  2.0 unx      356 b- defN 23-Oct-15 17:33 exllamav2/__init__.py
--rw-r--r--  2.0 unx    26117 b- defN 23-Oct-22 17:34 exllamav2/attn.py
--rw-r--r--  2.0 unx     7879 b- defN 23-Oct-22 15:45 exllamav2/cache.py
--rw-r--r--  2.0 unx     1839 b- defN 23-Oct-14 13:47 exllamav2/compat.py
--rw-r--r--  2.0 unx     6329 b- defN 23-Nov-05 01:28 exllamav2/config.py
--rw-r--r--  2.0 unx     3452 b- defN 23-Oct-22 18:11 exllamav2/embedding.py
--rw-r--r--  2.0 unx     5910 b- defN 23-Nov-10 19:13 exllamav2/ext.py
--rw-r--r--  2.0 unx     5603 b- defN 23-Oct-14 13:47 exllamav2/linear.py
--rw-r--r--  2.0 unx     6501 b- defN 23-Oct-22 18:11 exllamav2/lora.py
--rw-r--r--  2.0 unx     7512 b- defN 23-Oct-22 17:34 exllamav2/mlp.py
--rw-r--r--  2.0 unx    23708 b- defN 23-Nov-21 03:04 exllamav2/model.py
--rw-r--r--  2.0 unx     3114 b- defN 23-Oct-22 16:56 exllamav2/model_init.py
--rw-r--r--  2.0 unx     3540 b- defN 23-Nov-05 01:22 exllamav2/module.py
--rw-r--r--  2.0 unx     2298 b- defN 23-Nov-05 01:32 exllamav2/rmsnorm.py
--rw-r--r--  2.0 unx    12003 b- defN 23-Nov-07 23:47 exllamav2/tokenizer.py
--rw-r--r--  2.0 unx     2367 b- defN 23-Oct-14 13:43 exllamav2/util.py
--rw-r--r--  2.0 unx       21 b- defN 23-Nov-22 04:45 exllamav2/version.py
--rw-r--r--  2.0 unx      199 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/config.h
--rw-r--r--  2.0 unx    28327 b- defN 23-Nov-21 06:28 exllamav2/exllamav2_ext/ext.cpp
--rw-r--r--  2.0 unx     1530 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cpp/quantize_func.cpp
--rw-r--r--  2.0 unx      446 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cpp/quantize_func.h
--rw-r--r--  2.0 unx    15491 b- defN 23-Nov-21 06:31 exllamav2/exllamav2_ext/cpp/sampling.cpp
--rw-r--r--  2.0 unx     1940 b- defN 23-Nov-21 05:30 exllamav2/exllamav2_ext/cpp/sampling.h
--rw-r--r--  2.0 unx      932 b- defN 23-Oct-14 13:47 exllamav2/exllamav2_ext/cpp/util.h
--rw-r--r--  2.0 unx     4557 b- defN 23-Oct-15 20:43 exllamav2/exllamav2_ext/cuda/cache.cu
--rw-r--r--  2.0 unx      530 b- defN 23-Oct-15 19:19 exllamav2/exllamav2_ext/cuda/cache.cuh
--rw-r--r--  2.0 unx     1569 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/compat.cuh
--rw-r--r--  2.0 unx     7201 b- defN 23-Nov-10 19:13 exllamav2/exllamav2_ext/cuda/h_gemm.cu
--rw-r--r--  2.0 unx      447 b- defN 23-Oct-14 13:47 exllamav2/exllamav2_ext/cuda/h_gemm.cuh
--rw-r--r--  2.0 unx      935 b- defN 23-Oct-14 13:47 exllamav2/exllamav2_ext/cuda/lora.cu
--rw-r--r--  2.0 unx      463 b- defN 23-Oct-14 13:47 exllamav2/exllamav2_ext/cuda/lora.cuh
--rw-r--r--  2.0 unx     4293 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/matrix_view.cuh
--rw-r--r--  2.0 unx     7287 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/pack_tensor.cu
--rw-r--r--  2.0 unx      549 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/pack_tensor.cuh
--rw-r--r--  2.0 unx     5327 b- defN 23-Oct-14 13:47 exllamav2/exllamav2_ext/cuda/q_attn.cu
--rw-r--r--  2.0 unx     2018 b- defN 23-Oct-14 13:47 exllamav2/exllamav2_ext/cuda/q_attn.cuh
--rw-r--r--  2.0 unx     5372 b- defN 23-Nov-10 19:13 exllamav2/exllamav2_ext/cuda/q_gemm.cu
--rw-r--r--  2.0 unx      512 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/q_gemm.cuh
--rw-r--r--  2.0 unx    16616 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
--rw-r--r--  2.0 unx     6657 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
--rw-r--r--  2.0 unx    18225 b- defN 23-Oct-28 18:26 exllamav2/exllamav2_ext/cuda/q_matrix.cu
--rw-r--r--  2.0 unx     1256 b- defN 23-Oct-28 17:40 exllamav2/exllamav2_ext/cuda/q_matrix.cuh
--rw-r--r--  2.0 unx     4099 b- defN 23-Oct-22 17:34 exllamav2/exllamav2_ext/cuda/q_mlp.cu
--rw-r--r--  2.0 unx     1161 b- defN 23-Oct-14 13:47 exllamav2/exllamav2_ext/cuda/q_mlp.cuh
--rw-r--r--  2.0 unx     5208 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quantize.cu
--rw-r--r--  2.0 unx      831 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quantize.cuh
--rw-r--r--  2.0 unx     3329 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/rms_norm.cu
+Zip file size: 247086 bytes, number of entries: 157
+-rw-r--r--  2.0 unx      482 b- defN 24-May-25 20:52 exllamav2/__init__.py
+-rw-r--r--  2.0 unx    23883 b- defN 24-May-20 21:07 exllamav2/architecture.py
+-rw-r--r--  2.0 unx    34762 b- defN 24-May-25 20:52 exllamav2/attn.py
+-rw-r--r--  2.0 unx    16389 b- defN 24-May-25 20:52 exllamav2/cache.py
+-rw-r--r--  2.0 unx     2001 b- defN 24-May-16 22:11 exllamav2/compat.py
+-rw-r--r--  2.0 unx    12385 b- defN 24-May-16 01:34 exllamav2/config.py
+-rw-r--r--  2.0 unx     4545 b- defN 24-May-25 20:52 exllamav2/embedding.py
+-rw-r--r--  2.0 unx    12302 b- defN 24-May-25 23:35 exllamav2/ext.py
+-rw-r--r--  2.0 unx     6399 b- defN 24-May-11 12:57 exllamav2/fasttensors.py
+-rw-r--r--  2.0 unx     3969 b- defN 24-Apr-18 13:58 exllamav2/headnorm.py
+-rw-r--r--  2.0 unx     3502 b- defN 24-Apr-18 13:58 exllamav2/layernorm.py
+-rw-r--r--  2.0 unx    11159 b- defN 24-May-25 20:52 exllamav2/linear.py
+-rw-r--r--  2.0 unx     6408 b- defN 24-Apr-18 13:58 exllamav2/lora.py
+-rw-r--r--  2.0 unx    12405 b- defN 24-May-08 15:48 exllamav2/mlp.py
+-rw-r--r--  2.0 unx    31792 b- defN 24-May-25 20:52 exllamav2/model.py
+-rw-r--r--  2.0 unx     4737 b- defN 24-Mar-29 18:08 exllamav2/model_init.py
+-rw-r--r--  2.0 unx     7100 b- defN 24-May-11 12:57 exllamav2/module.py
+-rw-r--r--  2.0 unx    14288 b- defN 24-Apr-18 13:58 exllamav2/moe_mlp.py
+-rw-r--r--  2.0 unx     5262 b- defN 24-Apr-18 13:58 exllamav2/parallel_decoder.py
+-rw-r--r--  2.0 unx     3172 b- defN 24-May-11 12:57 exllamav2/pos_embedding.py
+-rw-r--r--  2.0 unx     3646 b- defN 24-Apr-18 13:58 exllamav2/rmsnorm.py
+-rw-r--r--  2.0 unx     6187 b- defN 24-May-25 20:52 exllamav2/util.py
+-rw-r--r--  2.0 unx       21 b- defN 24-May-25 20:52 exllamav2/version.py
+-rw-r--r--  2.0 unx      337 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/config.h
+-rw-r--r--  2.0 unx     4378 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_bindings.cpp
+-rw-r--r--  2.0 unx     8037 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_cache.cpp
+-rw-r--r--  2.0 unx     1105 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_cache.h
+-rw-r--r--  2.0 unx     1455 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_gemm.cpp
+-rw-r--r--  2.0 unx      161 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_gemm.h
+-rw-r--r--  2.0 unx     2505 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/ext_hadamard.cpp
+-rw-r--r--  2.0 unx       82 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/ext_hadamard.h
+-rw-r--r--  2.0 unx     3158 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_norm.cpp
+-rw-r--r--  2.0 unx      639 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_norm.h
+-rw-r--r--  2.0 unx     6577 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/ext_qattn.cpp
+-rw-r--r--  2.0 unx     1764 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/ext_qattn.h
+-rw-r--r--  2.0 unx     5665 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_qmatrix.cpp
+-rw-r--r--  2.0 unx      818 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_qmatrix.h
+-rw-r--r--  2.0 unx     8846 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_qmlp.cpp
+-rw-r--r--  2.0 unx     2274 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_qmlp.h
+-rw-r--r--  2.0 unx     5533 b- defN 24-Apr-28 15:55 exllamav2/exllamav2_ext/ext_quant.cpp
+-rw-r--r--  2.0 unx      895 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/ext_quant.h
+-rw-r--r--  2.0 unx     1339 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_rope.cpp
+-rw-r--r--  2.0 unx      186 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_rope.h
+-rw-r--r--  2.0 unx      344 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_safetensors.cpp
+-rw-r--r--  2.0 unx       29 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_safetensors.h
+-rw-r--r--  2.0 unx     9654 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_sampling.cpp
+-rw-r--r--  2.0 unx     1379 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/ext_sampling.h
+-rw-r--r--  2.0 unx     1275 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/avx2_target.h
+-rw-r--r--  2.0 unx    24414 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/avx_mathfun.h
+-rw-r--r--  2.0 unx     1299 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cpp/generator.cpp
+-rw-r--r--  2.0 unx      274 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cpp/generator.h
+-rw-r--r--  2.0 unx     1627 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/profiling.cpp
+-rw-r--r--  2.0 unx      362 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/profiling.h
+-rw-r--r--  2.0 unx     2265 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cpp/quantize_func.cpp
+-rw-r--r--  2.0 unx      611 b- defN 24-Mar-19 16:45 exllamav2/exllamav2_ext/cpp/quantize_func.h
+-rw-r--r--  2.0 unx     8308 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/cpp/safetensors.cpp
+-rw-r--r--  2.0 unx      801 b- defN 24-Jan-20 11:49 exllamav2/exllamav2_ext/cpp/safetensors.h
+-rw-r--r--  2.0 unx    20446 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cpp/sampling.cpp
+-rw-r--r--  2.0 unx     2283 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/sampling.h
+-rw-r--r--  2.0 unx     3089 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp
+-rw-r--r--  2.0 unx      334 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/sampling_avx2.h
+-rw-r--r--  2.0 unx     2204 b- defN 24-Mar-27 03:08 exllamav2/exllamav2_ext/cpp/util.h
+-rw-r--r--  2.0 unx    12970 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cuda/cache.cu
+-rw-r--r--  2.0 unx     1658 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cuda/cache.cuh
+-rw-r--r--  2.0 unx     2831 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/compat.cuh
+-rw-r--r--  2.0 unx     2604 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_add.cu
+-rw-r--r--  2.0 unx      265 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_add.cuh
+-rw-r--r--  2.0 unx     7202 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_gemm.cu
+-rw-r--r--  2.0 unx      667 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_gemm.cuh
+-rw-r--r--  2.0 unx     3160 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/head_norm.cu
+-rw-r--r--  2.0 unx      329 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/head_norm.cuh
+-rw-r--r--  2.0 unx     4970 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cuda/layer_norm.cu
+-rw-r--r--  2.0 unx      302 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/layer_norm.cuh
+-rw-r--r--  2.0 unx      935 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/lora.cu
+-rw-r--r--  2.0 unx      463 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/lora.cuh
+-rw-r--r--  2.0 unx     4293 b- defN 24-Apr-16 01:26 exllamav2/exllamav2_ext/cuda/matrix_view.cuh
+-rw-r--r--  2.0 unx     7287 b- defN 24-Apr-16 01:26 exllamav2/exllamav2_ext/cuda/pack_tensor.cu
+-rw-r--r--  2.0 unx      549 b- defN 24-Apr-16 01:26 exllamav2/exllamav2_ext/cuda/pack_tensor.cuh
+-rw-r--r--  2.0 unx     6642 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cuda/q_attn.cu
+-rw-r--r--  2.0 unx     2392 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cuda/q_attn.cuh
+-rw-r--r--  2.0 unx     8809 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/q_gemm.cu
+-rw-r--r--  2.0 unx      614 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/q_gemm.cuh
+-rw-r--r--  2.0 unx     1705 b- defN 24-Mar-19 16:45 exllamav2/exllamav2_ext/cuda/q_gemm_autotune.cuh
+-rw-r--r--  2.0 unx    19672 b- defN 24-Apr-14 21:33 exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
+-rw-r--r--  2.0 unx     7516 b- defN 24-Jan-20 11:49 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
+-rw-r--r--  2.0 unx     7516 b- defN 24-Jan-12 04:49 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq_old.cuh
+-rw-r--r--  2.0 unx    21853 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/cuda/q_matrix.cu
+-rw-r--r--  2.0 unx     1910 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cuda/q_matrix.cuh
+-rw-r--r--  2.0 unx     8195 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/q_mlp.cu
+-rw-r--r--  2.0 unx     2924 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/cuda/q_mlp.cuh
+-rw-r--r--  2.0 unx     5762 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/q_mlp_activation.cuh
+-rw-r--r--  2.0 unx     7296 b- defN 24-Mar-30 08:21 exllamav2/exllamav2_ext/cuda/q_mlp_softmax.cuh
+-rw-r--r--  2.0 unx     9975 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cuda/quantize.cu
+-rw-r--r--  2.0 unx     1303 b- defN 24-Apr-28 15:54 exllamav2/exllamav2_ext/cuda/quantize.cuh
+-rw-r--r--  2.0 unx     3834 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cuda/rms_norm.cu
 -rw-r--r--  2.0 unx      277 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/rms_norm.cuh
--rw-r--r--  2.0 unx     4016 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/rope.cu
--rw-r--r--  2.0 unx      366 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/rope.cuh
--rw-r--r--  2.0 unx     1925 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/util.cuh
--rw-r--r--  2.0 unx     2881 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_2.cuh
+-rw-r--r--  2.0 unx     5085 b- defN 24-Mar-10 20:56 exllamav2/exllamav2_ext/cuda/rms_norm_.cu
+-rw-r--r--  2.0 unx     7215 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/rope.cu
+-rw-r--r--  2.0 unx      736 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/rope.cuh
+-rw-r--r--  2.0 unx      624 b- defN 23-Dec-16 22:37 exllamav2/exllamav2_ext/cuda/util.cu
+-rw-r--r--  2.0 unx     3345 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/util.cuh
+-rw-r--r--  2.0 unx     1313 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu
+-rw-r--r--  2.0 unx     8080 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cuh
+-rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1b.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2b.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3b.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_1.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_2.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_3.cu
+-rw-r--r--  2.0 unx     2881 b- defN 23-Nov-27 16:47 exllamav2/exllamav2_ext/cuda/quant/qdq_2.cuh
 -rw-r--r--  2.0 unx     5782 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_3.cuh
 -rw-r--r--  2.0 unx     5755 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh
 -rw-r--r--  2.0 unx     7342 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_5.cuh
--rw-r--r--  2.0 unx      958 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh
+-rw-r--r--  2.0 unx     4530 b- defN 24-Mar-11 04:47 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh
 -rw-r--r--  2.0 unx      643 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_8.cuh
--rw-r--r--  2.0 unx     1272 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh
--rw-r--r--  2.0 unx      235 b- defN 23-Oct-14 13:47 exllamav2/generator/__init__.py
--rw-r--r--  2.0 unx     4215 b- defN 23-Oct-27 00:01 exllamav2/generator/base.py
--rw-r--r--  2.0 unx     6126 b- defN 23-Nov-21 06:27 exllamav2/generator/sampler.py
--rw-r--r--  2.0 unx    13303 b- defN 23-Nov-18 06:36 exllamav2/generator/streaming.py
--rw-r--r--  2.0 unx      173 b- defN 23-Oct-14 13:47 exllamav2/generator/filters/__init__.py
--rw-r--r--  2.0 unx      592 b- defN 23-Oct-14 13:47 exllamav2/generator/filters/base.py
--rw-r--r--  2.0 unx     3319 b- defN 23-Oct-14 13:47 exllamav2/generator/filters/select.py
+-rw-r--r--  2.0 unx     1367 b- defN 24-May-01 22:03 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh
+-rw-r--r--  2.0 unx      513 b- defN 24-May-25 20:52 exllamav2/generator/__init__.py
+-rw-r--r--  2.0 unx    12853 b- defN 24-May-25 20:52 exllamav2/generator/base.py
+-rw-r--r--  2.0 unx    80386 b- defN 24-May-25 22:25 exllamav2/generator/dynamic.py
+-rw-r--r--  2.0 unx     2687 b- defN 24-May-25 20:52 exllamav2/generator/dynamic_async.py
+-rw-r--r--  2.0 unx      497 b- defN 24-Mar-29 18:08 exllamav2/generator/hooks.py
+-rw-r--r--  2.0 unx     2370 b- defN 24-Mar-29 18:08 exllamav2/generator/ngram.py
+-rw-r--r--  2.0 unx    10162 b- defN 24-May-25 20:52 exllamav2/generator/sampler.py
+-rw-r--r--  2.0 unx    39172 b- defN 24-May-25 20:52 exllamav2/generator/streaming.py
+-rw-r--r--  2.0 unx      242 b- defN 24-Feb-27 07:47 exllamav2/generator/filters/__init__.py
+-rw-r--r--  2.0 unx      756 b- defN 24-Mar-29 18:08 exllamav2/generator/filters/base.py
+-rw-r--r--  2.0 unx     1866 b- defN 24-Mar-29 18:08 exllamav2/generator/filters/prefix.py
+-rw-r--r--  2.0 unx     3965 b- defN 24-Mar-29 18:08 exllamav2/generator/filters/select.py
+-rw-r--r--  2.0 unx     4346 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard.py
+-rw-r--r--  2.0 unx        1 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_1.txt
+-rw-r--r--  2.0 unx    10099 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_100.txt
+-rw-r--r--  2.0 unx    13571 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_116.txt
+-rw-r--r--  2.0 unx    24491 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_156.txt
+-rw-r--r--  2.0 unx    29755 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_172.txt
+-rw-r--r--  2.0 unx    35531 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_188.txt
+-rw-r--r--  2.0 unx    55931 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_236.txt
+-rw-r--r--  2.0 unx    59779 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_244.txt
+-rw-r--r--  2.0 unx   183612 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_428.txt
+-rw-r--r--  2.0 unx     2755 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_52.txt
+-rw-r--r--  2.0 unx     8555 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_92.txt
+-rw-r--r--  2.0 unx    58982 b- defN 24-Apr-21 13:11 exllamav2/hadamard/primes.txt
 -rw-r--r--  2.0 unx      107 b- defN 23-Oct-14 13:47 exllamav2/server/__init__.py
--rw-r--r--  2.0 unx     1286 b- defN 23-Oct-14 13:47 exllamav2/server/websocket.py
--rw-r--r--  2.0 unx     7437 b- defN 23-Nov-08 07:56 exllamav2/server/websocket_actions.py
--rw-r--r--  2.0 unx     1035 b- defN 23-Nov-22 06:31 exllamav2-0.0.9.dist-info/LICENSE
--rw-r--r--  2.0 unx      406 b- defN 23-Nov-22 06:31 exllamav2-0.0.9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Nov-22 06:31 exllamav2-0.0.9.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Nov-22 06:31 exllamav2-0.0.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6554 b- defN 23-Nov-22 06:31 exllamav2-0.0.9.dist-info/RECORD
-73 files, 341963 bytes uncompressed, 80917 bytes compressed:  76.3%
+-rw-r--r--  2.0 unx     1623 b- defN 24-Jan-20 11:49 exllamav2/server/websocket.py
+-rw-r--r--  2.0 unx    10260 b- defN 24-May-09 16:29 exllamav2/server/websocket_actions.py
+-rw-r--r--  2.0 unx      217 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/__init__.py
+-rw-r--r--  2.0 unx     2157 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/base.py
+-rw-r--r--  2.0 unx     3028 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/hf.py
+-rw-r--r--  2.0 unx     1972 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/spm.py
+-rw-r--r--  2.0 unx    23709 b- defN 24-May-25 20:52 exllamav2/tokenizer/tokenizer.py
+-rw-r--r--  2.0 unx     1035 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx      427 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    14579 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/RECORD
+157 files, 1295085 bytes uncompressed, 223716 bytes compressed:  82.7%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: exllamav2/__init__.py
 Comment: 
 
+Filename: exllamav2/architecture.py
+Comment: 
+
 Filename: exllamav2/attn.py
 Comment: 
 
 Filename: exllamav2/cache.py
 Comment: 
 
 Filename: exllamav2/compat.py
@@ -15,14 +18,23 @@
 
 Filename: exllamav2/embedding.py
 Comment: 
 
 Filename: exllamav2/ext.py
 Comment: 
 
+Filename: exllamav2/fasttensors.py
+Comment: 
+
+Filename: exllamav2/headnorm.py
+Comment: 
+
+Filename: exllamav2/layernorm.py
+Comment: 
+
 Filename: exllamav2/linear.py
 Comment: 
 
 Filename: exllamav2/lora.py
 Comment: 
 
 Filename: exllamav2/mlp.py
@@ -33,62 +45,182 @@
 
 Filename: exllamav2/model_init.py
 Comment: 
 
 Filename: exllamav2/module.py
 Comment: 
 
-Filename: exllamav2/rmsnorm.py
+Filename: exllamav2/moe_mlp.py
+Comment: 
+
+Filename: exllamav2/parallel_decoder.py
 Comment: 
 
-Filename: exllamav2/tokenizer.py
+Filename: exllamav2/pos_embedding.py
+Comment: 
+
+Filename: exllamav2/rmsnorm.py
 Comment: 
 
 Filename: exllamav2/util.py
 Comment: 
 
 Filename: exllamav2/version.py
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/config.h
 Comment: 
 
-Filename: exllamav2/exllamav2_ext/ext.cpp
+Filename: exllamav2/exllamav2_ext/ext_bindings.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_cache.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_cache.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_gemm.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_gemm.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_hadamard.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_hadamard.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_norm.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_norm.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_qattn.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_qattn.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_qmatrix.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_qmatrix.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_qmlp.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_qmlp.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_quant.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_quant.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_rope.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_rope.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_safetensors.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_safetensors.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_sampling.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/ext_sampling.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/avx2_target.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/avx_mathfun.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/generator.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/generator.h
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/profiling.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/profiling.h
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cpp/quantize_func.cpp
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cpp/quantize_func.h
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cpp/safetensors.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/safetensors.h
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cpp/sampling.cpp
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cpp/sampling.h
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cpp/sampling_avx2.h
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cpp/util.h
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/cache.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/cache.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/compat.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/h_add.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/h_add.cuh
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/h_gemm.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/h_gemm.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/head_norm.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/head_norm.cuh
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/layer_norm.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/layer_norm.cuh
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/lora.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/lora.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/matrix_view.cuh
@@ -108,53 +240,104 @@
 
 Filename: exllamav2/exllamav2_ext/cuda/q_gemm.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_gemm.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/q_gemm_autotune.cuh
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq_old.cuh
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/q_matrix.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_matrix.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_mlp.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_mlp.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/q_mlp_activation.cuh
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/q_mlp_softmax.cuh
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/quantize.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/quantize.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/rms_norm.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/rms_norm.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/rms_norm_.cu
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/rope.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/rope.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/util.cu
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/util.cuh
 Comment: 
 
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cuh
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1b.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2a.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2b.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3a.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3b.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_1.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_2.cu
+Comment: 
+
+Filename: exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_3.cu
+Comment: 
+
 Filename: exllamav2/exllamav2_ext/cuda/quant/qdq_2.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/quant/qdq_3.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh
@@ -174,47 +357,116 @@
 
 Filename: exllamav2/generator/__init__.py
 Comment: 
 
 Filename: exllamav2/generator/base.py
 Comment: 
 
+Filename: exllamav2/generator/dynamic.py
+Comment: 
+
+Filename: exllamav2/generator/dynamic_async.py
+Comment: 
+
+Filename: exllamav2/generator/hooks.py
+Comment: 
+
+Filename: exllamav2/generator/ngram.py
+Comment: 
+
 Filename: exllamav2/generator/sampler.py
 Comment: 
 
 Filename: exllamav2/generator/streaming.py
 Comment: 
 
 Filename: exllamav2/generator/filters/__init__.py
 Comment: 
 
 Filename: exllamav2/generator/filters/base.py
 Comment: 
 
+Filename: exllamav2/generator/filters/prefix.py
+Comment: 
+
 Filename: exllamav2/generator/filters/select.py
 Comment: 
 
+Filename: exllamav2/hadamard/hadamard.py
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_1.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_100.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_116.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_156.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_172.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_188.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_236.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_244.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_428.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_52.txt
+Comment: 
+
+Filename: exllamav2/hadamard/hadamard_92.txt
+Comment: 
+
+Filename: exllamav2/hadamard/primes.txt
+Comment: 
+
 Filename: exllamav2/server/__init__.py
 Comment: 
 
 Filename: exllamav2/server/websocket.py
 Comment: 
 
 Filename: exllamav2/server/websocket_actions.py
 Comment: 
 
-Filename: exllamav2-0.0.9.dist-info/LICENSE
+Filename: exllamav2/tokenizer/__init__.py
+Comment: 
+
+Filename: exllamav2/tokenizer/base.py
+Comment: 
+
+Filename: exllamav2/tokenizer/hf.py
+Comment: 
+
+Filename: exllamav2/tokenizer/spm.py
+Comment: 
+
+Filename: exllamav2/tokenizer/tokenizer.py
+Comment: 
+
+Filename: exllamav2-0.1.0.dist-info/LICENSE
 Comment: 
 
-Filename: exllamav2-0.0.9.dist-info/METADATA
+Filename: exllamav2-0.1.0.dist-info/METADATA
 Comment: 
 
-Filename: exllamav2-0.0.9.dist-info/WHEEL
+Filename: exllamav2-0.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: exllamav2-0.0.9.dist-info/top_level.txt
+Filename: exllamav2-0.1.0.dist-info/top_level.txt
 Comment: 
 
-Filename: exllamav2-0.0.9.dist-info/RECORD
+Filename: exllamav2-0.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## exllamav2/__init__.py

```diff
@@ -1,9 +1,12 @@
 from exllamav2.version import __version__
 
 from exllamav2.model import ExLlamaV2
 from exllamav2.cache import ExLlamaV2CacheBase
 from exllamav2.cache import ExLlamaV2Cache
+from exllamav2.cache import ExLlamaV2Cache_Q4
 from exllamav2.cache import ExLlamaV2Cache_8bit
 from exllamav2.config import ExLlamaV2Config
-from exllamav2.tokenizer import ExLlamaV2Tokenizer
+from exllamav2.tokenizer.tokenizer import ExLlamaV2Tokenizer
 from exllamav2.lora import ExLlamaV2Lora
+from exllamav2.util import SeqTensor
+from exllamav2.util import Timer
```

## exllamav2/attn.py

```diff
@@ -1,162 +1,413 @@
+from __future__ import annotations
+
 import torch
 from torch import nn
-import torch.nn.functional as F
 from exllamav2.module import ExLlamaV2Module
 from exllamav2.rmsnorm import ExLlamaV2RMSNorm
+from exllamav2.layernorm import ExLlamaV2LayerNorm
+from exllamav2.headnorm import ExLlamaV2HeadNorm
 from exllamav2.linear import ExLlamaV2Linear
 from exllamav2.cache import ExLlamaV2CacheBase
-from exllamav2.embedding import ExLlamaV2Embedding
+from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
+from exllamav2.compat import safe_move_tensor
+from exllamav2.lora import ExLlamaV2Lora
+from exllamav2.architecture import RopeStyle
 import math
-from exllamav2 import ext
-from exllamav2.ext import exllamav2_ext as ext_c
 # import xformers.ops as xops
 # from exllamav2.util import list_live_tensors, set_snapshot, diff_snapshot, print_vram_usage_peak
+# import torch.nn.functional as F
+
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.model import ExLlamaV2
 
 # Detect flash-attn
 
 has_flash_attn = False
+has_flash_attn_with_paged = False
+
 try:
     import flash_attn
     flash_attn_ver = [int(t) for t in flash_attn.__version__.split(".") if t.isdigit()]
     is_ampere_or_newer_gpu = any(torch.cuda.get_device_properties(i).major >= 8 for i in range(torch.cuda.device_count()))
-    
-    if flash_attn_ver >= [2, 2, 1] and is_ampere_or_newer_gpu:
+
+    if not is_ampere_or_newer_gpu:
+        print(" ## Warning: Flash Attention is installed but unsupported GPUs were detected.")
+
+    if [2, 2, 1] <= flash_attn_ver < [2, 5, 7]:
         from flash_attn import flash_attn_func
         has_flash_attn = True
+
+    if [2, 5, 7] <= flash_attn_ver:
+        from flash_attn import flash_attn_func, flash_attn_with_kvcache
+        has_flash_attn = True
+        has_flash_attn_with_paged = True
+
 except ModuleNotFoundError:
     pass
 
-class ExLlamaV2Attention(ExLlamaV2Module):
+def assert_paged_attn():
+    global has_flash_attn_with_paged
+    assert has_flash_attn_with_paged, \
+        "Paged attention required Flash Attention 2.5.7 or later"
 
-    layer_idx: int
-    input_layernorm: ExLlamaV2RMSNorm or None
-    q_proj: ExLlamaV2Linear or None
-    k_proj: ExLlamaV2Linear or None
-    v_proj: ExLlamaV2Linear or None
-    o_proj: ExLlamaV2Linear
+
+class ExLlamaV2Attention(ExLlamaV2Module):
 
     name: str = "Attention"
-    submodules: list
 
-    q_handle: int or None = None
+    layer_idx: int
+    input_layernorm: ExLlamaV2RMSNorm | ExLlamaV2LayerNorm | None
+    q_proj: ExLlamaV2Linear | None
+    k_proj: ExLlamaV2Linear | None
+    v_proj: ExLlamaV2Linear | None
+    o_proj: ExLlamaV2Linear | None
+    q_norm: ExLlamaV2HeadNorm | None
+    k_norm: ExLlamaV2HeadNorm | None
+
+    q_handle: int | None
 
     temp_state: torch.tensor
     temp_q: torch.tensor
     temp_k: torch.tensor
     temp_v: torch.tensor
     temp_o: torch.tensor
     temp_dq: torch.tensor
     # temp_kv: torch.tensor
 
-    temp_lora_size: int = 0
+    temp_lora_size: int
 
+    has_norm: bool
+    has_residual: bool
+
+
+    class Params:
+
+        batch_size: int
+        seq_len: int
+        past_len: int | None
+        past_lens: list[int] | None
+        input_mask: torch.Tensor | None
+        multi_cache: bool
+        attn_mask: torch.Tensor | None
+        attn_masks: torch.Tensor | None
+        position_offsets: torch.Tensor | None
+        past_lens_tensor: torch.Tensor | None
+        paged: bool
+
+        def __init__(
+            self,
+            batch_size: int,
+            seq_len: int | None = None,
+            past_len: int | list[int] | None = None,
+            input_mask: torch.Tensor | None = None,
+            position_offsets: torch.Tensor | None = None,
+            paged = False
+        ):
+
+            self.batch_size = batch_size
+            self.paged = paged
+
+            if paged: return
+
+            self.seq_len = seq_len
+            if isinstance(past_len, list):
+                self.past_len = None
+                self.past_lens = past_len
+                self.multi_cache = True
+            else:
+                self.past_len = past_len
+                self.past_lens = None
+                self.multi_cache = False
+            self.input_mask = input_mask
+
+            self.attn_mask = None
+            self.attn_masks = None
+
+            self.position_offsets = position_offsets
+            self.past_lens_tensor = None
+            self.paged = paged
+
+
+        def is_causal(self) -> bool:
+            return self.input_mask is None
+
+        def get_position_offsets(self, device) -> torch.Tensor | None:
+            assert self.position_offsets is not None
+            if self.position_offsets.device != device:
+                self.position_offsets = safe_move_tensor(self.position_offsets, device)
+            return self.position_offsets
+
+        def get_past_lens(self, device) -> torch.Tensor | None:
+            assert self.past_lens is not None
+            if self.past_lens_tensor is None:
+                self.past_lens_tensor = torch.tensor(self.past_lens, dtype = torch.int, device = device)
+            elif self.past_lens_tensor.device != device:
+                self.past_lens_tensor = safe_move_tensor(self.past_lens_tensor, device)
+            return self.past_lens_tensor
+
+        def get_attn_mask(self, device) -> torch.Tensor | None:
+            if self.attn_mask is None:
+                self.attn_mask = self.build_attn_mask(device)
+            elif self.attn_mask.device != device:
+                self.attn_mask = safe_move_tensor(self.attn_mask, device)
+            return self.attn_mask
+
+        def get_attn_masks(self, device) -> torch.Tensor | None:
+            if self.attn_masks is None:
+                self.attn_masks = self.build_attn_masks(device)
+            elif self.attn_masks[0] is not None and self.attn_masks[0].device != device:
+                self.attn_masks = [(safe_move_tensor(m, device) if m is not None else None) for m in self.attn_masks]
+            return self.attn_masks
+
+        def build_single_attn_mask(self, batch_size, seq_len, past_len, device, input_mask):
+            attn_mask = torch.zeros((batch_size, 1, seq_len, past_len + seq_len), dtype = torch.float16, device = device)
+            attn_mask_triu = torch.triu(torch.full((seq_len - 1, seq_len - 1), -65504.0))
+            attn_mask[:, :, : seq_len - 1, past_len + 1: past_len + seq_len] = attn_mask_triu
+            if input_mask is not None:
+                min_mask_width = min(input_mask.shape[-1], seq_len + past_len)
+                input_mask_part = safe_move_tensor(input_mask[:, :min_mask_width], attn_mask.device)
+                input_mask_part = input_mask_part.unsqueeze(1).unsqueeze(2)
+                attn_mask[:, :, :, :min_mask_width] = torch.minimum(attn_mask[:, :, :, :min_mask_width], input_mask_part)
+            return attn_mask
+
+        def build_attn_mask(self, device) -> torch.Tensor | None:
+            assert not self.multi_cache, "Building single mask for multiple caches"
+            if self.input_mask is None and self.seq_len == 1: return None
+            return self.build_single_attn_mask(self.batch_size, self.seq_len, self.past_len, device, self.input_mask)
+
+        def build_attn_masks(self, device) -> torch.Tensor | None:
+            assert self.multi_cache, "Building multiple masks for single cache"
+            attn_masks = []
+            for i, past_len in enumerate(self.past_lens):
+                if self.input_mask is None and self.seq_len == 1:
+                    attn_masks.append(None)
+                else:
+                    attn_masks.append(self.build_single_attn_mask(1, self.seq_len, past_len, device, self.input_mask[i]))
+            return attn_masks
+
+
+    class PagedParams(Params):
+
+        block_index: torch.Tensor
+        cache_seqlens: torch.Tensor
+        page_size: int
+
+        def __init__(
+            self,
+            batch_size: int,
+            block_index: torch.Tensor,
+            cache_seqlens: torch.Tensor,
+            page_size: int
+        ):
+            super().__init__(
+                batch_size = batch_size,
+                paged = True
+            )
+
+            self.block_index = block_index
+            self.cache_seqlens = cache_seqlens
+            self.page_size = page_size
+
+        def get_attn_mask(self, device):
+            raise NotImplementedError()
+
+        def get_block_index(self, device) -> torch.Tensor:
+            if self.block_index.device != device:
+                self.block_index = safe_move_tensor(self.block_index, device)
+            return self.block_index
+
+        def get_cache_seqlens(self, device) -> torch.Tensor:
+            if self.cache_seqlens.device != device:
+                self.cache_seqlens = safe_move_tensor(self.cache_seqlens, device)
+            return self.cache_seqlens
+
+
+    def __init__(self,
+                 model: ExLlamaV2,
+                 key: str,
+                 layer_idx: int,
+                 has_norm: bool = True,
+                 has_residual: bool = True):
 
-    def __init__(self, model, key, layer_idx):
         super().__init__(model, key)
 
+        cfg = self.model.config
+
         self.layer_idx = layer_idx
+        self.has_norm = has_norm
+        self.has_residual = has_residual
 
-        hidden_size = self.model.config.hidden_size
+        self.q_handle = None
+        self.temp_lora_size = 0
 
-        self.input_layernorm = ExLlamaV2RMSNorm(model, key + ".input_layernorm")
-        self.q_proj = ExLlamaV2Linear(model, key + ".self_attn.q_proj", hidden_size, self.model.config.num_attention_heads * self.model.config.head_dim, False)
-        self.k_proj = ExLlamaV2Linear(model, key + ".self_attn.k_proj", hidden_size, self.model.config.num_key_value_heads * self.model.config.head_dim, False)
-        self.v_proj = ExLlamaV2Linear(model, key + ".self_attn.v_proj", hidden_size, self.model.config.num_key_value_heads * self.model.config.head_dim, False)
-        self.o_proj = ExLlamaV2Linear(model, key + ".self_attn.o_proj", self.model.config.num_attention_heads * self.model.config.head_dim, hidden_size, False)
+        hidden_size = cfg.hidden_size
+
+        if self.has_norm:
+            if cfg.arch.norm == "layernorm":
+                self.input_layernorm = ExLlamaV2LayerNorm(model, key + cfg.arch.norm_key_1)
+            elif cfg.arch.norm == "rmsnorm":
+                self.input_layernorm = ExLlamaV2RMSNorm(model, key + cfg.arch.norm_key_1)
+        else:
+            self.input_layernorm = None
 
-        self.submodules = [self.input_layernorm,
-                           self.q_proj,
+        f_a = 0
+        f_b = cfg.num_attention_heads * cfg.head_dim
+        f_c = f_b + cfg.num_key_value_heads * cfg.head_dim
+        f_d = f_c + cfg.num_key_value_heads * cfg.head_dim
+        f_key = (key + ".self_attn." + cfg.arch.fused_qkv_key) if cfg.arch.fused_qkv_key else None
+
+        self.q_proj = ExLlamaV2Linear(model, key + ".self_attn.q_proj", hidden_size, cfg.num_attention_heads * cfg.head_dim, cfg.arch.attention_bias_qkv, f_key = f_key, f_beg = f_a, f_end = f_b)
+        self.k_proj = ExLlamaV2Linear(model, key + ".self_attn.k_proj", hidden_size, cfg.num_key_value_heads * cfg.head_dim, cfg.arch.attention_bias_qkv, f_key = f_key, f_beg = f_b, f_end = f_c)
+        self.v_proj = ExLlamaV2Linear(model, key + ".self_attn.v_proj", hidden_size, cfg.num_key_value_heads * cfg.head_dim, cfg.arch.attention_bias_qkv, f_key = f_key, f_beg = f_c, f_end = f_d)
+        self.o_proj = ExLlamaV2Linear(model, key + ".self_attn.o_proj", cfg.num_attention_heads * cfg.head_dim, hidden_size, cfg.arch.attention_bias_o)
+
+        if cfg.use_qk_norm:
+            self.q_norm = ExLlamaV2HeadNorm(model, key + ".self_attn.q_norm", cfg.num_attention_heads, cfg.head_dim)
+            self.k_norm = ExLlamaV2HeadNorm(model, key + ".self_attn.k_norm", cfg.num_key_value_heads, cfg.head_dim)
+        else:
+            self.q_norm = None
+            self.k_norm = None
+
+        self.submodules = [self.q_proj,
                            self.k_proj,
                            self.v_proj,
                            self.o_proj]
+        if self.has_norm:
+            self.submodules += [self.input_layernorm]
+        if cfg.use_qk_norm:
+            self.submodules += [self.q_norm,
+                                self.k_norm]
+
+        # if cfg.arch.scale_attn_weights:
+        #     self.unscale_factor = self.layer_idx + 1
+        #     self.scale_factor = 1 / self.unscale_factor
+        # else:
+        self.unscale_factor = 1
+        self.scale_factor = 1
+
+
+    def numel(self) -> int:
+
+        numel = self.q_proj.numel() + \
+                self.k_proj.numel() + \
+                self.v_proj.numel() + \
+                self.o_proj.numel()
+
+        if self.input_layernorm is not None: numel += self.input_layernorm.numel()
+        if self.q_norm is not None: numel += self.q_norm.numel()
+        if self.k_norm is not None: numel += self.k_norm.numel()
 
+        return numel
 
-    def load(self):
 
-        qkv_embed = self.model.config.qkv_embed and self.layer_idx == 0
+    def load(self):
 
-        self.input_layernorm.load()
+        if self.input_layernorm is not None: self.input_layernorm.load()
         self.q_proj.load()
         self.k_proj.load()
         self.v_proj.load()
         self.o_proj.load()
+        if self.q_norm is not None: self.q_norm.load()
+        if self.k_norm is not None: self.k_norm.load()
 
         if self.q_proj.is_quant():
 
             assert self.k_proj.is_quant() and self.v_proj.is_quant() and self.o_proj.is_quant(), "Partially quantized attention layer"
 
             device_tensors = self.model.get_device_tensors(self.device_idx)
             device_tensors.begin_scratch_alloc()
             self.temp_state = device_tensors.get_scratch_slice(self.temp_state_size())
             # self.temp_q = device_tensors.get_scratch_slice(self.temp_q_size())
             # self.temp_k = device_tensors.get_scratch_slice(self.temp_k_size())
             # self.temp_v = device_tensors.get_scratch_slice(self.temp_v_size())
             self.temp_dq = device_tensors.get_scratch_slice(self.temp_dq_size())
             # self.temp_kv = device_tensors.get_scratch_slice(self.temp_kv_size()) if self.model.config.num_attention_heads != self.model.config.num_key_value_heads else None
 
-            self.q_handle = ext_c.make_q_attn(self.input_layernorm.weight if not qkv_embed else ext.none_tensor,
-                                              self.input_layernorm.variance_epsilon if not qkv_embed else 0.0,
-                                              self.q_proj.q_handle if not qkv_embed else 0,
-                                              self.k_proj.q_handle if not qkv_embed else 0,
-                                              self.v_proj.q_handle if not qkv_embed else 0,
-                                              self.o_proj.q_handle,
-                                              self.temp_state,
-                                              # self.temp_q,
-                                              # self.temp_k,
-                                              # self.temp_v,
-                                              self.temp_dq,
-                                              self.model.config.max_input_len * self.model.config.max_batch_size,
-                                              self.model.config.hidden_size,
-                                              self.model.config.num_attention_heads,
-                                              self.model.config.num_key_value_heads,
-                                              self.model.config.head_dim,
-                                              self.model.config.max_seq_len)
-
-        if qkv_embed:
-
-            embedding = self.model.modules[0]
-            assert isinstance(embedding, ExLlamaV2Embedding)
-            q = self.q_proj.get_weight_tensor_dq()
-            k = self.k_proj.get_weight_tensor_dq()
-            v = self.v_proj.get_weight_tensor_dq()
-            norm = self.input_layernorm
-            embedding.make_qkv(norm, q, k, v)
-
-            self.q_proj.unload(); self.q_proj = None
-            self.k_proj.unload(); self.k_proj = None
-            self.v_proj.unload(); self.v_proj = None
-            self.input_layernorm.unload(); self.input_layernorm = None
+            if self.has_norm:
+                norm_weight = self.input_layernorm.weight if self.input_layernorm.weight is not None else none_tensor
+                norm_bias = self.input_layernorm.bias if self.input_layernorm.bias is not None else none_tensor
+                is_rms = isinstance(self.input_layernorm, ExLlamaV2RMSNorm)
+                eps = self.input_layernorm.variance_epsilon
+            else:
+                norm_weight = none_tensor
+                norm_bias = none_tensor
+                is_rms = False
+                eps = 0
+
+            if self.q_norm is None:
+                q_norm = none_tensor
+            else:
+                q_norm = self.q_norm.weight
+
+            if self.k_norm is None:
+                k_norm = none_tensor
+            else:
+                k_norm = self.k_norm.weight
+
+            self.q_handle = ext_c.make_q_attn(
+                norm_weight,
+                norm_bias,
+                is_rms,
+                eps,
+                self.q_proj.q_handle,
+                self.k_proj.q_handle,
+                self.v_proj.q_handle,
+                self.o_proj.q_handle,
+                self.temp_state,
+                # self.temp_q,
+                # self.temp_k,
+                # self.temp_v,
+                self.temp_dq,
+                self.model.config.max_input_len * self.model.config.max_batch_size,
+                self.model.config.hidden_size,
+                self.model.config.num_attention_heads,
+                self.model.config.num_key_value_heads,
+                self.model.config.head_dim,
+                self.model.config.max_seq_len,
+                self.has_residual,
+                self.model.config.arch.rope_style.value,
+                q_norm,
+                k_norm
+            )
 
 
     def unload(self):
         if self.q_handle is not None:
             ext_c.free_q_attn(self.q_handle)
             self.q_handle = None
 
         if self.input_layernorm is not None: self.input_layernorm.unload()
         if self.q_proj is not None: self.q_proj.unload()
         if self.k_proj is not None: self.k_proj.unload()
         if self.v_proj is not None: self.v_proj.unload()
         self.o_proj.unload()
 
+        self.temp_state = None
+        self.temp_dq = None
 
-    def weight_footprint(self, qkv_embed = False):
+        if self.q_norm is not None: self.q_norm.unload()
+        if self.k_norm is not None: self.k_norm.unload()
 
-        if self.layer_idx == 0 and self.model.config.qkv_embed:
 
-            return self.o_proj.weight_footprint()
+    def weight_footprint(self):
 
-        else:
+        fp = self.q_proj.weight_footprint() + \
+             self.k_proj.weight_footprint() + \
+             self.v_proj.weight_footprint() + \
+             self.o_proj.weight_footprint()
+        if self.input_layernorm is not None:
+            fp += self.input_layernorm.weight_footprint()
+        if self.q_norm is not None:
+            fp += self.q_norm.weight_footprint()
+        if self.k_norm is not None:
+            fp += self.k_norm.weight_footprint()
 
-            return self.input_layernorm.weight_footprint() + \
-                   self.q_proj.weight_footprint() + \
-                   self.k_proj.weight_footprint() + \
-                   self.v_proj.weight_footprint() + \
-                   self.o_proj.weight_footprint()
+        return fp
 
 
     def scratch_space_fixed(self):
 
         return self.temp_state_size() + \
                self.temp_dq_size()
 
@@ -170,15 +421,15 @@
                self.temp_dq_size() + \
                self.temp_kv_size()
                # self.temp_attn_size() +  # Accounted for separately in model.set_device_map()
 
 
     def temp_state_size(self):
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.hidden_size * 2 + 128
+        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.num_attention_heads * self.model.config.head_dim * 2 + 128
 
 
     def temp_q_size(self):
 
         return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.num_attention_heads * self.model.config.head_dim * 2 + 128
 
 
@@ -217,334 +468,332 @@
 
         return 2 * att_max * self.model.config.num_attention_heads * 2 + 128
 
 
     def set_device_idx(self, idx):
         super().set_device_idx(idx)
 
-        self.input_layernorm.set_device_idx(idx)
+        if self.input_layernorm is not None: self.input_layernorm.set_device_idx(idx)
         self.q_proj.set_device_idx(idx)
         self.k_proj.set_device_idx(idx)
         self.v_proj.set_device_idx(idx)
         self.o_proj.set_device_idx(idx)
+        if self.q_norm is not None: self.q_norm.set_device_idx(idx)
+        if self.k_norm is not None: self.k_norm.set_device_idx(idx)
 
 
     def repeat_kv(self, hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
 
         if n_rep == 1: return hidden_states
 
         batch, num_key_value_heads, slen, head_dim = hidden_states.shape
         hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
         hidden_states = hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
         return hidden_states
 
 
-    def forward(self, hidden_states, cache = None, attn_mask = None, past_len = None, intermediates = False, loras = None):
-        global has_flash_attn
-
-        qkv_embed = self.model.config.qkv_embed and self.layer_idx == 0
-
-        if self.q_handle is None or intermediates:
-            return self.forward_torch(hidden_states, cache, attn_mask, past_len, intermediates, loras = loras)
-
-        if qkv_embed:
-            batch_size = hidden_states[0].shape[0]
-            q_len = hidden_states[0].shape[1]
-        else:
-            batch_size = hidden_states.shape[0]
-            q_len = hidden_states.shape[1]
-
-        direct = (batch_size == 1 and cache is not None and isinstance(cache, ExLlamaV2CacheBase)) and not qkv_embed
-
-        # past_len = 0
-        # if cache is not None:
-        #     if isinstance(cache, ExLlamaV2Cache):
-        #         past_len = cache.current_seq_len
-        #     if isinstance(cache, list):
-        #         past_len = [c.current_seq_len for c in cache]
-
-        num_attention_heads = self.model.config.num_attention_heads
-        num_key_value_heads = self.model.config.num_key_value_heads
-        num_key_value_groups = self.model.config.num_key_value_groups
-        head_dim = self.model.config.head_dim
-        hidden_size = self.model.config.hidden_size
+    def forward_paged(self,
+                      hidden_states: torch.Tensor,
+                      cache: ExLlamaV2CacheBase | None = None,
+                      attn_params: ExLlamaV2Attention.PagedParams | None = None,
+                      loras: list[ExLlamaV2Lora] | None = None,
+                      **kwargs) -> torch.Tensor:
 
+        cfg = self.model.config
         constants = self.model.get_device_tensors(self.device_idx)
 
-        if not qkv_embed:
-
-            q_shape = hidden_states.shape[:-1] + (self.q_proj.out_features,)
-            k_shape = hidden_states.shape[:-1] + (self.k_proj.out_features,)
-            v_shape = hidden_states.shape[:-1] + (self.v_proj.out_features,)
-            q_states = torch.empty(q_shape, device = hidden_states.device, dtype = torch.half)
+        page_size = attn_params.page_size
 
-            # If conditions are right we can write the K/V projections directly into the cache
-
-            if direct:
-
-                batch_keys, batch_values = cache.get_kv_state(self.layer_idx, batch_size, 0, past_len)
-                k_states = batch_keys.narrow(0, 0, batch_size).narrow(1, past_len, q_len)
-                v_states = batch_values.narrow(0, 0, batch_size).narrow(1, past_len, q_len)
-
-            else:
-
-                k_states = torch.empty(k_shape, device = hidden_states.device, dtype = torch.half)
-                v_states = torch.empty(v_shape, device = hidden_states.device, dtype = torch.half)
-
-            # RMS norm, Q/K/V projections, position embeddings
-
-            if loras is None or self.temp_lora_size == 0:
-                pass_loras = []
-                pass_lora_temp = ext.none_tensor
-            else:
-                pass_loras = [id(x) for x in loras]
-                pass_lora_temp = torch.empty((self.temp_lora_size,), dtype = torch.half, device = hidden_states.device)
-
-            ext_c.q_attn_forward_1(self.q_handle,
-                                   hidden_states,
-                                   batch_size,
-                                   q_len,
-                                   -1 if isinstance(past_len, tuple) else past_len,
-                                   past_len[0] if isinstance(past_len, tuple) else ext.none_tensor,
-                                   q_states,
-                                   k_states,
-                                   v_states,
-                                   constants.sin,
-                                   constants.cos,
-                                   pass_loras,
-                                   pass_lora_temp)
-
-        # Alternative, for embedded QKV
+        batch_size, q_len, _ = hidden_states.shape
+        q = torch.empty((batch_size, q_len, cfg.num_attention_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+        k = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+        v = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+
+        cache_seqlens = attn_params.get_cache_seqlens(self.device())
+        block_table = attn_params.get_block_index(self.device())
+
+        k_cache, v_cache = cache.get_kv_state(self.layer_idx, batch_size, 0, 1, page_size, cache_seqlens, block_table)
+        k_cache = k_cache.view(k_cache.shape[1] // page_size, page_size, k_cache.shape[2], k_cache.shape[3])
+        v_cache = v_cache.view(v_cache.shape[1] // page_size, page_size, v_cache.shape[2], v_cache.shape[3])
 
+        if loras is None or self.temp_lora_size == 0:
+            pass_loras, pass_lora_temp = [], none_tensor
         else:
+            pass_loras, pass_lora_temp = [id(x) for x in loras], torch.empty((self.temp_lora_size,), dtype = torch.half, device = hidden_states.device)
 
-            q_states = hidden_states[1]
-            k_states = hidden_states[2]
-            v_states = hidden_states[3]
-            hidden_states = hidden_states[0]
-
-            ext_c.rope_(q_states, constants.sin, constants.cos, past_len, num_attention_heads, head_dim)
-            ext_c.rope_(k_states, constants.sin, constants.cos, past_len, num_key_value_heads, head_dim)
-
-        # Shape for attention
-
-        q_states = q_states.view(batch_size, q_len, num_attention_heads, head_dim)
-        k_states = k_states.view(batch_size, q_len, num_key_value_heads, head_dim)
-        v_states = v_states.view(batch_size, q_len, num_key_value_heads, head_dim)
-
-        # Regular (batched) attention with optional padding mask
-
-        if cache is None or isinstance(cache, ExLlamaV2CacheBase):
-
-            # Add keys and values to cache
-
-            if cache is not None:
-
-                if direct:
-
-                    k_states = batch_keys.narrow(0, 0, batch_size).narrow(1, 0, past_len + q_len)
-                    v_states = batch_values.narrow(0, 0, batch_size).narrow(1, 0, past_len + q_len)
-
-                else:
+        ext_c.q_attn_forward_1(
+            self.q_handle,
+            hidden_states,
+            batch_size,
+            q_len,
+            0,
+            attn_params.get_cache_seqlens(self.device()),
+            q,
+            k,
+            v,
+            constants.sin,
+            constants.cos,
+            pass_loras,
+            pass_lora_temp
+        )
+
+        attn_output = flash_attn_with_kvcache(
+            q = q,
+            k = k,
+            v = v,
+            k_cache = k_cache,
+            v_cache = v_cache,
+            cache_seqlens = cache_seqlens,
+            block_table = block_table,
+            causal = True
+        )
+        attn_output = attn_output.view((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
 
-                    batch_keys, batch_values = cache.get_kv_state(self.layer_idx, batch_size, 0, past_len)
-                    new_keys = batch_keys.narrow(0, 0, batch_size).narrow(1, past_len, q_len)
-                    new_values = batch_values.narrow(0, 0, batch_size).narrow(1, past_len, q_len)
-                    new_keys.copy_(k_states)
-                    new_values.copy_(v_states)
+        cache.store_kv_state(self.layer_idx, batch_size, 0, q_len, page_size, cache_seqlens, block_table)
 
-                    # Key/value tensors with past
-
-                    k_states = batch_keys.narrow(1, 0, past_len + q_len)
-                    v_states = batch_values.narrow(1, 0, past_len + q_len)
-
-            # Torch matmul attention
-
-            if self.model.config.no_flash_attn or not has_flash_attn:
-
-                q_states = q_states.transpose(1, 2)
-                k_states = k_states.transpose(1, 2)
-                v_states = v_states.transpose(1, 2)
-
-                k_states = self.repeat_kv(k_states, num_key_value_groups)
-                k_states = k_states.transpose(-1, -2)
+        # Output projection
 
-                attn_weights = torch.matmul(q_states, k_states)
-                k_states = None
-                q_states = None
+        ext_c.q_attn_forward_2(
+            self.q_handle,
+            hidden_states,
+            attn_output,
+            batch_size,
+            q_len,
+            pass_loras,
+            pass_lora_temp
+        )
 
-                attn_weights /= math.sqrt(head_dim)
-                if attn_mask is not None: attn_weights = attn_weights + attn_mask
-                attn_weights = nn.functional.softmax(attn_weights, dim = -1, dtype = torch.float16)
+        return hidden_states
 
-                v_states = self.repeat_kv(v_states, num_key_value_groups)
-                attn_output = torch.matmul(attn_weights, v_states)
-                v_states = None
 
-                attn_output = attn_output.transpose(1, 2)
-                attn_output = attn_output.reshape((batch_size, q_len, hidden_size))
+    def _attn_matmul(self, batch_size, q_len, q_states, k_states, v_states, attn_params, cfg):
 
-            # Flash Attention 2
+        q_states = q_states.transpose(1, 2)
+        k_states = k_states.transpose(1, 2)
+        v_states = v_states.transpose(1, 2)
+
+        k_states = self.repeat_kv(k_states, cfg.num_key_value_groups)
+        k_states = k_states.transpose(-1, -2)
+
+        attn_weights = torch.matmul(q_states, k_states)
+
+        attn_weights *= 1 / math.sqrt(cfg.head_dim)
+        attn_mask = attn_params.get_attn_mask(attn_weights.device)
+        if attn_mask is not None: attn_weights = attn_weights + attn_mask
+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float16)
+
+        v_states = self.repeat_kv(v_states, cfg.num_key_value_groups)
+        attn_output = torch.matmul(attn_weights, v_states)
+
+        attn_output = attn_output.transpose(1, 2)
+        attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
+        return attn_output
+
+
+    def _attn_flash(self, batch_size, q_len, q_states, k_states, v_states, attn_params, cfg):
+
+        attn_output = flash_attn_func(
+            q_states,
+            k_states,
+            v_states,
+            causal = True
+        )
+        attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
+        return attn_output
+
+
+    def forward(self,
+                hidden_states: torch.Tensor,
+                cache: ExLlamaV2CacheBase | None = None,
+                attn_params: ExLlamaV2Attention.Params | None = None,
+                past_len: int | None = None,
+                intermediates: bool = False,
+                loras: list[ExLlamaV2Lora] | None = None,
+                **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
-            else:
+        global has_flash_attn
 
-                attn_output = flash_attn_func(q_states, k_states, v_states, causal = True)
-                attn_output = attn_output.reshape((batch_size, q_len, hidden_size))
+        if isinstance(attn_params, ExLlamaV2Attention.PagedParams):
+            return self.forward_paged(
+                hidden_states,
+                cache,
+                attn_params,
+                loras = loras,
+                **kwargs
+            )
 
-            # xformers memory_efficient_attention
+        if self.q_handle is None or intermediates:
+            return self.forward_torch(
+                hidden_states,
+                cache,
+                attn_params,
+                past_len,
+                intermediates,
+                loras = loras,
+                **kwargs
+            )
 
-            # attn_output = xops.memory_efficient_attention(q_states, k_states, v_states, attn_bias = xops.LowerTriangularMask())
-            # attn_output = attn_output.reshape((batch_size, q_len, hidden_size));
+        cfg = self.model.config
+        constants = self.model.get_device_tensors(self.device_idx)
 
-            # Torch SDP attention:
+        batch_size, q_len, _ = hidden_states.shape
+        direct = (batch_size == 1 and cache is not None and isinstance(cache, ExLlamaV2CacheBase))
 
-            # q_states = q_states.transpose(1, 2)
-            # k_states = k_states.transpose(1, 2)
-            # v_states = v_states.transpose(1, 2)
-            #
-            # # k_states = self.repeat_kv(k_states, num_key_value_groups)
-            # # v_states = self.repeat_kv(v_states, num_key_value_groups)
-            #
-            # attn_output = F.scaled_dot_product_attention(q_states, k_states, v_states, attn_mask = attn_mask, is_causal = False)
-            # attn_output = attn_output.transpose(1, 2)
-            # attn_output = attn_output.reshape((batch_size, q_len, hidden_size))
+        q_shape = hidden_states.shape[:-1] + (self.q_proj.out_features,)
+        k_shape = hidden_states.shape[:-1] + (self.k_proj.out_features,)
+        v_shape = hidden_states.shape[:-1] + (self.v_proj.out_features,)
+        q_states = torch.empty(q_shape, device = hidden_states.device, dtype = torch.half)
 
-            # Update 8-bit cache
+        # If conditions are right we can write the K/V projections directly into the cache
 
-            if cache is not None:
-                cache.store_kv_state(self.layer_idx, batch_size, past_len, q_len)
+        if direct:
+            batch_keys, batch_values = cache.get_kv_state(self.layer_idx, batch_size, 0, past_len)
+            k_states = batch_keys[:batch_size, past_len : past_len + q_len, :]
+            v_states = batch_values[:batch_size, past_len : past_len + q_len, :]
+        else:
+            k_states = torch.empty(k_shape, device = hidden_states.device, dtype = torch.half)
+            v_states = torch.empty(v_shape, device = hidden_states.device, dtype = torch.half)
 
-        # Multiple caches
+        # RMS norm, Q/K/V projections, position embeddings
 
+        if loras is None or self.temp_lora_size == 0:
+            pass_loras = []
+            pass_lora_temp = none_tensor
         else:
+            pass_loras = [id(x) for x in loras]
+            pass_lora_temp = torch.empty((self.temp_lora_size,), dtype = torch.half, device = hidden_states.device)
 
-            attn_outputs = []
-            for i in range(len(cache)):
-
-                # TODO: Once nested tensors are finalized in Torch, this could all be batched, probably
+        if attn_params.position_offsets is not None:
+            pass_past_len_1 = past_len
+            pass_past_len_2 = attn_params.get_position_offsets(hidden_states.device)
+        else:
+            pass_past_len_1 = past_len
+            pass_past_len_2 = none_tensor
 
-                # Add keys and values to cache
+        ext_c.q_attn_forward_1(
+            self.q_handle,
+            hidden_states,
+            batch_size,
+            q_len,
+            pass_past_len_1,
+            pass_past_len_2,
+            q_states,
+            k_states,
+            v_states,
+            constants.sin,
+            constants.cos,
+            pass_loras,
+            pass_lora_temp
+        )
 
-                batch_keys, batch_values = cache[i].get_kv_state(self.layer_idx, batch_size, 0, past_len)
-                new_keys = batch_keys.narrow(1, past_len[1][i], q_len)
-                new_values = batch_values.narrow(1, past_len[1][i], q_len)
-                new_keys.copy_(k_states.narrow(0, i, 1))
-                new_values.copy_(v_states.narrow(0, i, 1))
+        # Select attention function
 
-                # Key/value tensors with past
+        if cfg.no_flash_attn or not has_flash_attn or not attn_params.is_causal():
+            attn_func = self._attn_matmul
+        else:
+            attn_func = self._attn_flash
 
-                k_states_b = batch_keys.narrow(1, 0, past_len[1][i] + q_len)
-                v_states_b = batch_values.narrow(1, 0, past_len[1][i] + q_len)
+        # Straight attention without cache
 
-                # Torch matmul attention
+        if cache is None:
 
-                # TODO: enable flash-attn
+            q_states = q_states.view(batch_size, q_len, cfg.num_attention_heads, cfg.head_dim)
+            k_states = k_states.view(batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim)
+            v_states = v_states.view(batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim)
 
-                q_states_b = q_states.transpose(1, 2).narrow(0, i, 1)
-                k_states_b = k_states_b.transpose(1, 2)
-                v_states_b = v_states_b.transpose(1, 2)
+            attn_output = attn_func(batch_size, q_len, q_states, k_states, v_states, attn_params, cfg)
 
-                k_states_b = self.repeat_kv(k_states_b, num_key_value_groups)
-                k_states_b = k_states_b.transpose(-1, -2)
+        # Regular cache (FP16, FP8, Q4)
 
-                attn_weights = torch.matmul(q_states_b, k_states_b)
-                q_states_b = None
-                k_states_b = None
+        elif isinstance(cache, ExLlamaV2CacheBase):
 
-                attn_weights /= math.sqrt(head_dim)
-                if attn_mask is not None: attn_weights = attn_weights + attn_mask[i]
-                attn_weights = nn.functional.softmax(attn_weights, dim = -1, dtype = torch.float16)
+            q_states = q_states.view(batch_size, q_len, cfg.num_attention_heads, cfg.head_dim)
+            k_states = k_states.view(batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim)
+            v_states = v_states.view(batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim)
 
-                v_states_b = self.repeat_kv(v_states_b, num_key_value_groups)
-                attn_output_b = torch.matmul(attn_weights, v_states_b)
-                v_states_b = None
+            if not direct:
+                batch_keys, batch_values = cache.get_kv_state(self.layer_idx, batch_size, 0, past_len)
+                batch_keys[:batch_size, past_len:past_len + q_len, :].copy_(k_states)
+                batch_values[:batch_size, past_len:past_len + q_len, :].copy_(v_states)
 
-                attn_outputs.append(attn_output_b)
+            k_states = batch_keys[:batch_size, :past_len + q_len, :]
+            v_states = batch_values[:batch_size, :past_len + q_len, :]
 
-            q_states = None
-            k_states = None
-            v_states = None
+            cache.store_kv_state(self.layer_idx, batch_size, past_len, q_len)
 
-            attn_output = torch.cat(attn_outputs, dim = 0)
-            attn_output = attn_output.transpose(1, 2)
-            attn_output = attn_output.reshape((batch_size, q_len, hidden_size))
+            attn_output = attn_func(batch_size, q_len, q_states, k_states, v_states, attn_params, cfg)
 
         # Output projection
 
-        ext_c.q_attn_forward_2(self.q_handle,
-                               hidden_states,
-                               attn_output,
-                               batch_size,
-                               q_len,
-                               pass_loras,
-                               pass_lora_temp)
-
-        attn_output = None
-        attn_weights = None
+        ext_c.q_attn_forward_2(
+            self.q_handle,
+            hidden_states,
+            attn_output,
+            batch_size,
+            q_len,
+            pass_loras,
+            pass_lora_temp
+        )
 
         return hidden_states
 
 
-    def forward_torch(self, hidden_states, cache = None, attn_mask = None, past_len = None, intermediates = False, loras = None):
+    def forward_torch(self,
+                      hidden_states: torch.Tensor,
+                      cache: ExLlamaV2CacheBase | None = None,
+                      attn_params: ExLlamaV2Attention.Params | None = None,
+                      past_len: int | None = None,
+                      intermediates: bool = False,
+                      loras: list[ExLlamaV2Lora] | None = None,
+                      **kwargs) -> torch.Tensor | dict:
+
+        cfg = self.model.config
+        num_attention_heads = cfg.num_attention_heads
+        num_key_value_heads = cfg.num_key_value_heads
+        num_key_value_groups = cfg.num_key_value_groups
+        head_dim = cfg.head_dim
+        hidden_size = cfg.hidden_size
 
-        num_attention_heads = self.model.config.num_attention_heads
-        num_key_value_heads = self.model.config.num_key_value_heads
-        num_key_value_groups = self.model.config.num_key_value_groups
-        head_dim = self.model.config.head_dim
-        hidden_size = self.model.config.hidden_size
-
-        qkv_embed = self.model.config.qkv_embed and self.layer_idx == 0
-
-        if not qkv_embed: batch_size, q_len, _ = hidden_states.size()
-        else: batch_size, q_len, _ = hidden_states[0].size()
+        batch_size, q_len, _ = hidden_states.size()
 
         past_len = 0 if cache is None else cache.current_seq_len
 
         # Project q, k, v
 
-        if not qkv_embed:
-
-            residual = hidden_states
-            post_norm = self.input_layernorm.forward(hidden_states)
-
-            query_states_im = self.q_proj.forward(post_norm, loras = loras)
-            key_states_im = self.k_proj.forward(post_norm, loras = loras)
-            value_states_im = self.v_proj.forward(post_norm, loras = loras)
+        residual = hidden_states
+        post_norm = self.input_layernorm.forward(hidden_states) if self.has_norm else hidden_states
 
-            if intermediates:
-
-                query_states = query_states_im.clone()
-                key_states = key_states_im.clone()
-                value_states = value_states_im.clone()
-
-            else:
+        query_states = self.q_proj.forward(post_norm, loras = loras)
+        key_states = self.k_proj.forward(post_norm, loras = loras)
+        value_states = self.v_proj.forward(post_norm, loras = loras)
 
-                query_states = query_states_im
-                key_states = key_states_im
-                value_states = value_states_im
+        # Shape for attention
 
-        # Alternative, for embedded QKV
+        query_states = query_states.view(batch_size, q_len, num_attention_heads, head_dim)
+        key_states = key_states.view(batch_size, q_len, num_key_value_heads, head_dim)
+        value_states = value_states.view(batch_size, q_len, num_key_value_heads, head_dim)
 
-        else:
+        # Apply Q/K norms
 
-            residual = hidden_states[0]
-            query_states = hidden_states[1]
-            key_states = hidden_states[2]
-            value_states = hidden_states[3]
+        if cfg.use_qk_norm:
+            query_states = self.q_norm.forward(query_states)
+            key_states = self.k_norm.forward(key_states)
 
         # Apply position embeddings
 
-        query_states = query_states.view(batch_size, q_len, num_attention_heads, head_dim)
-        key_states = key_states.view(batch_size, q_len, num_key_value_heads, head_dim)
-        value_states = value_states.view(batch_size, q_len, num_key_value_heads, head_dim)
-
         constants = self.model.get_device_tensors(self.device_idx, scratch = False)
 
-        ext_c.rope_(query_states, constants.sin, constants.cos, past_len, num_attention_heads, head_dim)
-        ext_c.rope_(key_states, constants.sin, constants.cos, past_len, num_key_value_heads, head_dim)
+        if attn_params.position_offsets is not None:
+            position_offsets = attn_params.get_position_offsets(hidden_states.device)
+        else:
+            position_offsets = none_tensor
+
+        if cfg.arch.rope_style != RopeStyle.NONE:
+            ext_c.rope_(query_states, constants.sin, constants.cos, past_len, num_attention_heads, head_dim, position_offsets, cfg.arch.rope_style == RopeStyle.NEOX)
+            ext_c.rope_(key_states, constants.sin, constants.cos, past_len, num_key_value_heads, head_dim, position_offsets, cfg.arch.rope_style == RopeStyle.NEOX)
 
         # Add keys and values to cache
 
         if cache is not None:
 
             batch_keys, batch_values = cache.get_kv_state(self.layer_idx, batch_size, 0, past_len)
             new_keys = batch_keys.narrow(1, past_len, q_len).narrow(0, 0, batch_size)
@@ -555,71 +804,78 @@
             # Key/value tensors with past
 
             key_states = batch_keys.narrow(1, 0, past_len + q_len).narrow(0, 0, batch_size)
             value_states = batch_values.narrow(1, 0, past_len + q_len).narrow(0, 0, batch_size)
 
         # Torch matmul attention
 
-        if self.model.config.no_flash_attn or not has_flash_attn:
+        if cfg.no_flash_attn or not has_flash_attn or not attn_params.is_causal():
 
             query_states = query_states.transpose(1, 2)
             key_states = key_states.transpose(1, 2)
             value_states = value_states.transpose(1, 2)
 
-            key_states = self.repeat_kv(key_states, self.model.config.num_key_value_groups)
+            key_states = self.repeat_kv(key_states, cfg.num_key_value_groups)
             key_states = key_states.transpose(-1, -2)
 
             attn_weights = torch.matmul(query_states, key_states)
-            attn_weights /= math.sqrt(head_dim)
+            # attn_weights *= self.scale_factor / math.sqrt(head_dim)
+            # attn_mask = attn_params.get_attn_mask(hidden_states.device)
+            # if self.scale_factor != 1: attn_weights *= self.unscale_factor
+            attn_weights *= 1 / math.sqrt(head_dim)
+            attn_mask = attn_params.get_attn_mask(hidden_states.device)
             if attn_mask is not None: attn_weights = attn_weights + attn_mask
             attn_weights = nn.functional.softmax(attn_weights, dim = -1, dtype = torch.float16)
 
-            value_states = self.repeat_kv(value_states, self.model.config.num_key_value_groups)
+            value_states = self.repeat_kv(value_states, cfg.num_key_value_groups)
             attn_output = torch.matmul(attn_weights, value_states)
 
             attn_output = attn_output.transpose(1, 2)
-            attn_output = attn_output.reshape((batch_size, q_len, hidden_size))
+            attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
 
         # Flash Attention 2
 
         else:
 
-            attn_output = flash_attn_func(query_states, key_states, value_states, causal = True)
-            attn_output = attn_output.reshape((batch_size, q_len, hidden_size))
+            attn_output = flash_attn_func(
+                query_states,
+                key_states,
+                value_states,
+                # softmax_scale = None if self.scale_factor == 1 else self.scale_factor / math.sqrt(head_dim),
+                causal = True
+            )
+            attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
 
-        # Update 8-bit cache
-        # TODO: Only update changed positions of the cache
+        # Update 8-bit/Q4 cache
 
         if cache is not None:
             cache.store_kv_state(self.layer_idx, batch_size, past_len, q_len)
 
         # Output projection
 
         attn_proj = self.o_proj.forward(attn_output, loras = loras)
 
         # Add residual connection
 
-        hidden_states = attn_proj + residual
+        hidden_states = (attn_proj + residual) if self.has_residual else attn_proj
 
         if intermediates:
             return {"post_norm": post_norm,
-                    "query_states": query_states_im,
-                    "key_states": key_states_im,
-                    "value_states": value_states_im,
                     "attn_output": attn_output,
-                    "attn_proj": attn_proj,
                     "hidden_states": hidden_states}
         else:
             return hidden_states
 
 
     def update_loras(self):
 
         if self.q_handle is None: return
 
+        cfg = self.model.config
+
         q_proj_lora_a = { id(k): v for k, v in self.q_proj.lora_a_tensors.items() }
         q_proj_lora_b = { id(k): v for k, v in self.q_proj.lora_b_tensors.items() }
         k_proj_lora_a = { id(k): v for k, v in self.k_proj.lora_a_tensors.items() }
         k_proj_lora_b = { id(k): v for k, v in self.k_proj.lora_b_tensors.items() }
         v_proj_lora_a = { id(k): v for k, v in self.v_proj.lora_a_tensors.items() }
         v_proj_lora_b = { id(k): v for k, v in self.v_proj.lora_b_tensors.items() }
         o_proj_lora_a = { id(k): v for k, v in self.o_proj.lora_a_tensors.items() }
@@ -631,12 +887,13 @@
                                                 k_proj_lora_a,
                                                 k_proj_lora_b,
                                                 v_proj_lora_a,
                                                 v_proj_lora_b,
                                                 o_proj_lora_a,
                                                 o_proj_lora_b)
 
-        self.temp_lora_size = temp_lora_size * self.model.config.max_batch_size * self.model.config.max_input_len
+        self.temp_lora_size = temp_lora_size * cfg.max_batch_size * cfg.max_input_len
 
 
     def is_quant(self):
         return self.q_handle is not None
+
```

## exllamav2/cache.py

```diff
@@ -1,214 +1,444 @@
+from __future__ import annotations
+
 import torch
-from exllamav2.ext import exllamav2_ext as ext_c
+from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
+
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.model import ExLlamaV2
+
 
 class ExLlamaV2CacheBase:
 
     model = None
     max_seq_len: int
     batch_size: int
 
     current_seq_len: int
 
-    key_states: list
-    value_states: list
+    key_states: list[torch.Tensor | None]
+    value_states: list[torch.Tensor | None]
+    key_scales: list[torch.Tensor | None]
+    value_scales: list[torch.Tensor | None]
     num_key_value_heads: int
     num_hidden_layers: int
     head_dim: int
 
-    dtype = None
+    dtype: torch.dtype
+    weights_per_element: int
+    has_scales: bool
 
 
-    def __init__(self, model, batch_size, max_seq_len):
+    def __init__(self,
+                 model: ExLlamaV2,
+                 batch_size: int,
+                 max_seq_len: int,
+                 dtype: torch.dtype,
+                 weights_per_element: int,
+                 has_scales: bool):
 
         self.model = model
         self.max_seq_len = max_seq_len if max_seq_len != -1 else self.model.config.max_seq_len
         self.batch_size = batch_size
-        self.current_seq_len = 0
+        self.dtype = dtype
+        self.weights_per_element = weights_per_element
+        self.has_scales = has_scales
 
         self.key_states = []
         self.value_states = []
+        self.key_scales = []
+        self.value_scales = []
 
         self.num_key_value_heads = self.model.config.num_key_value_heads
         self.num_hidden_layers = self.model.config.num_hidden_layers
         self.head_dim = self.model.config.head_dim
 
+        self.current_seq_len = 0
+        self.shape_basic = (self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim)
+        self.shape_w = (self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim // self.weights_per_element)
+        self.shape_s = (self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim // 32)
 
-    def create_state_tensors(self, copy_from, lazy = False):
+
+    def create_state_tensors(self,
+                             copy_from: ExLlamaV2CacheBase | None,
+                             lazy = False):
 
         assert copy_from is None or lazy == False, "Cannot use lazy cache initialization while copying"
 
+        if copy_from:
+            self.current_seq_len = copy_from.current_seq_len
+
         if not lazy:
 
             for i in range(self.num_hidden_layers):
 
                 if copy_from is None:
-                    p_key_states = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim, dtype = self.dtype, device = self.model.cache_map[i]).contiguous()
-                    p_value_states = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim, dtype = self.dtype, device = self.model.cache_map[i]).contiguous()
+                    device = self.model.cache_map[i]
+                    p_key_states = torch.zeros(self.shape_w, dtype = self.dtype, device = device).contiguous()
+                    p_value_states = torch.zeros(self.shape_w, dtype = self.dtype, device = device).contiguous()
+                    if self.has_scales:
+                        p_key_scales = torch.zeros(self.shape_s, dtype = torch.float16, device = device).contiguous()
+                        p_value_scales = torch.zeros(self.shape_s, dtype = torch.float16, device = device).contiguous()
                 else:
                     p_key_states = copy_from.key_states[i].clone()
                     p_value_states = copy_from.value_states[i].clone()
+                    if self.has_scales:
+                        p_key_scales = copy_from.key_scales[i].clone()
+                        p_value_scales = copy_from.value_scales[i].clone()
 
                 self.key_states.append(p_key_states)
                 self.value_states.append(p_value_states)
+                if self.has_scales:
+                    self.key_scales.append(p_key_scales)
+                    self.value_scales.append(p_value_scales)
 
         else:
 
             for i in range(self.num_hidden_layers):
 
                 self.key_states.append(None)
                 self.value_states.append(None)
+                if self.has_scales:
+                    self.key_scales.append(None)
+                    self.value_scales.append(None)
 
 
     def update_cache_tensors(self):
 
         for k, v in self.model.cache_map.items():
 
             self.touch_device(v)
 
             if self.key_states[k] is not None:
 
                 if str(self.key_states[k].device) == v: continue
                 self.key_states[k] = None
                 self.value_states[k] = None
 
-            p_key_states = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim, dtype = self.dtype, device = v).contiguous()
-            p_value_states = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim, dtype = self.dtype, device = v).contiguous()
+            p_key_states = torch.zeros(self.shape_w, dtype = self.dtype, device = v).contiguous()
+            p_value_states = torch.zeros(self.shape_w, dtype = self.dtype, device = v).contiguous()
             self.key_states[k] = p_key_states
             self.value_states[k] = p_value_states
+            if self.has_scales:
+                p_key_scales = torch.zeros(self.shape_s, dtype = torch.float16, device = v).contiguous()
+                p_value_scales = torch.zeros(self.shape_s, dtype = torch.float16, device = v).contiguous()
+                self.key_scales[k] = p_key_scales
+                self.value_scales[k] = p_value_scales
 
 
     def roll_left(self):
 
         for i in range(self.model.config.num_hidden_layers):
 
             self.key_states[i] = torch.roll(self.key_states[i], shifts = -1, dims = 2)
             self.value_states[i] = torch.roll(self.value_states[i], shifts = -1, dims = 2)
 
         self.current_seq_len -= 1
 
 
-    def get_kv_state(self, layer_idx: int, batch_size: int, offset: int, width: int) -> (torch.Tensor, torch.Tensor):
+    def get_kv_state(self,
+                     layer_idx: int,
+                     batch_size: int,
+                     offset: int,
+                     width: int,
+                     page_size: int = 0,
+                     cache_seqlens: torch.Tensor | None = None,
+                     block_table: torch.Tensor | None = None) -> (torch.Tensor, torch.Tensor):
         raise NotImplementedError
 
 
-    def store_kv_state(self, layer_idx: int, batch_size: int, offset: int, width: int):
+    def store_kv_state(self,
+                       layer_idx: int,
+                       batch_size: int,
+                       offset: int,
+                       width: int,
+                       page_size: int = 0,
+                       cache_seqlens: torch.Tensor | None = None,
+                       block_table: torch.Tensor | None = None):
         raise NotImplementedError
 
 
-    def copy_states(self, target, from_column, from_columns, to_column, to_columns, from_row, from_rows, to_row, to_rows):
+    @torch.inference_mode
+    def copy_states(self,
+                    target: ExLlamaV2CacheBase,
+                    from_column: int,
+                    from_columns: int,
+                    to_column: int,
+                    to_columns: int,
+                    from_row: int,
+                    from_rows: int,
+                    to_row: int,
+                    to_rows: int):
 
         assert from_rows == 1
         assert from_columns == to_columns
         assert to_column + to_columns <= target.max_seq_len
         assert from_column + from_columns <= self.max_seq_len
 
         num_hidden_layers = self.model.config.num_hidden_layers
 
-        for i in range(num_hidden_layers):
-
-            source_view_k = self.key_states[i].narrow(0, from_row, from_rows).narrow(2, from_column, from_columns)
-            source_view_v = self.value_states[i].narrow(0, from_row, from_rows).narrow(2, from_column, from_columns)
-            target_view_k = target.key_states[i].narrow(0, to_row, to_rows).narrow(2, to_column, to_columns)
-            target_view_v = target.value_states[i].narrow(0, to_row, to_rows).narrow(2, to_column, to_columns)
+        tensors = [
+            (self.key_states, target.key_states),
+            (self.value_states, target.value_states),
+        ]
+        if self.has_scales:
+            tensors += [
+                (self.key_scales, target.key_scales),
+                (self.value_scales, target.value_scales),
+            ]
 
-            if to_rows > 1:
-
-                source_view_k = source_view_k.expand_as(target_view_k)
-                source_view_v = source_view_v.expand_as(target_view_v)
-
-            target_view_k.copy_(source_view_k)
-            target_view_v.copy_(source_view_v)
+        for i in range(num_hidden_layers):
+            for (src, dst) in tensors:
+                src_view = src[i].narrow(0, from_row, from_rows).narrow(1, from_column, from_columns)
+                dst_view = dst[i].narrow(0, to_row, to_rows).narrow(1, to_column, to_columns)
+                if to_rows > 1:
+                    src_view = src_view.expand_as(dst_view)
+                dst_view.copy_(src_view, non_blocking = True)
 
 
     def touch_device(self, device):
         pass
 
 
 class ExLlamaV2Cache(ExLlamaV2CacheBase):
+    """
+    FP16 cache
+    """
+
+    def __init__(self,
+                 model: ExLlamaV2,
+                 batch_size: int = 1,
+                 max_seq_len: int = -1,
+                 copy_from: ExLlamaV2Cache | None = None,
+                 lazy: bool = False):
 
-    def __init__(self, model, batch_size = 1, max_seq_len = -1, copy_from = None, lazy = False):
-        super().__init__(model, batch_size, max_seq_len)
+        super().__init__(model, batch_size, max_seq_len, torch.half, 1, False)
 
-        self.dtype = torch.half
         self.create_state_tensors(copy_from, lazy)
 
 
-    def get_kv_state(self, layer_idx: int, batch_size: int, offset: int, width: int) -> (torch.Tensor, torch.Tensor):
+    def get_kv_state(self,
+                     layer_idx: int,
+                     batch_size: int,
+                     offset: int,
+                     width: int,
+                     page_size: int = 0,
+                     cache_seqlens: torch.Tensor | None = None,
+                     block_table: torch.Tensor | None = None) -> (torch.Tensor, torch.Tensor):
+
         return self.key_states[layer_idx], self.value_states[layer_idx]
 
 
-    def store_kv_state(self, layer_idx: int, batch_size: int, offset: int, width: int):
+    def store_kv_state(self,
+                       layer_idx: int,
+                       batch_size: int,
+                       offset: int,
+                       width: int,
+                       page_size: int = 0,
+                       cache_seqlens: torch.Tensor | None = None,
+                       block_table: torch.Tensor | None = None):
+
         pass
 
 
     def footprint(self):
+
         fp = []
         for layer in self.key_states + self.value_states:
             dev = layer.device.index
             while len(fp) <= dev: fp.append(0)
             fp[dev] += layer.numel() * 2
         return fp
 
 
     def clone(self):
-        new = ExLlamaV2Cache(self.model, batch_size = self.batch_size, max_seq_len = self.max_seq_len, copy_from = self)
+
+        new = ExLlamaV2Cache(self.model, self.batch_size, self.max_seq_len, self)
         return new
 
 
 class ExLlamaV2Cache_8bit(ExLlamaV2CacheBase):
+    """
+    8-bit cache. Keys and values are compressed to FP8 (e5m2) format by truncation.
+    """
+
+    def __init__(self,
+                 model: ExLlamaV2,
+                 batch_size: int = 1,
+                 max_seq_len: int = -1,
+                 copy_from: ExLlamaV2Cache_8bit | None = None,
+                 lazy: bool = False):
 
-    def __init__(self, model, batch_size = 1, max_seq_len = -1, copy_from = None, lazy = False):
-        super().__init__(model, batch_size, max_seq_len)
+        super().__init__(model, batch_size, max_seq_len, torch.uint8, 1, False)
 
-        self.dtype = torch.uint8
         self.create_state_tensors(copy_from, lazy)
 
         # Create temp FP16 tensors for accessing FP8 layers
 
         self.temp_tensors = {}
-
         if not lazy:
             for device in self.model.get_cache_devices(): self.touch_device(device)
 
 
     def touch_device(self, device):
 
         if device in self.temp_tensors: return
-        k = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim, dtype = torch.float16, device = device).contiguous()
-        v = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads, self.head_dim, dtype = torch.float16, device = device).contiguous()
+        k = torch.zeros(self.shape_basic, dtype = torch.float16, device = device).contiguous()
+        v = torch.zeros(self.shape_basic, dtype = torch.float16, device = device).contiguous()
         self.temp_tensors[device] = (k, v)
 
 
-    def get_kv_state(self, layer_idx: int, batch_size: int, offset: int, width: int) -> (torch.Tensor, torch.Tensor):
+    def get_kv_state(self,
+                     layer_idx: int,
+                     batch_size: int,
+                     offset: int,
+                     width: int,
+                     page_size: int = 0,
+                     cache_seqlens: torch.Tensor | None = None,
+                     block_table: torch.Tensor | None = None) -> (torch.Tensor, torch.Tensor):
 
         device = self.model.cache_map[layer_idx]
         temp_key_state, temp_value_state = self.temp_tensors[device]
         if width > 0: ext_c.fp8_to_fp16(self.key_states[layer_idx], temp_key_state, batch_size, offset, width)
         if width > 0: ext_c.fp8_to_fp16(self.value_states[layer_idx], temp_value_state, batch_size, offset, width)
         return temp_key_state, temp_value_state
 
 
-    def store_kv_state(self, layer_idx: int, batch_size: int, offset: int, width: int):
+    def store_kv_state(self,
+                       layer_idx: int,
+                       batch_size: int,
+                       offset: int,
+                       width: int,
+                       page_size: int = 0,
+                       cache_seqlens: torch.Tensor | None = None,
+                       block_table: torch.Tensor | None = None):
 
         device = self.model.cache_map[layer_idx]
         temp_key_state, temp_value_state = self.temp_tensors[device]
         if width > 0: ext_c.fp16_to_fp8(temp_key_state, self.key_states[layer_idx], batch_size, offset, width)
         if width > 0: ext_c.fp16_to_fp8(temp_value_state, self.value_states[layer_idx], batch_size, offset, width)
 
 
-    def footprint(self):
+    def footprint(self) -> list[int]:
+
         fp = []
         for layer in self.key_states + self.value_states:
             dev = layer.device.index
             while len(fp) <= dev: fp.append(0)
             fp[dev] += layer.numel() * 1
         for temp_k, temp_v in self.temp_tensors.values():
             fp[temp_k.device.index] += temp_k.numel() * 2
             fp[temp_v.device.index] += temp_v.numel() * 2
         return fp
 
 
-    def clone(self):
-        new = ExLlamaV2Cache_8bit(self.model, batch_size = self.batch_size, max_seq_len = self.max_seq_len, copy_from = self)
+    def clone(self) -> ExLlamaV2Cache_8bit:
+
+        new = ExLlamaV2Cache_8bit(self.model, self.batch_size, self.max_seq_len, self)
+        return new
+
+
+class ExLlamaV2Cache_Q4(ExLlamaV2CacheBase):
+    """
+    Q4 cache. Uses grouped RTN quantization for keys/values
+    """
+
+    def __init__(self,
+                 model: ExLlamaV2,
+                 batch_size: int = 1,
+                 max_seq_len: int = -1,
+                 copy_from: ExLlamaV2Cache_Q4 | None = None,
+                 lazy: bool = False):
+
+        super().__init__(model, batch_size, max_seq_len, torch.uint8, 2, True)
+
+        self.create_state_tensors(copy_from, lazy)
+
+        # Create temp FP16 tensors for accessing Q4 layers
+
+        self.temp_tensors = {}
+        if not lazy:
+            for device in self.model.get_cache_devices(): self.touch_device(device)
+
+
+    def touch_device(self, device):
+
+        if device in self.temp_tensors: return
+        k = torch.zeros(self.shape_basic, dtype = torch.float16, device = device).contiguous()
+        v = torch.zeros(self.shape_basic, dtype = torch.float16, device = device).contiguous()
+        self.temp_tensors[device] = (k, v)
+
+
+    def get_kv_state(self,
+                     layer_idx: int,
+                     batch_size: int,
+                     offset: int,
+                     width: int,
+                     page_size: int = 0,
+                     cache_seqlens: torch.Tensor | None = None,
+                     block_table: torch.Tensor | None = None) -> (torch.Tensor, torch.Tensor):
+
+        device = self.model.cache_map[layer_idx]
+        temp_key_state, temp_value_state = self.temp_tensors[device]
+        if width > 0: ext_c.q4_to_fp16_kv(self.key_states[layer_idx],
+                                          temp_key_state,
+                                          self.key_scales[layer_idx],
+                                          self.value_states[layer_idx],
+                                          temp_value_state,
+                                          self.value_scales[layer_idx],
+                                          batch_size,
+                                          offset,
+                                          width,
+                                          page_size,
+                                          cache_seqlens if cache_seqlens is not None else none_tensor,
+                                          block_table if block_table is not None else none_tensor)
+        return temp_key_state, temp_value_state
+
+
+    def store_kv_state(self,
+                       layer_idx: int,
+                       batch_size: int,
+                       offset: int,
+                       width: int,
+                       page_size: int = 0,
+                       cache_seqlens: torch.Tensor | None = None,
+                       block_table: torch.Tensor | None = None):
+
+        device = self.model.cache_map[layer_idx]
+        temp_key_state, temp_value_state = self.temp_tensors[device]
+        if width > 0: ext_c.fp16_to_q4_kv(temp_key_state,
+                                          self.key_states[layer_idx],
+                                          self.key_scales[layer_idx],
+                                          temp_value_state,
+                                          self.value_states[layer_idx],
+                                          self.value_scales[layer_idx],
+                                          batch_size,
+                                          offset,
+                                          width,
+                                          page_size,
+                                          cache_seqlens if cache_seqlens is not None else none_tensor,
+                                          block_table if block_table is not None else none_tensor)
+
+
+    def footprint(self) -> list[int]:
+
+        fp = []
+        for layer in self.key_states + self.value_states:
+            dev = layer.device.index
+            while len(fp) <= dev: fp.append(0)
+            fp[dev] += layer.numel() * 1
+        for layer in self.key_scales + self.value_scales:
+            dev = layer.device.index
+            while len(fp) <= dev: fp.append(0)
+            fp[dev] += layer.numel() * 2
+        for temp_k, temp_v in self.temp_tensors.values():
+            fp[temp_k.device.index] += temp_k.numel() * 2
+            fp[temp_v.device.index] += temp_v.numel() * 2
+        return fp
+
+
+    def clone(self) -> ExLlamaV2Cache_Q4:
+
+        new = ExLlamaV2Cache_Q4(self.model, self.batch_size, self.max_seq_len, self)
         return new
```

## exllamav2/compat.py

```diff
@@ -1,17 +1,18 @@
-
+from __future__ import annotations
 import torch
 
 # On some setups Torch will attempt to use GPU peer-to-peer copies even when they are not supported. This is either
 # a driver issue, a bug in Torch, or both. Either way, the result is that .to() will create an empty tensor on the
 # target device and silently fail to copy any data into it. This is a workaround.
 
 tested_peer_copy = None
 
-def test_gpu_peer_copy(device_a, device_b):
+def test_gpu_peer_copy(device_a: torch.Device,
+                       device_b: torch.Device):
     global tested_peer_copy
 
     if tested_peer_copy is None:
         num_dev = torch.cuda.device_count()
         tested_peer_copy = [[0 for _ in range(num_dev)] for _ in range(num_dev)]
 
     idx_a = device_a.index
@@ -31,15 +32,16 @@
         tested_peer_copy[idx_a][idx_b] = 1
         return True
     else:
         tested_peer_copy[idx_a][idx_b] = -1
         return False
 
 
-def safe_move_tensor(tensor, device):
+def safe_move_tensor(tensor: torch.Tensor | tuple[torch.Tensor],
+                     device: torch.Device | str):
 
     # Accept tensor or tuple of tensors
 
     if isinstance(tensor, tuple):
         return tuple(safe_move_tensor(x, device) for x in tensor)
 
     # Accept torch.device or string
```

## exllamav2/config.py

```diff
@@ -1,164 +1,318 @@
+from __future__ import annotations
 import torch
-from safetensors import safe_open
+from exllamav2.fasttensors import STFile
+from exllamav2.architecture import ExLlamaV2ArchParams
 import os, glob, json
+from typing import Any, Dict, List, TypeVar, Union, cast
 
-class ExLlamaV2Config:
 
-    debug_mode = False
-    model_dir: str = None                       # Directory containing model files
+T = TypeVar('T')
+no_default = object()
+
+def read(input_dict: dict[str, Any], expected_type: type, keys: str | list[str], default = no_default) -> T:
+
+    if isinstance(keys, str): keys = [keys]
+
+    for key in keys:
+        input_dict_s = input_dict
+
+        key_split = key.split("->")
+        for subk in key_split[:-1]:
+            input_dict_s = input_dict_s.get(subk, None)
+            if not input_dict_s:
+                key = None
+                break
+        if key is None: continue
+        key = key_split[-1]
+
+        x = input_dict_s.get(key, None)
+        if x is not None:
+
+            if expected_type == float and isinstance(x, int):
+                x = float(x)
+            if expected_type == int and isinstance(x, float) and x == int(x):
+                x = int(x)
+
+            if isinstance(x, expected_type):
+                return cast(T, x)
+            else:
+                raise TypeError(f"Value for {key} is not of expected type {expected_type}")
+
+    if default != no_default: return default
+    raise ValueError(f"Missing any of the following keys: {keys}")
+
+
+class ExLlamaV2Config:
 
-    max_seq_len: int = 2048                     # Maximum sequence length. Sequences longer than this will throw an exception
-    max_batch_size: int = 1                     # Maximum size of batches to process
-    max_input_len: int = 2048                   # Maximum length of input IDs in a single forward pass. Sequences longer than this will be processed in multiple steps
-    max_attention_size: int = 2048 ** 2         # Sequences will be processed in chunks to keep the size of the attention weights matrix <= this
+    model_dir: str | None                       # Directory containing model files
 
-    scale_pos_emb: float = 1.0                  # Factor by which to scale positional embeddings, e.g. for 4096-token sequence use a scaling factor of 2.0, requires finetuned model or LoRA
-    scale_alpha_value: float = 1.0              # Alpha value for NTK RoPE scaling. Similar to compress_pos_emb but works without finetuned model
+    max_seq_len: int                            # Maximum sequence length. Sequences longer than this will throw an exception
+    max_batch_size: int                         # Maximum size of batches to process
+    max_input_len: int                          # Maximum length of input IDs in a single forward pass. Sequences longer than this will be processed in multiple steps
+    max_attention_size: int                     # Sequences will be processed in chunks to keep the size of the attention weights matrix <= this
+    max_output_len: int | None                  # Maximum number of output tokens per forward pass
+
+    scale_pos_emb: float                        # Factor by which to scale positional embeddings, e.g. for 4096-token sequence use a scaling factor of 2.0, requires finetuned model or LoRA
+    scale_alpha_value: float                    # Alpha value for NTK RoPE scaling. Similar to compress_pos_emb but works without finetuned model
+
+    no_flash_attn: bool                         # Implementation will automatically use flash-attn-2 when available
+    fasttensors: bool                           # Experimental, Linux only
+    load_in_q4: bool                            # Load float linear layers in Q4 format (for test/dev purposes, not performant)
 
-    no_flash_attn: bool = False                 # Implementation will automatically use flash-attn-2 when available
+    max_dq_size: int                            # Max number of elements to dequantize at once
 
     # Loaded/set by .prepare():
 
     architecture: str
+    arch: ExLlamaV2ArchParams
 
     model_config: str
     tensor_file_map: dict
     tensor_files: list
 
     tokenizer_path: str
 
     bos_token_id: int
     eos_token_id: int
     pad_token_id: int
 
     hidden_size: int
-    initializer_range: int
+    initializer_range: float
     intermediate_size: int
     num_attention_heads: int
     num_key_value_heads: int
     num_key_value_groups: int
     num_hidden_layers: int
-    rms_norm_eps: float
+    norm_eps: float | None
     vocab_size: int
-    rotary_embedding_base: float = 10000.0      # Constant for all Llama models, nodified by .prepare() if scale_alpha_value != 1.0
-    head_dim: int = 128                         # Constant for all Llama models, except 3b
-
-    qkv_embed: bool = False
+    rotary_embedding_base: float
+    scale_long_factor: list[float] | None
+    scale_short_factor: list[float] | None
+    alt_rope_method: str | None
+    original_max_seq_len: int
+    head_dim: int
+    num_experts: int | None
+    num_experts_per_token: int | None
+    logit_scale: float
+    use_qk_norm: bool
+
+    checkpoint_fused_mlp: bool
+
+
+    def __init__(self,
+                 model_dir: str | None = None):
+        """
+        :param model_dir:
+            If specified, initialize ExLlamaV2Config with values read from model config.
+        """
+
+        self.max_batch_size = 1
+        self.max_input_len = 2048
+        self.max_attention_size = 2048**2
+        self.max_output_len = None
+        self.scale_pos_emb = 1.0
+        self.scale_alpha_value = 1.0
+        self.scale_long_factor = None
+        self.scale_short_factor = None
+        self.alt_rope_method = None
+
+        self.no_flash_attn = False
+        self.fasttensors = False
+        self.load_in_q4 = False
+
+        if model_dir is not None:
+            self.model_dir = model_dir
+            self.prepare()
+        else:
+            self.model_dir = None
 
-
-    def __init__(self):
-        pass
+        self.max_dq_size = 512*(1024**2)
 
 
     # Set low-mem options
 
     def set_low_mem(self):
 
-        self.qkv_embed = True
         self.max_input_len = 1024
         self.max_attention_size = 1024 ** 2
+        self.max_output_len = min(self.max_output_len, 1024)
 
 
     # Populate config with required files from model_dir
 
-    def prepare(self):
+    def prepare(self, no_tensors: bool = False):
 
         assert self.model_dir is not None, "No model_dir specified in ExLlamaV2Config"
         assert os.path.exists(self.model_dir), "Can't find " + self.model_dir
 
         # Load config.json
 
         self.model_config = os.path.join(self.model_dir, "config.json")
         assert os.path.exists(self.model_config), "Can't find " + self.model_config
 
-        with open(self.model_config) as f:
+        with open(self.model_config, encoding = "utf8") as f:
             read_config = json.load(f)
 
-            if "LlamaForCausalLM" in read_config["architectures"]: self.architecture = "Llama"
-            elif "MistralForCausalLM" in read_config["architectures"]: self.architecture = "Llama"
-            elif "YiForCausalLM" in read_config["architectures"]: self.architecture = "Yi"
-            else:
-                print(f" !! Warning, unknown architecture: {repr(read_config['architectures'])}")
-                print(f" !! Loading as LlamaForCausalLM")
-                self.architecture = "Llama"
-
-            self.bos_token_id = read_config["bos_token_id"] if "bos_token_id" in read_config else 1
-            self.eos_token_id = read_config["eos_token_id"] if "eos_token_id" in read_config else 2
-            self.pad_token_id = read_config["pad_token_id"] if "pad_token_id" in read_config else 0
-
-            self.hidden_size = read_config["hidden_size"]
-            self.initializer_range = read_config["initializer_range"]
-            self.intermediate_size = read_config["intermediate_size"]
-            self.num_attention_heads = read_config["num_attention_heads"]
-            self.num_hidden_layers = read_config["num_hidden_layers"]
-            self.rms_norm_eps = read_config["rms_norm_eps"]
-            self.vocab_size = read_config["vocab_size"]
-
-            self.rotary_embedding_base = read_config["rope_theta"] if "rope_theta" in read_config else 10000.0
-
-            if "num_key_value_heads" in read_config:
-                self.num_key_value_heads = read_config["num_key_value_heads"]
-                self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads
-            else:
-                self.num_key_value_heads = self.num_attention_heads
-                self.num_key_value_groups = 1
+        # Load generation_config.json
+
+        self.generation_config_path = os.path.join(self.model_dir, "generation_config.json")
+        if os.path.exists(self.generation_config_path):
+            with open(self.generation_config_path, encoding = "utf8") as f:
+                gen_config = json.load(f)
+                self.generation_config = {}
+                try:
+                    self.generation_config['eos_token_id'] = read(gen_config, list, "eos_token_id", None)
+                except (ValueError, TypeError):
+                    eos_token_id_as_int = read(gen_config, int, "eos_token_id", None)
+                    if eos_token_id_as_int is not None:
+                        self.generation_config['eos_token_id'] = [eos_token_id_as_int]
+                    else:
+                        self.generation_config['eos_token_id'] = None
+                    
+        
+        # Model architecture
+
+        assert len(read_config["architectures"]) == 1, "Multiple architectures defined in config.json"
+        self.architecture = read_config["architectures"][0]
+        self.arch = ExLlamaV2ArchParams(self.architecture, read_config)
+
+        # Vocab params
+
+        self.bos_token_id = read(read_config, int, "bos_token_id", None)  # 1
+        self.eos_token_id = read(read_config, int, "eos_token_id", None)  # 2
+        self.pad_token_id = read(read_config, int, "pad_token_id", None)  # 0
+        self.vocab_size = read(read_config, int, "vocab_size")
+
+        # Standard params
+
+        self.initializer_range = read(read_config, float, ["initializer_range"])
+        self.num_hidden_layers = read(read_config, int, ["num_hidden_layers", "n_layers", "n_layer"])
+
+        # Norm params
+
+        if self.arch.norm_eps_key:
+            self.norm_eps = read(read_config, float, self.arch.norm_eps_key)
+        else:
+            self.norm_eps = 1e-5  # Torch default
+
+        # Model dimensions
 
-            if "max_sequence_length" in read_config: self.max_seq_len = read_config["max_sequence_length"]
-            elif "max_position_embeddings" in read_config: self.max_seq_len = read_config["max_position_embeddings"]
+        self.hidden_size = read(read_config, int, ["hidden_size", "d_model", "n_embd"])
+
+        # Attn params
+
+        self.num_attention_heads = read(read_config, int, ["num_attention_heads", "n_heads", "n_head"])
+        self.head_dim = read(read_config, int, "head_dim", self.hidden_size // self.num_attention_heads)
+
+        if self.arch.mqa:
+            self.num_key_value_heads = 1
+        else:
+            self.num_key_value_heads = read(read_config, int, ["num_key_value_heads", "attn_config->kv_n_heads"], self.num_attention_heads)
+        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads
+        self.use_qk_norm = read(read_config, bool, ["use_qk_norm"], False)
+
+        # MLP params
+
+        if self.arch.default_inner_dim_mult is not None:
+            default_intermediate_size = self.arch.default_inner_dim_mult * self.hidden_size
+        else:
+            default_intermediate_size = no_default
+
+        self.intermediate_size = read(read_config, int, ["intermediate_size", "ffn_config->ffn_hidden_size", "n_inner"], default_intermediate_size)
+        self.num_experts = read(read_config, int, ["num_local_experts", "ffn_config->moe_num_experts"], None)
+        self.num_experts_per_token = read(read_config, int,["num_experts_per_tok", "ffn_config->moe_top_k"], None)
+
+        # Logit scale
+
+        self.logit_scale = read(read_config, float, "logit_scale", 1)
+
+        # Positional embeddings
+
+        self.rotary_embedding_base = read(read_config, float, ["rope_theta", "attn_config->rope_theta"], 10000.0)
+
+        self.max_seq_len = read(read_config, int,["max_sequence_length",
+                                                  "model_max_length",
+                                                  "max_position_embeddings",
+                                                  "max_seq_len",
+                                                  "n_positions"], 2048)
+        self.original_max_seq_len = self.max_seq_len
+
+        rs = read(read_config, dict, "rope_scaling", None)
+        if rs:
+            scaling_type = rs.get("type", None)
+            if scaling_type == "linear":
+                assert "factor" in rs, "'factor' missing from 'rope_scaling' config"
+                self.scale_pos_emb = rs.get("factor", 1.0)
+            if scaling_type == "su":
+                assert "long_factor" in rs, "'long_factor' missing from 'rope_scaling' config"
+                assert "short_factor" in rs, "'short_factor' missing from 'rope_scaling' config"
+                assert "original_max_position_embeddings" in read_config, \
+                    "'original_max_position_embeddings' required for 'su' scaling"
+                self.scale_long_factor = rs["long_factor"]
+                self.scale_short_factor = rs["short_factor"]
+                self.original_max_seq_len = read_config["original_max_position_embeddings"]
+                self.alt_rope_method = "su"
+            # if scaling_type == "yarn":
+            #     self.scale_alpha_value = factor
 
         # Create map of model tensors
 
+        if no_tensors: return
+
         self.tensor_file_map = {}
 
         st_pattern = os.path.join(self.model_dir, "*.safetensors")
         self.tensor_files = glob.glob(st_pattern)
 
         if len(self.tensor_files) == 0:
             raise ValueError(f" ## No .safetensors files found in {self.model_dir}")
 
         for st_file in self.tensor_files:
-
-            with safe_open(st_file, framework = "pt", device = "cpu") as f:
-                for key in f.keys():
-                    self.tensor_file_map[key] = st_file
+            f = STFile.open(st_file, fast = self.fasttensors, keymap = self.arch.keymap)
+            for key in f.get_dict():
+                self.tensor_file_map[key] = st_file
+
+        # For loading checkpoints with fused MLP layers
+
+        if "model.layers.0.mlp.down_proj.weight" not in self.tensor_file_map and \
+            "model.layers.0.mlp.swiglu.w12.weight" in self.tensor_file_map:
+            self.checkpoint_fused_mlp = True
+            self.arch.make_fused_mlp()
+        else:
+            self.checkpoint_fused_mlp = False
 
         # Make sure we found all the layers we need
 
-        layer_keys = [
-            ["input_layernorm", "ln1"],
-            ["post_attention_layernorm", "ln2"],
-            ["self_attn.q_proj"],
-            ["self_attn.k_proj"],
-            ["self_attn.v_proj"],
-            ["self_attn.o_proj"],
-            ["mlp.down_proj"],
-            ["mlp.gate_proj"],
-            ["mlp.up_proj"]
-        ]
-
-        expect_keys = []
-        expect_keys += [
-            ["lm_head"],
-            ["model.norm"],
-            ["model.embed_tokens"]
-        ]
+        expect_keys = self.arch.expect_keys.copy()
+
+        if not self.num_experts or self.num_experts == 1:
+            per_layer_keys = self.arch.layer_keys
+        else:
+            per_layer_keys = set()
+            for expert_idx in range(self.num_experts):
+                for k in self.arch.layer_keys:
+                    skt = [sk.replace(".*.", f".{expert_idx}.") for sk in k]
+                    per_layer_keys.add(tuple(skt))
+            per_layer_keys = list(per_layer_keys)
 
         for layer_idx in range(self.num_hidden_layers):
-            for ks in layer_keys:
+            for ks in per_layer_keys:
                 prefixes = [f"model.layers.{layer_idx}.{k}" for k in ks]
                 expect_keys.append(prefixes)
 
+        all_keys = set(self.tensor_file_map.keys())
+        suffixes = [".q_weight", ".qweight", ".weight", ""]
+
         for prefixes in expect_keys:
+            match = False
             for prefix in prefixes:
-                if any(key.startswith(prefix) for key in self.tensor_file_map):
-                    break
-            else:
+                for suffix in suffixes:
+                    if (prefix + suffix) in all_keys:
+                        match = True
+                        break
+                    if match: break
+                if match: break
+            if not match:
                 raise ValueError(f" ## Could not find {prefix}.* in model")
 
-        # Model dimensions
-
-        self.head_dim = self.hidden_size // self.num_attention_heads
-
-        # Tokenizer
-
-        self.tokenizer_path = os.path.join(self.model_dir, "tokenizer.model")
+        x = 0
```

## exllamav2/embedding.py

```diff
@@ -1,120 +1,148 @@
+from __future__ import annotations
 import torch
 from torch import nn
 from exllamav2.module import ExLlamaV2Module
+from exllamav2.attn import ExLlamaV2Attention
+from exllamav2.compat import safe_move_tensor
 
-class ExLlamaV2Embedding(ExLlamaV2Module):
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.model import ExLlamaV2
+
+EMBEDDING_INDEX: int = 1000000
 
-    embedding: nn.Embedding or None
-    embedding_q: nn.Embedding or None
-    embedding_k: nn.Embedding or None
-    embedding_v: nn.Embedding or None
+class ExLlamaV2Embedding(ExLlamaV2Module):
 
     name: str = "Embedding"
 
-    def __init__(self, model, key):
+    embedding: nn.Embedding | None
+    native_vocab_size: int | None
+
+
+    def __init__(self,
+                 model: ExLlamaV2,
+                 key: str):
         super().__init__(model, key)
 
+        self.native_vocab_size = None
+        self.embedding = None
+
 
     def load(self):
 
         vocab_size = self.model.config.vocab_size
         hidden_size = self.model.config.hidden_size
         pad_token_id = self.model.config.pad_token_id
 
         w = self.load_weight()
         assert isinstance(w, nn.Parameter)
+        w.pin_memory()
+        self.native_vocab_size = w.shape[0]
 
-        pad_id = self.model.config.pad_token_id
-
-        # Padding token should embed a zero vector, but sometimes it doesn't (?)
-
-        # if not torch.is_grad_enabled():
-        #     w[pad_id] *= 0
-
-        self.embedding = nn.Embedding(vocab_size, hidden_size, pad_token_id, device ="meta")
+        self.embedding = nn.Embedding(vocab_size, hidden_size, pad_token_id, device = "meta")
         self.embedding.weight = w
 
 
     def unload(self):
 
         del self.embedding
         self.embedding = None
 
 
-    def get_weight(self):
+    def get_weight(self) -> torch.Tensor:
 
         return self.embedding.weight.data
 
 
-    def weight_footprint(self):
+    def weight_footprint(self) -> int:
 
         vocab_size = self.model.config.vocab_size
         hidden_size = self.model.config.hidden_size
-        kv_size = self.model.config.num_key_value_heads * self.model.config.head_dim
+        # kv_size = self.model.config.num_key_value_heads * self.model.config.head_dim
 
-        if self.model.config.qkv_embed:
-            return vocab_size * hidden_size * 2 + vocab_size * kv_size * 2 * 2
-        else:
-            return vocab_size * hidden_size * 2
+        return vocab_size * hidden_size * 2
 
 
-    def scratch_space_fixed(self):
+    def scratch_space_fixed(self) -> int:
 
         return 0
 
 
-    def scratch_space(self):
+    def scratch_space(self) -> int:
 
         return 0
 
 
-    def forward(self, hidden_states, cache = None, attn_mask = None, past_len = None, intermediates = False, loras = None):
+    def forward(self,
+                hidden_states: torch.Tensor,
+                cache = None,
+                attn_params: ExLlamaV2Attention.Params = None,
+                past_len = None,
+                intermediates: bool = False,
+                loras = None,
+                **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
-        if self.model.config.qkv_embed:
+        # If input IDs contain negative values, assume they are padding tokens from a model with not pad_token_id
+        # defined
 
-            assert not intermediates, "Intermediate values not supported with QKV embeddings"
-            hidden_states = (self.embedding.forward(hidden_states),
-                             self.embedding_q.forward(hidden_states),
-                             self.embedding_k.forward(hidden_states),
-                             self.embedding_v.forward(hidden_states))
+        hidden_states = hidden_states.clamp(min = 0)
 
-        else:
+        # Apply indexed embeddings
 
-            hidden_states = self.embedding.forward(hidden_states)
+        indexed_embeddings = kwargs.get("indexed_embeddings")
+        if indexed_embeddings is not None:
 
-        if intermediates:
-            return {"hidden_states": hidden_states}
-        else:
-            return hidden_states
+            # Split prompt
 
+            offset = EMBEDDING_INDEX
+            input_ids = hidden_states
+            standard_mask = input_ids < offset
+            indexed_mask = input_ids >= offset
 
-    def make_qkv(self, norm, q, k, v):
+        if indexed_embeddings is not None and indexed_mask.any():
 
-        with torch.inference_mode():
+            # Create combined tensor on the target device
 
-            vocab_size = self.model.config.vocab_size
+            batch_size, seq_len = input_ids.shape
             hidden_size = self.model.config.hidden_size
-            hidden_kv_size = self.model.config.num_key_value_heads * self.model.config.head_dim
-            pad_token_id = self.model.config.pad_token_id
+            combined_embeddings = torch.empty(batch_size, seq_len, hidden_size,
+                                              device = indexed_embeddings.device,
+                                              dtype = indexed_embeddings.dtype)
+
+            # Extract standard embeddings, copy to target device and insert in-place
+
+            attn_params.rope_mask = standard_mask
+            if standard_mask.any():
+                for i in range(batch_size):
+                    standard_mask_ = standard_mask[i]
+                    input_ids_ = input_ids[i]
+                    standard_ids_ = input_ids_[standard_mask_]
+                    standard_embeddings_ = self.embedding(standard_ids_)
+                    standard_embeddings_ = safe_move_tensor(standard_embeddings_, indexed_embeddings.device)
+                    combined_embeddings[i][standard_mask_] = standard_embeddings_
+
+            # Normalization
+
+            if self.model.config.arch.normalize_embeddings:
+                combined_embeddings *= self.model.config.hidden_size ** 0.5
+
+            # Extract indexed embeddings and insert in-place
 
-            temp = self.embedding.weight.to(q.device)
-            temp = norm.forward(temp)
+            for i in range(batch_size):
+                indexed_ids_ = input_ids[i][indexed_mask[i]] - offset
+                combined_embeddings[i][indexed_mask[i]] = indexed_embeddings[i][indexed_ids_]
 
-            temp_q = temp @ q
-            temp_k = temp @ k
-            temp_v = temp @ v
-
-        self.embedding_q = nn.Embedding(vocab_size, hidden_size, pad_token_id, device ="meta")
-        self.embedding_k = nn.Embedding(vocab_size, hidden_kv_size, pad_token_id, device ="meta")
-        self.embedding_v = nn.Embedding(vocab_size, hidden_kv_size, pad_token_id, device ="meta")
-        self.embedding_q.weight = nn.Parameter(temp_q.cpu())
-        self.embedding_k.weight = nn.Parameter(temp_k.cpu())
-        self.embedding_v.weight = nn.Parameter(temp_v.cpu())
-
-        del temp
-        del temp_q
-        del temp_k
-        del temp_v
+            hidden_states = combined_embeddings
 
+        # Call embedding module if no indexed embeddings
 
+        else:
+            hidden_states = self.embedding.forward(hidden_states)
 
+            if self.model.config.arch.normalize_embeddings:
+                hidden_states *= self.model.config.hidden_size ** 0.5
+
+        if intermediates:
+            return {"hidden_states": hidden_states}
+        else:
+            return hidden_states
```

## exllamav2/ext.py

```diff
@@ -1,22 +1,122 @@
+from __future__ import annotations
 import torch
 from torch.utils.cpp_extension import load
-import os
+import os, glob
 import sys
 import platform
+import threading
+from exllamav2.util import get_basic_progress
 
 extension_name = "exllamav2_ext"
-verbose = False
-ext_debug = False
+verbose = False  # Print wall of text when compiling
+ext_debug = False  # Compile with debug options
+
+# Since Torch 2.3.0 an annoying warning is printed every time the C++ extension is loaded, unless the
+# TORCH_CUDA_ARCH_LIST variable is set. The default behavior from pytorch/torch/utils/cpp_extension.py
+# is copied in the function below, but without the warning.
+
+def maybe_set_arch_list_env():
+
+    if os.environ.get('TORCH_CUDA_ARCH_LIST', None):
+        return
+
+    if not torch.version.cuda:
+        return
+
+    arch_list = []
+    for i in range(torch.cuda.device_count()):
+        capability = torch.cuda.get_device_capability(i)
+        supported_sm = [int(arch.split('_')[1])
+                        for arch in torch.cuda.get_arch_list() if 'sm_' in arch]
+        if not supported_sm:
+            continue
+        max_supported_sm = max((sm // 10, sm % 10) for sm in supported_sm)
+        # Capability of the device may be higher than what's supported by the user's
+        # NVCC, causing compilation error. User's NVCC is expected to match the one
+        # used to build pytorch, so we use the maximum supported capability of pytorch
+        # to clamp the capability.
+        capability = min(max_supported_sm, capability)
+        arch = f'{capability[0]}.{capability[1]}'
+        if arch not in arch_list:
+            arch_list.append(arch)
+    arch_list = sorted(arch_list)
+    arch_list[-1] += '+PTX'
+
+    os.environ["TORCH_CUDA_ARCH_LIST"] = ";".join(arch_list)
+
+
+# Print feedback from JIT extension build
+
+feedback_stop_event: threading.Event
+feedback_thread: threading.Thread
+feedback_thread_started = False
+
+def count_object_files(directory):
+    pattern = os.path.join(directory, '*.o')
+    files = glob.glob(pattern)
+    return len(files)
+
+def build_feedback():
+    global feedback_stop_event
+    from torch.utils.cpp_extension import _get_build_directory
+    build_dir = _get_build_directory(extension_name, False)
+    num_sources = len(sources_)
+    num_objects = count_object_files(build_dir)
+
+    while not feedback_stop_event.is_set():
+        if num_objects != num_sources: break
+        feedback_stop_event.wait(1)
+
+    progressbar = get_basic_progress()
+    progressbar.start()
+    task_id = progressbar.add_task("Building C++/CUDA extension", total = num_sources)
+
+    while not feedback_stop_event.is_set():
+        num_objects = count_object_files(build_dir)
+        progressbar.update(task_id, completed = num_objects)
+        feedback_stop_event.wait(1)
+
+    progressbar.stop()
+
+def start_build_feedback():
+    global feedback_stop_event, feedback_thread, feedback_thread_started
+    feedback_stop_event = threading.Event()
+    feedback_thread = threading.Thread(target = build_feedback)
+    feedback_thread.start()
+    feedback_thread_started = True
+
+
+def end_build_feedback():
+    global feedback_thread_started
+    if feedback_thread_started:
+        feedback_stop_event.set()
+        feedback_thread.join()
+
+
+# Determine if we're on Windows
 
 windows = (os.name == "nt")
 
+# Determine if extension is already installed or needs to be built
+
+build_jit = False
 try:
     import exllamav2_ext
 except ModuleNotFoundError:
+    build_jit = True
+except ImportError as e:
+    if "undefined symbol" in str(e):
+        print("\"undefined symbol\" error here usually means you are attempting to load a prebuilt extension wheel "
+              "that was compiled against a different version of PyTorch than the one you are you using. Please verify "
+              "that the versions match.")
+        raise e
+
+if build_jit:
+
     # Kludge to get compilation working on Windows
 
     if windows:
 
         def find_msvc():
 
             # Possible locations for MSVC, in order of preference
@@ -64,116 +164,193 @@
             if cl_path:
                 if verbose:
                     print(" -- Injected compiler path:", cl_path)
                 os.environ["path"] += ";" + cl_path
             else:
                 print(" !! Unable to find cl.exe; compilation will probably fail", file = sys.stderr)
 
-
     # gcc / cl.exe flags
 
     extra_cflags = ["/Ox"] if windows else ["-O3"]
 
     if ext_debug:
         extra_cflags += ["-ftime-report", "-DTORCH_USE_CUDA_DSA"]
 
-
     # nvcc flags
 
     extra_cuda_cflags = ["-lineinfo", "-O3"]
-    # extra_cuda_cflags += ["-maxrregcount=128"]
 
     if torch.version.hip:
         extra_cuda_cflags += ["-DHIPBLAS_USE_HIP_HALF"]
 
     # linker flags
 
     extra_ldflags = []
 
     if windows:
         extra_ldflags += ["cublas.lib"]
         if sys.base_prefix != sys.prefix:
             extra_ldflags += [f"/LIBPATH:{os.path.join(sys.base_prefix, 'libs')}"]
 
-
     # sources
 
     library_dir = os.path.dirname(os.path.abspath(__file__))
     sources_dir = os.path.join(library_dir, extension_name)
 
     sources_ = \
     [
-        "ext.cpp",
+        "ext_bindings.cpp",
+        "ext_cache.cpp",
+        "ext_gemm.cpp",
+        "ext_hadamard.cpp",
+        "ext_norm.cpp",
+        "ext_qattn.cpp",
+        "ext_qmatrix.cpp",
+        "ext_qmlp.cpp",
+        "ext_quant.cpp",
+        "ext_rope.cpp",
+        "ext_safetensors.cpp",
+        "ext_sampling.cpp",
+        "cuda/h_add.cu",
         "cuda/h_gemm.cu",
         "cuda/lora.cu",
         "cuda/pack_tensor.cu",
         "cuda/quantize.cu",
         "cuda/q_matrix.cu",
         "cuda/q_attn.cu",
         "cuda/q_mlp.cu",
         "cuda/q_gemm.cu",
         "cuda/rms_norm.cu",
+        "cuda/head_norm.cu",
+        "cuda/layer_norm.cu",
         "cuda/rope.cu",
         "cuda/cache.cu",
+        "cuda/util.cu",
+        "cuda/comp_units/kernel_select.cu",
+        "cuda/comp_units/unit_gptq_1.cu",
+        "cuda/comp_units/unit_gptq_2.cu",
+        "cuda/comp_units/unit_gptq_3.cu",
+        "cuda/comp_units/unit_exl2_1a.cu",
+        "cuda/comp_units/unit_exl2_1b.cu",
+        "cuda/comp_units/unit_exl2_2a.cu",
+        "cuda/comp_units/unit_exl2_2b.cu",
+        "cuda/comp_units/unit_exl2_3a.cu",
+        "cuda/comp_units/unit_exl2_3b.cu",
         "cpp/quantize_func.cpp",
-        "cpp/sampling.cpp"
+        "cpp/profiling.cpp",
+        "cpp/generator.cpp",
+        "cpp/sampling.cpp",
+        "cpp/sampling_avx2.cpp",
+        "cpp/safetensors.cpp"
     ]
 
     sources = [os.path.join(sources_dir, s) for s in sources_]
 
+    # Suppress warning
+
+    maybe_set_arch_list_env()
+
+    # Provide build feedback if loading takes a long time, suggesting the extension is being compiled
+
+    if not verbose:
+
+        def load_feedback():
+            print("Loading exllamav2_ext extension (JIT)...")
+            start_build_feedback()
+
+        timer = threading.Timer(1, load_feedback)
+        timer.start()
 
     # Load extension
 
-    exllamav2_ext = load \
-    (
-        name = extension_name,
-        sources = sources,
-        extra_include_paths = [sources_dir],
-        verbose = verbose,
-        extra_ldflags = extra_ldflags,
-        extra_cuda_cflags = extra_cuda_cflags,
-        extra_cflags = extra_cflags
-    )
+    try:
+        exllamav2_ext = load \
+        (
+            name = extension_name,
+            sources = sources,
+            extra_include_paths = [sources_dir],
+            verbose = verbose,
+            extra_ldflags = extra_ldflags,
+            extra_cuda_cflags = extra_cuda_cflags,
+            extra_cflags = extra_cflags
+        )
+    finally:
+        if not verbose:
+            timer.cancel()
+            end_build_feedback()
 
 ext_c = exllamav2_ext
 
 
 # Dummy tensor to pass to C++ extension in place of None/NULL
 
 none_tensor = torch.empty((1, 1), device = "meta")
 
 
+# Group map needed for irregular group sizes
+
+def make_group_map(q_groups: torch.Tensor, num_qrows: int) -> torch.Tensor:
+
+    gr = q_groups.tolist()
+    group_map = []
+    num_groups = len(gr) // 2
+    row = 0
+
+    for i in range(num_groups):
+        bits = gr[i * 2]
+        if i < num_groups - 1:
+            qrows = gr[i * 2 + 3] - gr[i * 2 + 1]
+        else:
+            qrows = num_qrows - gr[i * 2 + 1]
+        rows = qrows * 32 // bits
+        for j in range(rows):
+            group_map += [i]
+            group_map += [rows - j]
+
+    return torch.tensor(group_map, dtype = torch.short, device = q_groups.device)
+
+
 # Create Q matrix
 
-def make_q_matrix(w: dict, temp_dq, key: str = None):
+def make_q_matrix(w: dict,
+                  temp_dq: torch.Tensor,
+                  key: str = None,
+                  prescale: float = 1,
+                  max_dq_rows = 0):
 
     # EXL2
 
     if "q_weight" in w:
 
-        w["q_scale_max"] /= 256
+        w["q_scale_max"] *= prescale / 256
         w["q_perm"] = w["q_perm"].short()
         w["q_invperm"] = w["q_invperm"].short()
 
+        if "q_group_map" not in w:
+            w["q_group_map"] = make_group_map(w["q_groups"], w["q_weight"].shape[0])
+
         return ext_c.make_q_matrix(w["q_weight"],
                                    w["q_perm"],
                                    w["q_invperm"],
                                    w["q_scale"],
                                    w["q_scale_max"],
                                    w["q_groups"],
+                                   w["q_group_map"],
                                    none_tensor,
                                    none_tensor,
                                    none_tensor,
-                                   temp_dq)
-
+                                   w.get("bias", none_tensor),
+                                   temp_dq,
+                                   max_dq_rows)
 
     # GPTQ
 
     elif "qweight" in w:
 
+        if prescale != 1: w["scales"] *= prescale
         if w["scales"].dtype == torch.float: w["scales"] = w["scales"].half()
 
         # GPTQ with g_idx (act_order)
 
         if "g_idx" in w and not (w["g_idx"] == 0).all().item():
 
             w["q_perm"] = torch.empty((w["qweight"].shape[0] * 8,), dtype = torch.short, device = w["qweight"].device)
@@ -181,28 +358,34 @@
 
             return ext_c.make_q_matrix(w["qweight"],
                                        w["q_perm"],
                                        w["q_invperm"],
                                        none_tensor,
                                        none_tensor,
                                        none_tensor,
+                                       none_tensor,
                                        w["qzeros"],
                                        w["scales"],
                                        w["g_idx"].cpu(),
-                                       temp_dq)
+                                       w.get("bias", none_tensor),
+                                       temp_dq,
+                                       max_dq_rows)
 
         # GPTQ without g_idx
 
         else:
 
             return ext_c.make_q_matrix(w["qweight"],
                                        none_tensor,
                                        none_tensor,
                                        none_tensor,
                                        none_tensor,
                                        none_tensor,
+                                       none_tensor,
                                        w["qzeros"],
                                        w["scales"],
                                        none_tensor,
-                                       temp_dq)
+                                       w.get("bias", none_tensor),
+                                       temp_dq,
+                                       max_dq_rows)
```

## exllamav2/linear.py

```diff
@@ -1,63 +1,168 @@
+from __future__ import annotations
 import torch
 import torch.nn.functional as F
-from exllamav2.module import ExLlamaV2Module
 from torch import nn
 from exllamav2 import ext
 from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
-from safetensors import safe_open
+from exllamav2.module import ExLlamaV2Module
+
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.lora import ExLlamaV2Lora
+    from exllamav2.model import ExLlamaV2
 
 
 class ExLlamaV2Linear(ExLlamaV2Module):
 
+    name: str = "Linear"
+
     in_features: int
     out_features: int
     has_bias: bool
+    prescale: float
 
-    linear: nn.Linear or None = None
-    q_handle: int or None = None
-    q_tensors: dict or None = None
-
-    name: str = "Linear"
+    linear: nn.Linear | None
+    q_handle: int | None
+    q_tensors: dict | None
+    q4_weight: torch.Tensor | None
+    q4_scales: torch.Tensor | None
+    fp16_bias: torch.Tensor | None
 
     temp_dq: torch.tensor
-    padding: int = 0
+    padding: int
+    max_out_len: int
 
     lora_a_tensors: dict
     lora_b_tensors: dict
 
-    def __init__(self, model, key, in_features, out_features, has_bias):
+    f_key: str | None
+    f_beg: int | None
+    f_end: int | None
+
+    is_sub_module: bool
+
+    def __init__(self,
+                 model: ExLlamaV2,
+                 key: str,
+                 in_features: int,
+                 out_features: int,
+                 has_bias: bool,
+                 pad32: bool = True,
+                 max_out_len: int | None = None,
+                 prescale: float = 1,
+                 f_key: str = None,
+                 f_beg: int = None,
+                 f_end: int = None,
+                 is_sub_module: bool = True):
         super().__init__(model, key)
 
-        self.padding = -out_features % 32
+        self.is_sub_module = is_sub_module
+
+        if pad32:
+            self.padding = -out_features % 32
+        else:
+            self.padding = 0
 
         self.in_features = in_features
         self.out_features = out_features + self.padding
         self.has_bias = has_bias
         self.temp_dq = None
         self.footprint = -1
+        self.max_out_len = max_out_len
+        self.prescale = prescale
+        self.prev_prescale = None
+
+        self.linear = None
+        self.q_handle = None
+        self.q_tensors = None
+        self.q4_weight = None
+        self.q4_scales = None
+        self.fp16_bias = None
 
         self.lora_a_tensors = {}
         self.lora_b_tensors = {}
 
+        self.f_key = f_key
+        self.f_beg = f_beg
+        self.f_end = f_end
+
+        self.assumed_footprint = in_features * (out_features + self.padding) * 2 + 128
+
 
-    def load(self, w = None):
+    def load(self,
+             w: dict | nn.Parameter | tuple | None = None,
+             device_tensors: bool = True):
 
+        if self.f_key: w = self.load_weight_fused(self.f_key, self.f_beg, self.f_end, self.in_features, self.out_features)
         if w is None: w = self.load_weight()
+
+        # Load quantized linear layer from dictionary
+
         if isinstance(w, dict):
-            device_tensors = self.model.get_device_tensors(self.device_idx)
-            device_tensors.begin_scratch_alloc()
-            self.temp_dq = device_tensors.get_scratch_slice(self.temp_dq_size())
+            assert not self.model.config.load_in_q4, "Can't load quantized layer in Q4 mode"
+            if self.has_bias:
+                assert "bias" in w, self.key + " has no bias but bias expected"
+            else:
+                assert "bias" not in w, self.key + " has bias but bias is not expected"
+            if device_tensors:
+                device_tensors = self.model.get_device_tensors(self.device_idx)
+                device_tensors.begin_scratch_alloc()
+                self.temp_dq = device_tensors.get_scratch_slice(self.temp_dq_size())
+            else:
+                self.temp_dq = none_tensor
             self.q_tensors = w
-            self.q_handle = ext.make_q_matrix(w, self.temp_dq)
+            self.q_handle = ext.make_q_matrix(w,
+                                              self.temp_dq,
+                                              prescale = self.prescale,
+                                              max_dq_rows = self.model.config.max_dq_size // self.out_features)
+            self.prev_prescale = self.prescale
+            self.prescale = 1
+
+        # Load FP16 linear layer without bias, optionally quantize to Q4
 
         elif isinstance(w, nn.Parameter):
+            assert not self.has_bias, self.key + " has no bias tensor but bias is expected"
             if self.padding > 0: w = nn.Parameter(F.pad(w.data, (0, 0, 0, self.padding)).contiguous())
-            self.linear = nn.Linear(self.in_features, self.out_features, self.has_bias, device = "meta", dtype = torch.float16)
-            self.linear.weight = w
+            if not self.model.config.load_in_q4 or not ".layers." in self.key:
+                self.linear = nn.Linear(self.in_features, self.out_features, self.has_bias, device = "meta", dtype = torch.float16)
+                self.linear.weight = w
+            else:
+                self.q4_weight = torch.empty((self.out_features * self.in_features // 2,), device = self.device(), dtype = torch.uint8)
+                self.q4_scales = torch.empty((self.out_features * self.in_features // 32,), device = self.device(), dtype = torch.half)
+                ext_c.matrix_fp16_to_q4(w.contiguous(), self.q4_weight, self.q4_scales)
+
+        # Load FP16 linear layer with bias, optionally quantize to Q4
+
+        elif isinstance(w, tuple):
+            assert self.has_bias, self.key + " has bias tensor but bias is not expected"
+            ww = w[0]
+            wb = w[1]
+            if self.padding > 0:
+                ww = nn.Parameter(F.pad(ww.data, (0, 0, 0, self.padding)).contiguous())
+                wb = nn.Parameter(F.pad(wb.data, (0, 0, 0, self.padding)).contiguous())
+            if not self.model.config.load_in_q4 or not ".layers." in self.key:
+                self.linear = nn.Linear(self.in_features, self.out_features, self.has_bias, device = "meta", dtype = torch.float16)
+                self.linear.weight = ww
+                self.linear.bias = wb
+            else:
+                self.q4_weight = torch.empty((self.out_features * self.in_features // 2,), device = self.device(), dtype = torch.uint8)
+                self.q4_scales = torch.empty((self.out_features * self.in_features // 32,), device = self.device(), dtype = torch.half)
+                ext_c.matrix_fp16_to_q4(ww.contiguous(), self.q4_weight, self.q4_scales)
+                self.fp16_bias = wb
+
+
+    def matrix_shape(self):
+
+        return self.in_features, self.out_features
+
+
+    def numel(self) -> int:
+
+        return self.in_features * self.out_features
 
 
     def unload(self):
 
         if self.linear is not None:
             del self.linear
             self.linear = None
@@ -66,126 +171,159 @@
             ext_c.free_q_matrix(self.q_handle)
             self.q_handle = None
 
         if self.q_tensors is not None:
             for k, v in self.q_tensors.items(): del v
             self.q_tensors = None
 
+        if self.q4_weight is not None:
+            del self.q4_weight
+            self.q4_weight = None
+
+        if self.q4_scales is not None:
+            del self.q4_scales
+            self.q4_scales = None
+
+        if self.fp16_bias is not None:
+            del self.fp16_bias
+            self.fp16_bias = None
+
+        self.temp_dq = None
+        if self.prev_prescale is not None:
+            self.prescale = self.prev_prescale
+            self.prev_prescale = None
+
 
-    def get_weight(self):
+    def get_weight(self) -> torch.Tensor:
 
         return self.linear.weight.data
 
 
-    def scratch_space_fixed(self):
+    def scratch_space_fixed(self) -> int:
 
         return self.temp_dq_size() + \
-               self.temp_fwd_size()
+               (self.temp_fwd_size() if self.is_sub_module else 0)
 
 
-    def scratch_space(self):
+    def scratch_space(self) -> int:
 
         return self.temp_dq_size() + \
-               self.temp_fwd_size()
+               (self.temp_fwd_size() if self.is_sub_module else 0)
 
 
-    def temp_dq_size(self):
+    def temp_dq_size(self) -> int:
 
-        return self.in_features * self.out_features * 2 + 128
+        dq = self.in_features * self.out_features
+        dq = min(dq, self.model.config.max_dq_size)
+        dq = dq * 2 + 128
+        return dq
 
 
-    def temp_fwd_size(self):
+    def temp_fwd_size(self) -> int:
 
-        return self.out_features * self.model.config.max_input_len * self.model.config.max_batch_size * 4 + 128
+        max_len = self.model.config.max_input_len if self.max_out_len is None else \
+            min(self.max_out_len, self.model.config.max_input_len)
+        return self.out_features * max_len * self.model.config.max_batch_size * 4 + 128
 
 
-    def forward(self, hidden_states, cache = None, attn_mask = None, past_len = None, intermediates = False, loras = None, force_recons = False, force_cuda = False):
+    def forward(self,
+                hidden_states: torch.Tensor,
+                cache = None,
+                attn_params = None,
+                past_len = None,
+                intermediates: bool = False,
+                loras: list[ExLlamaV2Lora] | None = None,
+                force_recons: bool = False,
+                force_cuda: bool = False,
+                **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
         # Linear forward
 
         if self.q_handle is not None and not force_recons:
 
             output_shape = hidden_states.shape[:-1] + (self.out_features,)
             hidden_states = hidden_states.view(-1, hidden_states.shape[-1])
-            # hidden_states = hidden_states[:, self.q_tensors["q_perm"]]
             output = torch.empty((hidden_states.shape[0], self.out_features), dtype = torch.half, device = self.device())
             ext_c.gemm_half_q_half(hidden_states, self.q_handle, output, force_cuda)
 
             hidden_states_out = output.view(output_shape)
 
         else:
 
             matrix = self.get_weight_tensor_dq()
             hidden_states_out = torch.matmul(hidden_states, matrix)
+            if self.has_bias:
+                bias = self.get_bias_tensor()
+                hidden_states_out += bias
+
+            if self.prescale != 1:
+                hidden_states_out.mul_(self.prescale)
 
         # Evaluate LoRAs
 
         if loras is not None:
             for lora in loras:
-                lora_a = self.lora_a_tensors[lora] if lora in self.lora_a_tensors else None
-                lora_b = self.lora_b_tensors[lora] if lora in self.lora_b_tensors else None
+                lora_a = self.lora_a_tensors.get(lora)
+                lora_b = self.lora_b_tensors.get(lora)
                 if lora_a is not None:
                     assert lora_b is not None
                     temp = torch.matmul(hidden_states, lora_a)
                     hidden_states_out += torch.matmul(temp, lora_b)
 
         if intermediates:
             return {"hidden_states": hidden_states_out}
         else:
             return hidden_states_out
 
 
-    def dump_group_info(self):
-
-        if "q_groups" in self.q_tensors:
+    def get_weight_tensor_dq(self) -> torch.Tensor:
 
-            groups = self.q_tensors["q_groups"].cpu()
-
-            if "q_invperm" in self.q_tensors:
-                height = self.q_tensors["q_invperm"].shape[0]
-            else:
-                height = self.q_tensors["q_weight"].shape[0] * 8
+        if self.linear is not None:
+            return self.linear.weight.data.T
 
-            groupsize = 1
-            while groupsize * groups.shape[0] / 2 < height:
-                groupsize *= 2
-
-            gis = f"gs: {groupsize}, "
-            i = 0
-            pg = 0
-            gc = 0
-            while i <= groups.shape[0]:
-                g = groups[i].item() if i < groups.shape[0] else -1
-                if g != pg:
-                    if pg != 0:
-                        gis += f"{pg}: {gc}, "
-                        gc = 0
-                    pg = g
-                gc += 1
-                i += 2
+        elif self.q_handle is not None:
+            tensor = torch.empty((self.in_features, self.out_features), dtype = torch.half, device = self.device())
+            ext_c.reconstruct(self.q_handle, tensor)
+            return tensor
+            # ext_c.reconstruct(self.q_handle, self.temp_dq)
+            # return self.temp_dq
 
-            return gis
+        elif self.q4_weight is not None:
+            tensor = torch.empty((self.out_features, self.in_features), dtype = torch.half, device = self.device())
+            ext_c.matrix_q4_to_fp16(self.q4_weight, self.q4_scales, tensor)
+            return tensor.T
 
         else:
-
-            return "GPTQ"
+            raise ValueError(f"Layer {self.key} has no data")
 
 
-    def get_weight_tensor_dq(self):
+    def get_bias_tensor(self) -> torch.Tensor:
 
         if self.linear is not None:
-            return self.linear.weight.data.T
+            return self.linear.bias.data
 
         elif self.q_handle is not None:
-            tensor = torch.empty((self.in_features, self.out_features), dtype = torch.half, device = self.device())
-            ext_c.reconstruct(self.q_handle, tensor)
-            return tensor
-            # ext_c.reconstruct(self.q_handle, self.temp_dq)
-            # return self.temp_dq
+            return self.q_tensors["bias"]
+
+        elif self.fp16_bias is not None:
+            return self.fp16_bias
 
         else:
             raise ValueError(f"Layer {self.key} has no data")
 
 
-    def is_quant(self):
+    def is_quant(self) -> bool:
+
+        return self.q_handle is not None
+
+
+    def rank_reduce(self, k: float):
+
+        assert not self.is_quant(), "Can't rank-reduce quantized layer"
+
+        weight = self.linear.weight.data.float()
+        max_rank = min(weight.shape[0], weight.shape[1])
+        desired_rank = int(max_rank * k)
+        results = torch.svd_lowrank(weight, q = desired_rank, niter = 10)
+        weight_approx = results[0] @ torch.diag(results[1]) @ results[2].T
 
-        return self.q_handle is not None
+        self.linear.weight = nn.Parameter(weight_approx.half())
```

## exllamav2/lora.py

```diff
@@ -1,16 +1,24 @@
+from __future__ import annotations
 from exllamav2.config import ExLlamaV2Config
 from exllamav2.linear import ExLlamaV2Linear
 import os, json
 from safetensors.torch import load_file as safe_load_file
 from torch import load as load_file
 import torch
 from exllamav2.compat import safe_move_tensor
 
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.model import ExLlamaV2
+
 class ExLlamaV2Lora:
+
+    model: ExLlamaV2
+
     lora_config_path: str
     lora_path: str
 
     lora_config_path: str
     lora_path: str
     lora_r: int
     lora_alpha: float
@@ -18,143 +26,141 @@
     config: ExLlamaV2Config
     bias_ignored: bool
 
     tensors: dict
     target_modules: dict
 
     @staticmethod
-    def from_directory(model, directory):
+    def from_directory(model, directory, lora_scaling = 1.0):
         config_path = os.path.join(directory, "adapter_config.json")
         lora_path_bin = os.path.join(directory, "adapter_model.bin")
         lora_path_st = os.path.join(directory, "adapter_model.safetensors")
-        if os.path.exists(lora_path_bin): return ExLlamaV2Lora(model, config_path, lora_path_bin)
-        if os.path.exists(lora_path_st): return ExLlamaV2Lora(model, config_path, lora_path_st)
+        if os.path.exists(lora_path_bin): return ExLlamaV2Lora(model, config_path, lora_path_bin, lora_scaling)
+        if os.path.exists(lora_path_st): return ExLlamaV2Lora(model, config_path, lora_path_st, lora_scaling)
         raise ValueError(f"No LoRA found in {directory}")
 
+    @torch.inference_mode
+    def __init__(self,
+                 model: ExLlamaV2,
+                 lora_config_path: str,
+                 lora_path: str,
+                 lora_scaling: float = 1.0):
 
-    def __init__(self, model, lora_config_path, lora_path):
-
-        with torch.inference_mode():
-
-            self.lora_config_path = lora_config_path
-            self.lora_path = lora_path
-            self.model = model
-            self.config = model.config
-            self.tensors = {}
-            self.target_modules = {}
-            self.bias_ignored = False
-
-            # Check for QKV embed
-
-            assert not self.model.config.qkv_embed, "LoRAs not implemented for models loaded with QKV embeddings"
-
-            # Grab relevant items from LoRA config
+        self.lora_config_path = lora_config_path
+        self.lora_path = lora_path
+        self.model = model
+        self.config = model.config
+        self.tensors = {}
+        self.target_modules = {}
+        self.bias_ignored = False
+        self.lora_scaling = lora_scaling
 
-            with open(lora_config_path) as f:
-                read_config = json.load(f)
+        # Grab relevant items from LoRA config
 
-            self.lora_r = read_config["r"]
-            self.lora_alpha = float(read_config["lora_alpha"])
-            self.lora_scaling = self.lora_alpha / self.lora_r
+        with open(lora_config_path, encoding = "utf8") as f:
+            read_config = json.load(f)
 
-            if "fan_in_fan_out" in read_config and read_config["fan_in_fan_out"]:
-                raise ValueError(" ## Error: fan_in_fan_out mode not supported.")
+        self.lora_r = read_config["r"]
+        self.lora_alpha = float(read_config["lora_alpha"])
+        self.lora_scaling *= self.lora_alpha / self.lora_r
 
-            # Load LoRA weights
+        if "fan_in_fan_out" in read_config and read_config["fan_in_fan_out"]:
+            raise ValueError(" ## Error: fan_in_fan_out mode not supported.")
 
-            if self.lora_path.endswith(".safetensors"):
-                f = safe_load_file(self.lora_path, device = "cpu")
-            else:
-                f = load_file(self.lora_path, map_location = "cpu")
+        # Load LoRA weights
 
-            for key in f.keys():
-                tensor = f[key]
+        if self.lora_path.endswith(".safetensors"):
+            f = safe_load_file(self.lora_path, device = "cpu")
+        else:
+            f = load_file(self.lora_path, map_location = "cpu")
 
-                # Find target
+        for key in f.keys():
+            tensor = f[key]
 
-                i = key.find("model.layers.")
-                if i == -1: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
+            # Find target
 
-                target_key = key[i:]
-                ks = target_key.split(".")
-                decoder_idx = int(ks[2])
-                decoder_part = ks[3]
-                decoder_layer = ks[4]
-                lora_half = ks[5]
+            i = key.find("model.layers.")
+            if i == -1: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
 
-                if lora_half == "bias":
-                    epsilon = 1e-6
-                    if torch.max(tensor) > epsilon or torch.max(tensor) < -epsilon:
-                        raise ValueError(f" ## Error: unsupported bias target {self.lora_path}: {key}")
-                    self.bias_ignored = True
-                    continue
+            target_key = key[i:]
+            ks = target_key.split(".")
+            decoder_idx = int(ks[2])
+            decoder_part = ks[3]
+            decoder_layer = ".".join(ks[4:-2])
+            lora_half = ks[-2]
 
-                target_module = self.model.modules_dict["model.layers." + str(decoder_idx) + "." + decoder_part + "." + decoder_layer]
-                # if decoder_part == "self_attn": target_module = target_module.self_attn
-                # elif decoder_part == "mlp": target_module = target_module.mlp
-                # else: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
+            if lora_half == "bias":
+                epsilon = 1e-6
+                if torch.max(tensor) > epsilon or torch.max(tensor) < -epsilon:
+                    raise ValueError(f" ## Error: unsupported bias target {self.lora_path}: {key}")
+                self.bias_ignored = True
+                continue
 
-                # if   decoder_layer == "q_proj": target_module = target_module.q_proj
-                # elif decoder_layer == "k_proj": target_module = target_module.k_proj
-                # elif decoder_layer == "v_proj": target_module = target_module.v_proj
-                # elif decoder_layer == "o_proj": target_module = target_module.o_proj
-                # elif decoder_layer == "gate_proj": target_module = target_module.gate_proj
-                # elif decoder_layer == "up_proj": target_module = target_module.up_proj
-                # elif decoder_layer == "down_proj": target_module = target_module.down_proj
-                # else: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
+            target_module = self.model.modules_dict["model.layers." + str(decoder_idx) + "." + decoder_part + "." + decoder_layer]
+            # if decoder_part == "self_attn": target_module = target_module.self_attn
+            # elif decoder_part == "mlp": target_module = target_module.mlp
+            # else: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
 
-                # Check that shape is compatible
+            # if   decoder_layer == "q_proj": target_module = target_module.q_proj
+            # elif decoder_layer == "k_proj": target_module = target_module.k_proj
+            # elif decoder_layer == "v_proj": target_module = target_module.v_proj
+            # elif decoder_layer == "o_proj": target_module = target_module.o_proj
+            # elif decoder_layer == "gate_proj": target_module = target_module.gate_proj
+            # elif decoder_layer == "up_proj": target_module = target_module.up_proj
+            # elif decoder_layer == "down_proj": target_module = target_module.down_proj
+            # else: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
 
-                assert isinstance(target_module, ExLlamaV2Linear)
+            # Check that shape is compatible
 
-                if lora_half == "lora_A":
-                    in_features = tensor.shape[1]
-                    out_features = None
-                elif lora_half == "lora_B":
-                    in_features = None
-                    out_features = tensor.shape[0]
-                else: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
+            assert isinstance(target_module, ExLlamaV2Linear)
 
-                if (in_features and in_features != target_module.in_features) or (out_features and out_features != target_module.out_features):
-                    raise ValueError(f" ## Error: incompatible tensor shape in {self.lora_path}: {key}")
+            if lora_half == "lora_A":
+                in_features = tensor.shape[1]
+                out_features = None
+            elif lora_half == "lora_B":
+                in_features = None
+                out_features = tensor.shape[0]
+            else: raise ValueError(f" ## Error: unsupported layer in {self.lora_path}: {key}")
 
-                # For efficiency, transpose adapter instead of transposing state during inference
+            if (in_features and in_features != target_module.in_features) or (out_features and out_features != target_module.out_features):
+                raise ValueError(f" ## Error: incompatible tensor shape in {self.lora_path}: {key}")
 
-                tensor = tensor.T.contiguous()
+            # For efficiency, transpose adapter instead of transposing state during inference
 
-                # Pre-scale
+            tensor = tensor.T.contiguous()
 
-                if lora_half == "lora_B" and self.lora_scaling != 1.0: tensor.mul_(self.lora_scaling)
+            # Pre-scale
 
-                # Check that dtype is compatible, or convert
+            if lora_half == "lora_B" and self.lora_scaling != 1.0: tensor.mul_(self.lora_scaling)
 
-                if tensor.dtype == torch.bfloat16:
-                    tensor = tensor.to(torch.float16)
+            # Check that dtype is compatible, or convert
 
-                elif tensor.dtype == torch.float32:
-                    tensor = tensor.to(torch.float16)
+            if tensor.dtype == torch.bfloat16:
+                tensor = tensor.to(torch.float16)
 
-                elif tensor.dtype == torch.float16:
-                    pass
+            elif tensor.dtype == torch.float32:
+                tensor = tensor.to(torch.float16)
 
-                else: raise ValueError(f" ## Error: unsupported tensor dtype in {self.lora_path}")
+            elif tensor.dtype == torch.float16:
+                pass
 
-                # Move to target device
+            else: raise ValueError(f" ## Error: unsupported tensor dtype in {self.lora_path}")
 
-                tensor = safe_move_tensor(tensor, target_module.device())
-                if lora_half == "lora_A": target_module.lora_a_tensors[self] = tensor
-                if lora_half == "lora_B": target_module.lora_b_tensors[self] = tensor
+            # Move to target device
 
-                # Store adapter tensor
+            tensor = safe_move_tensor(tensor, target_module.device())
+            if lora_half == "lora_A": target_module.lora_a_tensors[self] = tensor
+            if lora_half == "lora_B": target_module.lora_b_tensors[self] = tensor
 
-                self.tensors[target_key] = tensor
-                self.target_modules[target_key] = target_module
+            # Store adapter tensor
 
-            self.model.update_loras()
+            self.tensors[target_key] = tensor
+            self.target_modules[target_key] = target_module
 
+        self.model.update_loras()
 
 
     def unload(self):
 
         for k, v in self.target_modules.items():
             if self in v.lora_a_tensors: del v.lora_a_tensors[self]
             if self in v.lora_b_tensors: del v.lora_b_tensors[self]
```

## exllamav2/mlp.py

```diff
@@ -1,188 +1,307 @@
+from __future__ import annotations
 import torch
 import torch.nn.functional as F
+from torch import nn
 from exllamav2.module import ExLlamaV2Module
 from exllamav2.rmsnorm import ExLlamaV2RMSNorm
+from exllamav2.layernorm import ExLlamaV2LayerNorm
 from exllamav2.linear import ExLlamaV2Linear
 from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
-from exllamav2 import ext
+from exllamav2.lora import ExLlamaV2Lora
 
-class ExLlamaV2MLP(ExLlamaV2Module):
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.model import ExLlamaV2
 
-    layer_idx: int
-    post_attention_layernorm: ExLlamaV2RMSNorm
-    gate_proj: ExLlamaV2Linear
-    up_proj: ExLlamaV2Linear
-    down_proj: ExLlamaV2Linear
 
-    name: str = "MLP"
-    submodules: list
+class ExLlamaV2MLP(ExLlamaV2Module):
 
-    q_handle: int or None = None
+    name: str = "MLP"
 
-    temp_lora_size: int = 0
+    layer_idx: int
+    post_attention_layernorm: ExLlamaV2RMSNorm | ExLlamaV2LayerNorm | None
+    gate_proj: ExLlamaV2Linear | None
+    up_proj: ExLlamaV2Linear | None
+    down_proj: ExLlamaV2Linear | None
+
+    q_handle: int | None
+
+    temp_lora_size: int
+
+    has_norm: bool
+    has_residual: bool
+
+    def __init__(self,
+                 model: ExLlamaV2,
+                 key: str,
+                 layer_idx: int,
+                 has_norm: bool = True,
+                 has_residual: bool = True):
 
-    def __init__(self, model, key, layer_idx):
         super().__init__(model, key)
+        cfg = self.model.config
 
         self.layer_idx = layer_idx
+        self.has_norm = has_norm
+        self.has_residual = has_residual
+
+        self.q_handle = None
+        self.temp_lora_size = 0
+
+        f_a = 0
+        f_b = cfg.intermediate_size
+        f_c = f_b + cfg.intermediate_size
+        f_key = (key + ".mlp." + cfg.arch.fused_mlp_key_12) if cfg.arch.fused_mlp_key_12 else None
+
+        if self.has_norm:
+            if cfg.arch.norm == "layernorm":
+                self.post_attention_layernorm = ExLlamaV2LayerNorm(model, key + cfg.arch.norm_key_2)
+            elif cfg.arch.norm == "rmsnorm":
+                self.post_attention_layernorm = ExLlamaV2RMSNorm(model, key + cfg.arch.norm_key_2)
+        else:
+            self.post_attention_layernorm = None
 
-        hidden_size = self.model.config.hidden_size
-        intermediate_size = self.model.config.intermediate_size
+        self.up_proj = ExLlamaV2Linear(model, key + cfg.arch.mlp_key_up, cfg.hidden_size, cfg.intermediate_size, self.model.config.arch.mlp_bias, f_key = f_key, f_beg = f_b, f_end = f_c)
+        self.down_proj = ExLlamaV2Linear(model, key + cfg.arch.mlp_key_down, cfg.intermediate_size, cfg.hidden_size, self.model.config.arch.mlp_bias)
 
-        self.post_attention_layernorm = ExLlamaV2RMSNorm(model, key + ".post_attention_layernorm")
-        self.gate_proj = ExLlamaV2Linear(model, key + ".mlp.gate_proj", hidden_size, intermediate_size, False)
-        self.up_proj = ExLlamaV2Linear(model, key + ".mlp.up_proj", hidden_size, intermediate_size, False)
-        self.down_proj = ExLlamaV2Linear(model, key + ".mlp.down_proj", intermediate_size, hidden_size, False)
-
-        self.submodules = [self.post_attention_layernorm,
-                           self.gate_proj,
-                           self.up_proj,
+        self.submodules = [self.up_proj,
                            self.down_proj]
+        if self.has_norm:
+            self.submodules += [self.post_attention_layernorm]
+
+        if cfg.arch.mlp_gate:
+            self.gate_proj = ExLlamaV2Linear(model, key + cfg.arch.mlp_key_gate, cfg.hidden_size, cfg.intermediate_size, self.model.config.arch.mlp_bias, f_key = f_key, f_beg = f_a, f_end = f_b)
+            self.submodules += [self.gate_proj]
+        else:
+            self.gate_proj = None
+
+
+    def numel(self) -> int:
+
+        numel = self.up_proj.numel() + \
+                self.down_proj.numel()
+
+        if self.model.config.arch.mlp_gate:
+            numel += self.gate_proj.numel()
+
+        if self.post_attention_layernorm is not None:
+            numel += self.post_attention_layernorm.numel()
+
+        return numel
 
 
     def load(self):
 
-        self.post_attention_layernorm.load()
-        self.gate_proj.load()
-        self.up_proj.load()
-        self.down_proj.load()
+        cfg = self.model.config
+
+        if self.post_attention_layernorm is not None:
+            self.post_attention_layernorm.load()
 
-        if self.gate_proj.is_quant():
-            assert self.up_proj.is_quant() and self.down_proj.is_quant(), "Partially quantized MLP layer"
+        if cfg.checkpoint_fused_mlp:
+            w12 = self.load_weight(self.key + cfg.arch.fused_mlp_key_12)
+            w1 = nn.Parameter(w12[:cfg.intermediate_size, :].contiguous())
+            w2 = nn.Parameter(w12[cfg.intermediate_size:, :].contiguous())
+            w3 = self.load_weight(self.key + cfg.arch.fused_mlp_key_3)
+            self.gate_proj.load(w1)
+            self.up_proj.load(w2)
+            self.down_proj.load(w3)
+        else:
+            if self.gate_proj is not None: self.gate_proj.load()
+            self.up_proj.load()
+            self.down_proj.load()
+
+        if self.up_proj.is_quant():
+            assert self.gate_proj is None or self.gate_proj.is_quant()
+            assert self.up_proj.is_quant(), "Partially quantized MLP layer"
             device_tensors = self.model.get_device_tensors(self.device_idx)
             device_tensors.begin_scratch_alloc()
-            self.q_handle = ext_c.make_q_mlp(self.post_attention_layernorm.weight,
-                                             self.post_attention_layernorm.variance_epsilon,
-                                             self.gate_proj.q_handle,
+
+            if self.has_norm:
+                norm_weight = self.post_attention_layernorm.weight if self.post_attention_layernorm.weight is not None else none_tensor
+                norm_bias = self.post_attention_layernorm.bias if self.post_attention_layernorm.bias is not None else none_tensor
+                is_rms = isinstance(self.post_attention_layernorm, ExLlamaV2RMSNorm)
+                eps = self.post_attention_layernorm.variance_epsilon
+            else:
+                norm_weight = none_tensor
+                norm_bias = none_tensor
+                is_rms = False
+                eps = 0
+
+            self.q_handle = ext_c.make_q_mlp(norm_weight,
+                                             norm_bias,
+                                             is_rms,
+                                             eps,
+                                             0 if self.gate_proj is None else self.gate_proj.q_handle,
                                              self.up_proj.q_handle,
                                              self.down_proj.q_handle,
                                              device_tensors.get_scratch_slice(self.temp_state_size()),
                                              device_tensors.get_scratch_slice(self.temp_a_size()),
                                              device_tensors.get_scratch_slice(self.temp_b_size()),
                                              device_tensors.get_scratch_slice(self.temp_dq_size()),
-                                             self.model.config.max_input_len * self.model.config.max_batch_size)
+                                             cfg.max_input_len * cfg.max_batch_size,
+                                             cfg.arch.mlp_act_func == "gelu",
+                                             self.has_residual)
 
 
     def unload(self):
+
         if self.q_handle is not None:
             ext_c.free_q_mlp(self.q_handle)
             self.q_handle = None
 
-        self.post_attention_layernorm.unload()
-        self.gate_proj.unload()
+        if self.post_attention_layernorm is not None: self.post_attention_layernorm.unload()
+        if self.gate_proj is not None: self.gate_proj.unload()
         self.up_proj.unload()
         self.down_proj.unload()
 
 
-    def weight_footprint(self):
+    def weight_footprint(self) -> int:
+
+        if self.model.config.checkpoint_fused_mlp:
+            fp = 3 * self.model.config.intermediate_size * self.model.config.hidden_size * 2
+        else:
+            fp = self.up_proj.weight_footprint() + \
+                 self.down_proj.weight_footprint()
+            if self.gate_proj is not None:
+                fp += self.gate_proj.weight_footprint()
+
+        if self.post_attention_layernorm is not None:
+            fp += self.post_attention_layernorm.weight_footprint()
 
-        return self.post_attention_layernorm.weight_footprint() + \
-               self.gate_proj.weight_footprint() + \
-               self.up_proj.weight_footprint() + \
-               self.down_proj.weight_footprint()
+        return fp
 
 
-    def scratch_space_fixed(self):
+    def scratch_space_fixed(self) -> int:
 
         return self.temp_state_size() + \
                self.temp_a_size() + \
                self.temp_b_size() + \
                self.temp_dq_size()
 
 
-    def scratch_space(self):
+    def scratch_space(self) -> int:
 
         assert self.model.config.intermediate_size >= self.model.config.hidden_size
         return self.temp_state_size() + \
                self.temp_a_size() + \
                self.temp_b_size() + \
                self.temp_dq_size()
 
 
-    def temp_state_size(self):
+    def temp_state_size(self) -> int:
 
         return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.hidden_size * 2 + 128
 
 
-    def temp_a_size(self):
+    def temp_a_size(self) -> int:
 
         return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.intermediate_size * 2 + 128
 
 
-    def temp_b_size(self):
+    def temp_b_size(self) -> int:
 
         return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.intermediate_size * 2 + 128
 
 
-    def temp_dq_size(self):
+    def temp_dq_size(self) -> int:
 
-        return max(self.gate_proj.temp_dq_size(),
+        return max(0 if self.gate_proj is None else self.gate_proj.temp_dq_size(),
                    self.up_proj.temp_dq_size(),
                    self.down_proj.temp_dq_size())
 
 
-    def set_device_idx(self, idx):
+    def set_device_idx(self, idx: int):
         super().set_device_idx(idx)
 
-        self.post_attention_layernorm.set_device_idx(idx)
-        self.gate_proj.set_device_idx(idx)
+        if self.post_attention_layernorm is not None:
+            self.post_attention_layernorm.set_device_idx(idx)
+        if self.gate_proj is not None: self.gate_proj.set_device_idx(idx)
         self.up_proj.set_device_idx(idx)
         self.down_proj.set_device_idx(idx)
 
-    def forward(self, hidden_states, cache = None, attn_mask = None, past_len = None, intermediates = False, loras = None):
+
+    def forward(self,
+                hidden_states: torch.Tensor,
+                cache = None,
+                attn_params = None,
+                past_len = None,
+                intermediates: bool = False,
+                loras: list[ExLlamaV2Lora] | None = None,
+                **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
         if self.q_handle is None or intermediates:
-            return self.forward_torch(hidden_states, cache, attn_mask, intermediates, loras = loras)
+            return self.forward_torch(hidden_states, cache, attn_params, past_len, intermediates, loras = loras, **kwargs)
 
         if loras is None or self.temp_lora_size == 0:
             pass_loras = []
-            pass_lora_temp = ext.none_tensor
+            pass_lora_temp = none_tensor
         else:
             pass_loras = [id(x) for x in loras]
             pass_lora_temp = torch.empty((self.temp_lora_size,), dtype = torch.half, device = hidden_states.device)
 
         ext_c.q_mlp_forward_(self.q_handle,
                              hidden_states.view(-1, hidden_states.shape[-1]),
                              pass_loras,
                              pass_lora_temp)
 
         return hidden_states
 
 
-    def forward_torch(self, hidden_states, cache = None, attn_mask = None, intermediates = False, loras = None):
+    def forward_torch(self,
+                      hidden_states: torch.Tensor,
+                      cache = None,
+                      attn_params = None,
+                      past_len = None,
+                      intermediates: bool = False,
+                      loras: list[ExLlamaV2Lora] | None = None,
+                      **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
         residual = hidden_states
-        post_norm = self.post_attention_layernorm.forward(hidden_states)
+        post_norm = self.post_attention_layernorm.forward(hidden_states) \
+            if self.has_norm else hidden_states
 
-        gate = self.gate_proj.forward(post_norm, loras = loras)
-        y = F.silu(gate)
-        up = self.up_proj.forward(post_norm, loras = loras)
-        y *= up
-        down = self.down_proj.forward(y, loras = loras)
+        if self.gate_proj is not None:
+            gate = self.gate_proj.forward(post_norm, loras = loras)
+            if self.model.config.arch.mlp_act_func == "silu":
+                y = F.silu(gate)
+            elif self.model.config.arch.mlp_act_func == "gelu":
+                y = F.gelu(gate)
+            up = self.up_proj.forward(post_norm, loras = loras)
+            y *= up
+            y.clamp_(min = -65504.0, max = 65504.0)
+        else:
+            up = self.up_proj.forward(post_norm, loras = loras)
+            if self.model.config.arch.mlp_act_func == "silu":
+                y = F.silu(up)
+            elif self.model.config.arch.mlp_act_func == "gelu":
+                y = F.gelu(up)
 
-        hidden_states = down + residual
+        down = self.down_proj.forward(y, loras = loras)
+        hidden_states = down + residual if self.has_residual else down
 
         if intermediates:
             return {"post_norm": post_norm,
-                    "gate": gate,
-                    "up": up,
                     "pre_down": y,
-                    "down": down,
                     "hidden_states": hidden_states}
         else:
             return hidden_states
 
 
     def update_loras(self):
 
         if self.q_handle is None: return
 
-        gate_proj_lora_a = { id(k): v for k, v in self.gate_proj.lora_a_tensors.items() }
-        gate_proj_lora_b = { id(k): v for k, v in self.gate_proj.lora_b_tensors.items() }
+        if self.gate_proj is None:
+            gate_proj_lora_a = {}
+            gate_proj_lora_b = {}
+        else:
+            gate_proj_lora_a = { id(k): v for k, v in self.gate_proj.lora_a_tensors.items() }
+            gate_proj_lora_b = { id(k): v for k, v in self.gate_proj.lora_b_tensors.items() }
+
         up_proj_lora_a = { id(k): v for k, v in self.up_proj.lora_a_tensors.items() }
         up_proj_lora_b = { id(k): v for k, v in self.up_proj.lora_b_tensors.items() }
         down_proj_lora_a = { id(k): v for k, v in self.down_proj.lora_a_tensors.items() }
         down_proj_lora_b = { id(k): v for k, v in self.down_proj.lora_b_tensors.items() }
 
         temp_lora_size = ext_c.q_mlp_set_loras(self.q_handle,
                                                gate_proj_lora_a,
@@ -195,7 +314,12 @@
         self.temp_lora_size = temp_lora_size * self.model.config.max_batch_size * self.model.config.max_input_len
 
 
     def is_quant(self):
         return self.q_handle is not None
 
 
+    def rank_reduce(self, k):
+
+        if self.gate_proj is not None: self.gate_proj.rank_reduce(k)
+        self.up_proj.rank_reduce(k)
+        self.down_proj.rank_reduce(k)
```

## exllamav2/model.py

```diff
@@ -1,74 +1,116 @@
+from __future__ import annotations
+import os, sys
+
+from exllamav2.architecture import RopeStyle
 
-import sys
 min_version = (3, 8)
 if sys.version_info < min_version:
     print("")
     print(f" ## Warning: this project requires Python {min_version[0]}.{min_version[1]} or higher.")
     print("")
 
 # Set CUDA context to lazy loading since we won't need 95% of the modules in Torch
+os.environ["CUDA_MODULE_LOADING"] = "LAZY"
 
-import os
-os.environ['CUDA_MODULE_LOADING']='LAZY'
+# # Set cudaMallocAsync allocator by default as it appears slightly more memory efficient, unless Torch is already
+# # imported in which case changing the allocator would cause it to crash
+# if not "PYTORCH_CUDA_ALLOC_CONF" in os.environ:
+#     try:
+#         x = torch.__version__
+#     except NameError:
+#         os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "backend:cudaMallocAsync"
 
 import torch
+
+if not (torch.version.cuda or torch.version.hip):
+    print("")
+    print(f" ## Warning: The installed version of PyTorch is {torch.__version__} and does not support CUDA or ROCm.")
+    print("")
+
 import math
 from exllamav2.config import ExLlamaV2Config
 from exllamav2.cache import ExLlamaV2CacheBase
 from exllamav2.linear import ExLlamaV2Linear
 from exllamav2.module import ExLlamaV2Module
 from exllamav2.rmsnorm import ExLlamaV2RMSNorm
-from exllamav2.attn import ExLlamaV2Attention
+from exllamav2.layernorm import ExLlamaV2LayerNorm
+from exllamav2.attn import ExLlamaV2Attention, has_flash_attn
 from exllamav2.lora import ExLlamaV2Lora
 from exllamav2.mlp import ExLlamaV2MLP
+from exllamav2.moe_mlp import ExLlamaV2MoEMLP
+from exllamav2.parallel_decoder import ExLlamaV2ParallelDecoder
 from exllamav2.embedding import ExLlamaV2Embedding
-# from exllamav2.util import list_live_tensors, print_vram_usage, set_snapshot, diff_snapshot, print_vram_usage_peak
+from exllamav2.pos_embedding import ExLlamaV2PosEmbedding
 from exllamav2.compat import safe_move_tensor
+from exllamav2.fasttensors import cleanup_stfiles
 import gc
+import threading
+from typing import Callable
+# from exllamav2.util import list_live_tensors, print_vram_usage, set_snapshot, diff_snapshot, print_vram_usage_peak
+from exllamav2.util import get_basic_progress
+
 
 def _torch_device(idx):
     if idx == -1: return "cpu"
     return f"cuda:{idx}"
 
 
 class ExLlamaV2DeviceTensors:
 
-    model = None
+    model: ExLlamaV2
     device_idx: int
     ready: bool
 
     scratch_bytes: int
     scratch_idx: int
 
-    sin: torch.tensor
-    cos: torch.tensor
+    sin: torch.Tensor | None
+    cos: torch.Tensor | None
 
-    scratch: torch.tensor = None
+    scratch: torch.Tensor | None
 
 
-    def __init__(self, model, device_idx, scratch_bytes):
+    def __init__(self,
+                 model: ExLlamaV2,
+                 device_idx: int,
+                 scratch_bytes: int):
 
         self.model = model
         self.device_idx = device_idx
         self.ready = False
+        self.scratch = None
         self.scratch_bytes = scratch_bytes
         self.scratch_idx = 0
 
 
     def prepare(self, scratch):
 
         self.prepare_sincos()
 
         if scratch:
             self.scratch = torch.empty((self.scratch_bytes // 2,), dtype = torch.half, device = _torch_device(self.device_idx))
 
         self.ready = True
 
 
+    def drop(self):
+
+        self.scratch = None
+        self.sin = None
+        self.cos = None
+        self.ready = False
+
+
+    def free(self):
+
+        self.drop()
+        self.scratch_bytes = 1
+
+
     def begin_scratch_alloc(self):
 
         self.scratch_idx = 0
 
 
     def get_scratch_slice(self, size_bytes):
 
@@ -79,99 +121,167 @@
         scratch_slice = self.scratch.narrow(0, self.scratch_idx, size_half)
         self.scratch_idx += size_half
         return scratch_slice
 
 
     def prepare_sincos(self):
 
-        base = self.model.config.rotary_embedding_base
-        alpha = self.model.config.scale_alpha_value
-        scale = self.model.config.scale_pos_emb
-        head_dim = self.model.config.head_dim
         device = _torch_device(self.device_idx)
 
-        if alpha != 1.0: base *= alpha ** (self.model.config.head_dim / (self.model.config.head_dim - 2))
+        cfg = self.model.config
+        if cfg.arch.rope_style == RopeStyle.NONE:
+            self.sin = torch.zeros((1,), device = device, dtype = torch.half)
+            self.cos = self.sin
+            return
+
+        base = cfg.rotary_embedding_base
+        alpha = cfg.scale_alpha_value or 1.0
+        scale = cfg.scale_pos_emb or 1.0
+        head_dim = cfg.head_dim
+        scaling_factor = 1.0
+
+        # Alpha scaling for any rope_scaling type
+
+        if alpha != 1.0: base *= alpha ** (cfg.head_dim / (cfg.head_dim - 2))
+
+        # "su"
+
+        if cfg.alt_rope_method == "su":
+
+            a = cfg.max_seq_len
+            b = cfg.original_max_seq_len
+            if a > b:
+                ext_factors = torch.tensor(cfg.scale_long_factor, dtype = torch.float32, device = device)
+                scaling_factor = math.sqrt(1 + math.log(a / b) / math.log(b))
+            else:
+                ext_factors = torch.tensor(cfg.scale_short_factor, dtype = torch.float32, device = device)
+
+            inv_freq = 1.0 / (ext_factors * base ** (torch.arange(0, head_dim, 2, device = device).float() / head_dim))
+
+        # Regular
 
-        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2, device = device).float() / head_dim))
-        t = torch.arange(self.model.config.max_seq_len, device = device, dtype = torch.float32)
+        else:
+
+            inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2, device = device).float() / head_dim))
+
+        # Common
 
+        t = torch.arange(cfg.max_seq_len, device = device, dtype = torch.float32)
         if scale != 1.0: t /= scale
 
         freqs = torch.einsum("i,j->ij", t, inv_freq)
-        emb = torch.cat((freqs, freqs), dim=-1)
+        if cfg.arch.rope_style == RopeStyle.NEOX:
+            emb = torch.cat((freqs, freqs), dim=-1)
+        elif cfg.arch.rope_style == RopeStyle.GPTJ:
+            emb = torch.repeat_interleave(freqs, 2, dim=-1)
+        else:
+            raise ValueError()
 
-        self.sin = emb.sin()[None, None, :, :].half()
-        self.cos = emb.cos()[None, None, :, :].half()
+        self.sin = emb.sin()[None, None, :, :]
+        self.cos = emb.cos()[None, None, :, :]
+        if scaling_factor != 1.0:
+            self.sin *= scaling_factor
+            self.cos *= scaling_factor
+        self.sin = self.sin.half()
+        self.cos = self.cos.half()
 
 
 class ExLlamaV2:
 
     config: ExLlamaV2Config
-    modules: list = []
-    modules_dict: dict = {}
-    device_tensors: list = []
-    cache_map: dict
+    modules: list[ExLlamaV2Module]
+    modules_dict: dict[str: ExLlamaV2Module]
+    device_tensors: list[ExLlamaV2DeviceTensors]
+    cache_map: dict[int: str]
     last_kv_layer_idx: int
     head_layer_idx: int
     loaded: bool
 
-
     def __init__(self, config: ExLlamaV2Config, lazy_load = False):
 
         self.config = config
         self.modules = []
         self.modules_dict = {}
         self.device_tensors = []
         self.cache_map = {}
         self.loaded = False
 
         # Build model
 
-        self.modules.append(ExLlamaV2Embedding(self, "model.embed_tokens"))
-        self.modules_dict[self.modules[-1].key] = self.modules[-1]
+        emb = ExLlamaV2Embedding(self, "model.embed_tokens")
+        self.modules += [emb]
 
-        for layer_idx in range(self.config.num_hidden_layers):
+        if self.config.arch.learned_pos_emb_key:
+            pos_emb = ExLlamaV2PosEmbedding(self, self.config.arch.learned_pos_emb_key)
+            self.modules += [pos_emb]
 
-            self.modules.append(ExLlamaV2Attention(self, f"model.layers.{layer_idx}", layer_idx))
-            for m in self.modules[-1].submodules: self.modules_dict[m.key] = m
-            self.modules.append(ExLlamaV2MLP(self, f"model.layers.{layer_idx}", layer_idx))
-            for m in self.modules[-1].submodules: self.modules_dict[m.key] = m
+        for layer_idx in range(self.config.num_hidden_layers):
 
-        self.modules.append(ExLlamaV2RMSNorm(self, "model.norm"))
-        self.modules_dict[self.modules[-1].key] = self.modules[-1]
+            layer_key = f"model.layers.{layer_idx}"
+            if self.config.arch.parallel_decoder_blocks:
+                pd = ExLlamaV2ParallelDecoder(self, layer_key, layer_idx)
+                self.modules += [pd]
+            else:
+                attn = ExLlamaV2Attention(self, layer_key, layer_idx)
+                if self.config.arch.is_moe: mlp = ExLlamaV2MoEMLP(self, layer_key, layer_idx)
+                else: mlp = ExLlamaV2MLP(self, layer_key, layer_idx)
+                self.modules += [attn, mlp]
+
+        if self.config.arch.norm == "layernorm": norm = ExLlamaV2LayerNorm(self, "model.norm")
+        elif self.config.arch.norm == "rmsnorm": norm = ExLlamaV2RMSNorm(self, "model.norm")
+        else: raise ValueError("unknown norm type")
+        self.modules += [norm]
 
         self.head_layer_idx = len(self.modules)
-        self.modules.append(ExLlamaV2Linear(self, "lm_head", self.config.hidden_size, self.config.vocab_size, False))
-        self.modules_dict[self.modules[-1].key] = self.modules[-1]
+        head = ExLlamaV2Linear(self, "lm_head",
+                               self.config.hidden_size,
+                               self.config.vocab_size,
+                               False,
+                               max_out_len = self.config.max_output_len,
+                               prescale = self.config.logit_scale,
+                               is_sub_module = False)
+        if self.config.arch.lm_head_key != "lm_head":
+            head.alt_key = self.config.arch.lm_head_key
+        self.modules += [head]
+
+        # Compile dictionary of modules
+
+        for module in self.modules:
+            if len(module.submodules) > 0:
+                for m in module.submodules: self.modules_dict[m.key] = m
+            else:
+                self.modules_dict[module.key] = module
 
         # Find last layer that affects k/v cache
 
         layer_idx = len(self.modules)
         while True:
             layer_idx -= 1
-            if isinstance(self.modules[layer_idx], ExLlamaV2Attention):
+            if isinstance(self.modules[layer_idx], ExLlamaV2Attention) or \
+               isinstance(self.modules[layer_idx], ExLlamaV2ParallelDecoder):
                 break
 
         self.last_kv_layer_idx = layer_idx
 
 
-    def set_device_map(self, allocation, embed_cpu = True):
+    def set_device_map(self,
+                       allocation: list[float],
+                       embed_cpu: bool = True) -> list[float]:
 
         self.cache_map = {}
 
         # Constant shared between layers
 
         sincos_size = self.config.head_dim * self.config.max_seq_len * 2
         constant_size = sincos_size * 2
 
         # Max size of hidden state
-        # TODO: Option to reserve space for cache while loading model
 
         state_size = self.config.hidden_size * self.config.max_input_len * self.config.max_batch_size * 2
-        mask_size = self.config.max_input_len ** 2 * 2
+        mask_size = self.config.max_input_len ** 2 * self.config.max_batch_size * 2
 
         # Bytes remaining per device
 
         allocation_bytes =  [a * 1024**3 - (constant_size + state_size + mask_size) for a in allocation]
 
         # Scratch space required per device
 
@@ -230,21 +340,49 @@
         self.set_cache_map()
 
         # Return unused space, in GB
 
         return [(ab - rb - rba) / 1024**3 for (ab, rb, rba) in zip(allocation_bytes, reserve_bytes, reserve_bytes_attn)]
 
 
-    def load(self, gpu_split = None, lazy = False, stats = False, callback = None, callback_gen = None):
+    def load(
+        self,
+        gpu_split: list[float] | None = None,
+        lazy: bool = False,
+        stats: bool = False,
+        callback: Callable[[int, int], None] | None = None,
+        callback_gen: Callable[[int, int], None] | None = None,
+        progress: bool = False
+    ):
+
+        if progress:
+            progressbar = get_basic_progress()
+            progressbar.start()
+            task_id = progressbar.add_task("Loading: " + self.config.model_dir, total = len(self.modules))
+            module = 0
+            def callback_pb(a, b):
+                progressbar.update(task_id, advance = 1)
+            assert callback is None, \
+                "Cannot use callback function and console progress bar at the same time."
+            callback = callback_pb
         f = self.load_gen(gpu_split, lazy, stats, callback, callback_gen)
-        for item in f: return item
+        for item in f:
+            pass
+        if progress:
+            progressbar.stop()
 
-    def load_gen(self, gpu_split = None, lazy = False, stats = False, callback = None, callback_gen = None):
 
-        assert not self.config.qkv_embed or not lazy, "Lazy initialization is unsupported when config.qkv_embed = True"
+    def load_gen(
+        self,
+        gpu_split: list[float] | None = None,
+        lazy: bool = False,
+        stats: bool = False,
+        callback: Callable[[int, int], None] | None = None,
+        callback_gen: Callable[[int, int], None] | None = None
+    ):
 
         with torch.inference_mode():
 
             stats_ = self.set_device_map(gpu_split or [99999])
 
             # Load module weights
 
@@ -261,56 +399,86 @@
                 if callback_gen is not None: yield from callback_gen(len(self.modules), len(self.modules))
 
             # Cache map
 
             self.set_cache_map()
 
             self.loaded = True
-            if stats: yield gpu_split, stats_
-            else: yield gpu_split
+            cleanup_stfiles()
 
+            # if stats: yield gpu_split, stats_
+            # else: yield gpu_split
 
-    def load_autosplit(self, cache, reserve_vram = None, last_id_only = False, callback = None, callback_gen = None):
-        f = self.load_autosplit_gen(cache, reserve_vram, last_id_only, callback, callback_gen)
-        for item in f: x = item
 
-    def load_autosplit_gen(self, cache, reserve_vram = None, last_id_only = False, callback = None, callback_gen = None):
+    def load_autosplit(
+        self,
+        cache: ExLlamaV2CacheBase,
+        reserve_vram: int | None = None,
+        last_id_only: bool = False,
+        callback: Callable[[int, int], None] | None = None,
+        callback_gen: Callable[[int, int], None] | None = None,
+        progress: bool = False
+    ):
+
+        if progress:
+            progressbar = get_basic_progress()
+            progressbar.start()
+            task_id = progressbar.add_task("Loading: " + self.config.model_dir, total = len(self.modules))
+            module = 0
+            def callback_pb(a, b):
+                progressbar.update(task_id, advance = 1)
+            assert callback is None, \
+                "Cannot use callback function and console progress bar at the same time."
+            callback = callback_pb
+        f = self.load_autosplit_gen(cache, reserve_vram, last_id_only, callback, callback_gen)
+        for item in f:
+            pass
+        if progress:
+            progressbar.stop()
+
+    def load_autosplit_gen(
+        self,
+        cache: ExLlamaV2CacheBase,
+        reserve_vram: int | None = None,
+        last_id_only: bool = False,
+        callback: Callable[[int, int], None] | None = None,
+        callback_gen: Callable[[int, int], None] | None = None
+    ):
 
-        assert not self.config.qkv_embed, "Auto GPU split is unsupported when config.qkv_embed = True"
+        # Limit model's max_input_len to max_seq_len if necessary
+        self.config.max_input_len = min(self.config.max_input_len, self.config.max_seq_len)
 
-        minimum_reserve_vram = 32 * 1024**2
+        minimum_reserve_vram = 256 * 1024**2
         last_touched_device = -1
         current_device = 0
         num_devices = torch.torch.cuda.device_count()
-        loras = None  # TODO:
+        loras = None  # TODO: Autosplit load with LoRAs
 
         with torch.inference_mode():
 
             self.device_tensors = []
 
             # Reserved space
 
             if reserve_vram is None:
-                reserve_vram = [32 * 1024**2] + [0] * (num_devices - 1)
+                reserve_vram = [192 * 1024**2] + [64 * 1024**2] * (num_devices - 1)
 
             reserved_vram_tensors = []
             minimum_reserve_tensor = None
 
             # Largest hidden state to ever forward through model
 
             hidden_state = torch.zeros((1, self.config.max_input_len), dtype = torch.long)
             batch_size, seq_len = hidden_state.shape
             past_len = 0
-            attn_mask = None
+            attn_params = ExLlamaV2Attention.Params(batch_size, seq_len, past_len, None, None)
 
             # Size of fixed scratch space
 
-            scratch_fixed = 0
-            for module in self.modules:
-                scratch_fixed = max(scratch_fixed, module.scratch_space_fixed())
+            scratch_fixed = max(module.scratch_space_fixed() for module in self.modules)
 
             # Load modules and create cache tensors sequentially
 
             self.cache_map = {}
             for idx, module in enumerate(self.modules):
 
                 if callback is not None: callback(idx, len(self.modules))
@@ -323,56 +491,61 @@
                     module.set_device_idx(-1)
                     module.load()
                     hidden_state = module.forward(hidden_state)
                     continue
 
                 while True:
 
-                    # If we've reached a new device, allocate fixed tensors and attention mask
+                    # If we've reached a new device, allocate fixed tensors
 
                     if current_device > last_touched_device:
 
                         self.device_tensors.append(ExLlamaV2DeviceTensors(self, current_device, scratch_fixed))
-                        if attn_mask is not None:
-                            reserved_vram_tensors.append(attn_mask)
-                            attn_mask = safe_move_tensor(attn_mask, _torch_device(current_device))
-                        else:
-                            attn_mask = self.build_attn_mask(batch_size, seq_len, past_len, None, _torch_device(current_device))
+                        # if attn_mask is not None:
+                        #     reserved_vram_tensors.append(attn_mask)
+                        #     attn_mask = safe_move_tensor(attn_mask, _torch_device(current_device))
+                        # else:
+                        #     attn_mask = self.build_attn_mask(batch_size, seq_len, past_len, None, _torch_device(current_device))
 
                         b = reserve_vram[current_device]
                         reserved_vram_tensors.append(torch.empty((b,), dtype = torch.int8, device = _torch_device(current_device)))
                         minimum_reserve_tensor = torch.empty((minimum_reserve_vram,), dtype = torch.int8, device = _torch_device(current_device))
 
                         last_touched_device = current_device
 
                     # Attempt to load module and forward state
 
                     module.set_device_idx(current_device)
 
                     hidden_state_backup = safe_move_tensor(hidden_state, "cpu").clone()
 
                     try:
-                        if isinstance(module, ExLlamaV2Attention):
+                        if isinstance(module, ExLlamaV2Attention) or \
+                           isinstance(module, ExLlamaV2ParallelDecoder):
                             self.cache_map[module.layer_idx] = module.device()
                             cache.update_cache_tensors()
 
                         module.load()
 
                         if idx == self.head_layer_idx:
                             if last_id_only:
                                 hidden_state = hidden_state.narrow(-2, -1, 1)
+                            elif self.config.max_output_len is not None:
+                                hidden_state = hidden_state.narrow(-2, -self.config.max_output_len, self.config.max_output_len)
 
                         hidden_state = safe_move_tensor(hidden_state, _torch_device(current_device))
-                        hidden_state = module.forward(hidden_state, cache = cache, attn_mask = attn_mask, past_len = past_len, loras = loras)
+                        hidden_state = module.forward(hidden_state, cache = cache, attn_params = attn_params, past_len = past_len, loras = loras)
                         fail = False
 
                     except Exception as e:
 
                         test = 0
-                        if ("CUDA out of memory" in str(e)) or ("HIP out of memory" in str(e)):
+                        if e.__class__.__name__ == "OutOfMemoryError" or \
+                            "CUDA out of memory" in str(e) or \
+                            "HIP out of memory" in str(e):
                             fail = True  # Exception object will hold references to tensors so we can't free them here
                         else:
                             raise
 
                     # If we failed, roll back and advance to next device
 
                     if fail:
@@ -395,20 +568,21 @@
 
                     break
 
             if callback is not None: callback(len(self.modules), len(self.modules))
             if callback_gen is not None: yield from callback_gen(len(self.modules), len(self.modules))
 
             hidden_state = None
-            attn_mask = None
+            attn_params = None
             reserved_vram_tensors = None
 
             gc.collect()
             torch.cuda.empty_cache()
             self.loaded = True
+            cleanup_stfiles()
 
         if 'yield' in locals():
             yield
 
 
     def unload(self):
 
@@ -419,129 +593,156 @@
         self.modules_dict = {}
         self.device_tensors = []
 
 
     def set_cache_map(self):
 
         for module in self.modules:
-            if isinstance(module, ExLlamaV2Attention): self.cache_map[module.layer_idx] = module.device()
+            if isinstance(module, ExLlamaV2Attention) or \
+               isinstance(module, ExLlamaV2ParallelDecoder):
+                self.cache_map[module.layer_idx] = module.device()
 
 
-    def get_cache_devices(self):
+    def get_cache_devices(self) -> list[str]:
 
         return list(set(self.cache_map.values()))
 
 
     def create_device_tensors(self, scratch_bytes):
 
-        for idx, bytes in enumerate(scratch_bytes):
+        for idx, b in enumerate(scratch_bytes):
 
-            tensors = ExLlamaV2DeviceTensors(self, idx, bytes)
+            tensors = ExLlamaV2DeviceTensors(self, idx, b)
             self.device_tensors.append(tensors)
 
 
+    def drop_device_tensors(self):
+
+        for dt in self.device_tensors:
+            dt.drop()
+
+
+    def free_device_tensors(self):
+
+        for dt in self.device_tensors:
+            dt.free()
+
+
     def get_device_tensors(self, device_idx, scratch = True):
 
         tensors = self.device_tensors[device_idx]
         if not tensors.ready: tensors.prepare(scratch)
         return tensors
 
 
-    def get_modules(self):
+    def get_modules(self) -> list[ExLlamaV2Module]:
 
-        return [module for module in self.modules]
+        return [module for module in self.modules]  #?
 
 
     def update_loras(self):
 
         for module in self.modules:
             if isinstance(module, ExLlamaV2Attention): module.update_loras()
             if isinstance(module, ExLlamaV2MLP): module.update_loras()
+            if isinstance(module, ExLlamaV2MoEMLP): module.update_loras()
 
 
-    def is_quant(self):
+    def is_quant(self) -> bool:
 
         for module in self.modules:
             if isinstance(module, ExLlamaV2Attention):
                 if module.is_quant(): return True
 
         return False
 
 
-    def build_attn_mask(self, batch_size, seq_len, past_len, input_mask, device):
-
-        if input_mask is None and seq_len == 1: return None
-
-        if isinstance(past_len, tuple):
-
-            attn_masks = []
-
-            for i in range(len(past_len[1])):
-
-                attn_mask = torch.zeros((1, 1, seq_len, past_len[1][i] + seq_len), dtype = torch.float16, device = device)
-                attn_mask_triu = torch.triu(torch.full((seq_len - 1, seq_len - 1), -65504.))
-                attn_mask[:, :, : seq_len - 1, past_len[1][i] + 1: past_len[1][i] + seq_len] = attn_mask_triu
-
-                if input_mask is not None:
-                    min_mask_width = min(input_mask[i].shape[-1], seq_len + past_len[1][i])
-                    input_mask_part = safe_move_tensor(input_mask[i][:, :min_mask_width], attn_mask.device)
-                    input_mask_part = input_mask_part.unsqueeze(1).unsqueeze(2)
-                    attn_mask[:, :, :, :min_mask_width] = torch.minimum(attn_mask[:, :, :, :min_mask_width], input_mask_part)
-
-                attn_masks.append(attn_mask)
-
-            return attn_masks
-
-        else:
-
-            attn_mask = torch.zeros((batch_size, 1, seq_len, past_len + seq_len), dtype = torch.float16, device = device)
-            attn_mask_triu = torch.triu(torch.full((seq_len - 1, seq_len - 1), -65504.))
-            attn_mask[:, :, : seq_len - 1, past_len + 1: past_len + seq_len] = attn_mask_triu
-
-            if input_mask is not None:
-                min_mask_width = min(input_mask.shape[-1], seq_len + past_len)
-                input_mask_part = safe_move_tensor(input_mask[:, :min_mask_width], attn_mask.device)
-                input_mask_part = input_mask_part.unsqueeze(1).unsqueeze(2)
-                attn_mask[:, :, :, :min_mask_width] = torch.minimum(attn_mask[:, :, :, :min_mask_width], input_mask_part)
-
-            return attn_mask
-
     @torch.inference_mode()
     def forward(self,
-                input_ids,
-                cache = None,
-                input_mask = None,
-                preprocess_only = False,
-                last_id_only = False,
-                loras = None,
-                return_last_state = False):
+                input_ids: torch.Tensor,
+                cache: ExLlamaV2CacheBase | list[ExLlamaV2CacheBase] | None = None,
+                input_mask: torch.Tensor | None = None,
+                preprocess_only: bool = False,
+                last_id_only: bool = False,
+                loras: list[ExLlamaV2Lora] | None = None,
+                return_last_state: bool = False,
+                position_offsets: torch.Tensor | None = None,
+                abort_event: threading.Event | None = None,
+                **kwargs) \
+        -> torch.Tensor | tuple[torch.Tensor, torch.Tensor] | None:
+        """
+        Runs a forward pass through the model. If a cache is used, also appends keys/values to the cache
+        and advances it.
+
+        :param input_ids:
+            LongTensor of input token IDs, shape (batch_size, q_len)
+
+        :param cache:
+            Optional ExLlamaV2Cache. If not provided, q_len must be less than config.max_input_len
+
+        :param input_mask:
+            Additive attention bias, shape (batch_size, past_len + q_len, q_len)
+
+        :param preprocess_only:
+            Only forward up to the last layer that affects the K/V cache. Does not return logits. Used
+            to prefill the cache.
+
+        :param last_id_only:
+            Process the entire input sequence but only pass the last token through the head layer and
+            only return logits for the last token.
+
+        :param loras:
+            List of ExLlamaV2Lora objects to apply during the forward pass
+
+        :param return_last_state:
+            Also return the hidden state right before the head layer
+
+        :param position_offsets:
+            Tensor of position offsets, shape (batch_size, 1). Offset is applied to position IDs during
+            RoPE application.
+
+        :param abort_event:
+            Optional event that, if set, will abort the forward pass. Function will return None if aborted.
+
+        :return:
+            FP16 logits tensor, shape (batch_size, q_len, vocab_size)
+            (optional) state tensor, shape (batch_size, q_len, hidden_size)
+
+        :indexed_embeddings:
+            Tensor of embeddings, shape (batch_size, q_len, hidden_size), indexed by input token IDs >=
+            ExLlamaV2.EMBEDDING_INDEX
+        """
 
-        q_len = input_ids.shape[-1]
+        bsz, q_len = input_ids.shape
         remaining_q_len = q_len
-        bsz = input_ids.shape[0]
 
         # Attn and MLP layers have preallocated buffers for temp states, sized by the model config. Effective max input
         # length depends on the current batch size
 
         effective_max_input_len = self.config.max_input_len * self.config.max_batch_size // bsz
 
         # Without a cache we can't process the sequence in chunks, so forward the whole thing and assume the input length
         # is less than config.max_input_len
 
         if cache is None or not isinstance(cache, ExLlamaV2CacheBase):
 
             assert q_len <= effective_max_input_len, "Maximum input length exceeded in model.forward"
 
-            result, last_state = self._forward(input_ids = input_ids,
-                                               cache = cache,
-                                               input_mask = input_mask,
-                                               preprocess_only = preprocess_only,
-                                               last_id_only = last_id_only,
-                                               loras = loras,
-                                               return_last_state = return_last_state)
+            result, last_state = self.forward_chunk(input_ids = input_ids,
+                                                    cache = cache,
+                                                    input_mask = input_mask,
+                                                    preprocess_only = preprocess_only,
+                                                    last_id_only = last_id_only,
+                                                    loras = loras,
+                                                    return_last_state = return_last_state,
+                                                    position_offsets = position_offsets,
+                                                    abort_event = abort_event,
+                                                    **kwargs)
+
+            if abort_event and abort_event.is_set(): return
 
             if last_state is None:
                 return result
             else:
                 return result, last_state
 
         # Confirm that the input fits within the allocated cache space
@@ -559,37 +760,50 @@
 
             # Limit chunk_size to max_input_len
 
             chunk_size = min(remaining_q_len, effective_max_input_len)
 
             # Limit chunk_size to keep size of attention operation <= max_attention_size
 
-            past_len = cache.current_seq_len
-            attn_size = (past_len + remaining_q_len) * remaining_q_len
-            max_a = self.config.max_attention_size
-            if attn_size > max_a:
-                cs = (math.sqrt(past_len ** 2 + 4 * max_a) - past_len) / 2
-                chunk_size = min(chunk_size, math.floor(cs))
+            if has_flash_attn and not self.config.no_flash_attn:
+
+                # Can't measure increase in VRAM usage with longer k_len, assume usage is constant
+                # for given chunk_size
+                pass
+
+            else:
+
+                past_len = cache.current_seq_len
+                attn_size = (past_len + remaining_q_len) * remaining_q_len
+                max_a = self.config.max_attention_size
+                if attn_size > max_a:
+                    cs = (math.sqrt(past_len ** 2 + 4 * max_a) - past_len) / 2
+                    chunk_size = min(chunk_size, math.floor(cs))
 
             # Process chunk
 
             chunk_end = min(chunk_begin + chunk_size, q_len)
 
             # print(f"Forward chunk length: {chunk_end - chunk_begin}")
 
             _last_id_only = last_id_only
             _preprocess_only = preprocess_only or (chunk_end < q_len and last_id_only)
 
-            r, ls = self._forward(input_ids = input_ids[:, chunk_begin : chunk_end],
-                                  cache = cache,
-                                  input_mask = input_mask,
-                                  preprocess_only = _preprocess_only,
-                                  last_id_only = _last_id_only,
-                                  loras = loras,
-                                  return_last_state = return_last_state and remaining_q_len <= chunk_size)
+            r, ls = self.forward_chunk(input_ids = input_ids[:, chunk_begin : chunk_end],
+                                       cache = cache,
+                                       input_mask = input_mask,
+                                       preprocess_only = _preprocess_only,
+                                       last_id_only = _last_id_only,
+                                       loras = loras,
+                                       return_last_state = return_last_state and remaining_q_len <= chunk_size,
+                                       position_offsets = position_offsets,
+                                       abort_event = abort_event,
+                                       **kwargs)
+
+            if abort_event and abort_event.is_set(): return
 
             if not _preprocess_only:
                 result = r if result is None else torch.cat((result, r), dim = 1)
                 r = None
 
             chunk_begin = chunk_end
             remaining_q_len -= chunk_size
@@ -598,81 +812,94 @@
         if last_state is None:
             return result
         else:
             return result, last_state
 
 
     @torch.inference_mode()
-    def _forward(self,
-                 input_ids,
-                 cache = None,
-                 input_mask = None,
-                 preprocess_only = False,
-                 last_id_only = False,
-                 loras = None,
-                 return_last_state = False):
+    def forward_chunk(self,
+                      input_ids: torch.Tensor,
+                      cache: ExLlamaV2CacheBase | list[ExLlamaV2CacheBase] | None = None,
+                      input_mask: torch.Tensor | None = None,
+                      preprocess_only: bool = False,
+                      last_id_only: bool = False,
+                      loras: list[ExLlamaV2Lora] | None = None,
+                      return_last_state: bool = False,
+                      position_offsets: torch.Tensor | None = None,
+                      abort_event: threading.Event | None = None,
+                      attn_params: ExLlamaV2Attention.Params | None = None,
+                      **kwargs) \
+        -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
 
         batch_size, seq_len = input_ids.shape
         past_len = 0
         if cache is not None:
             if isinstance(cache, ExLlamaV2CacheBase):
                 past_len = cache.current_seq_len
-            else:
-                pl = [c.current_seq_len for c in cache]
-                past_len = torch.tensor(pl, dtype = torch.int)
-                past_len = (past_len, past_len)
+            # else:
+            #     past_len = [c.current_seq_len for c in cache]
+
+        assert self.config.max_output_len is None or \
+            preprocess_only or \
+            last_id_only or \
+            seq_len <= self.config.max_output_len, \
+            "seq_len exceeds max_output_len"
 
         # assert cache is None or isinstance(cache, list) or batch_size <= cache.batch_size
 
         x = input_ids
-        prev_device = None
-        attn_mask = None
+
+        if not attn_params:
+            attn_params = ExLlamaV2Attention.Params(batch_size, seq_len, past_len, input_mask, position_offsets)
+        else:
+            if not isinstance(attn_params, ExLlamaV2Attention.PagedParams):
+                past_len = attn_params.past_len
+                cache.current_seq_len = past_len
         last_state = None
+        last_module = None
 
         for idx, module in enumerate(self.modules):
 
-            device = _torch_device(module.device_idx)
-
-            # Build attention mask
+            # Respect abort signal
 
-            if device != prev_device and device != "cpu":
-
-                prev_device = device
-                attn_mask = self.build_attn_mask(batch_size, seq_len, past_len, input_mask, device)
-                if isinstance(past_len, tuple): past_len = (safe_move_tensor(past_len[0], device), past_len[1])
+            if abort_event and abort_event.is_set(): return None, None
 
             # Onward
 
+            device = _torch_device(module.device_idx)
+
             if idx == self.head_layer_idx:
                 if last_id_only and return_last_state:
                     x = x.narrow(-2, -1, 1)
                     last_state = x
                 elif last_id_only:
                     x = x.narrow(-2, -1, 1)
                 elif return_last_state:
                     last_state = x.narrow(-2, -1, 1)
 
             x = safe_move_tensor(x, device)
-            x = module.forward(x, cache = cache, attn_mask = attn_mask, past_len = past_len, loras = loras)
+            x = module.forward(x, cache = cache, attn_params = attn_params, past_len = past_len, loras = loras, **kwargs)
 
             if preprocess_only and idx == self.last_kv_layer_idx:
                 x = None
                 break
 
-            # print(module.key, module.name, x[0, 0])
-            # print("max", torch.max(x).item(), "min",torch.min(x).item())
-
         # Advance cache
 
         if cache is not None:
             if isinstance(cache, list):
                 for c in cache: c.current_seq_len += seq_len
             else:
                 cache.current_seq_len += seq_len
 
+        # Apply logit scale
+
+        # if x is not None and self.config.logit_scale != 1:
+        #     x.mul_(self.config.logit_scale)
+
         # Set padding logits to -inf
 
         if x is not None:
             head_padding = self.modules[-1].padding
             if head_padding > 0:
                 x[:, :, -head_padding:] = -65504.
```

## exllamav2/model_init.py

```diff
@@ -1,99 +1,126 @@
 
-import argparse, sys, os, glob
+import argparse, sys, os, glob, time
 
 from exllamav2 import(
     ExLlamaV2,
     ExLlamaV2Config,
     ExLlamaV2Tokenizer
 )
 
 def add_args(parser):
 
     parser.add_argument("-m", "--model_dir", type = str, help = "Path to model directory")
     parser.add_argument("-gs", "--gpu_split", type = str, help = "\"auto\", or VRAM allocation per GPU in GB")
     parser.add_argument("-l", "--length", type = int, help = "Maximum sequence length")
-    parser.add_argument("-rs", "--rope_scale", type = float, default = 1.0, help = "RoPE scaling factor")
-    parser.add_argument("-ra", "--rope_alpha", type = float, default = 1.0, help = "RoPE alpha value (NTK)")
+    parser.add_argument("-rs", "--rope_scale", type = float, help = "RoPE scaling factor")
+    parser.add_argument("-ra", "--rope_alpha", type = float, help = "RoPE alpha value (NTK)")
     parser.add_argument("-nfa", "--no_flash_attn", action = "store_true", help = "Disable Flash Attention")
     parser.add_argument("-lm", "--low_mem", action = "store_true", help = "Enable VRAM optimizations, potentially trading off speed")
+    parser.add_argument("-ept", "--experts_per_token", type = int, help = "Override MoE model's default number of experts per token")
+    parser.add_argument("-lq4", "--load_q4", action = "store_true", help = "Load weights in Q4 mode")
+    if os.name != "nt":
+        parser.add_argument("-fst", "--fast_safetensors", action = "store_true", help = "Optimized safetensors loading with direct I/O (experimental!)")
 
 
 def print_options(args):
 
     print(f" -- Model: {args.model_dir}")
 
     print_opts = []
-    if args.gpu_split: print_opts += [f"gpu_split: {args.gpu_split}"]
-    if args.length: print_opts += [f"length: {args.length}"]
-    print_opts += [f"rope_scale {args.rope_scale}"]
-    print_opts += [f"rope_alpha {args.rope_alpha}"]
+    if args.gpu_split is not None: print_opts += [f"gpu_split: {args.gpu_split}"]
+    if args.length is not None: print_opts += [f"length: {args.length}"]
+    if args.rope_scale is not None: print_opts += [f"rope_scale: {args.rope_scale}"]
+    if args.rope_alpha is not None: print_opts += [f"rope_alpha: {args.rope_alpha}"]
     if args.no_flash_attn: print_opts += ["no_flash_attn"]
     if args.low_mem: print_opts += ["low_mem"]
+    if hasattr(args, "fast_safetensors") and args.fast_safetensors: print_opts += ["fast_safetensors"]
+    if args.experts_per_token is not None: print_opts += [f"experts_per_token: {args.experts_per_token}"]
+    if args.load_q4: print_opts += ["load_q4"]
     print(f" -- Options: {print_opts}")
 
 
 def check_args(args):
 
     if not args.model_dir:
         print(" ## Error: No model directory specified")
         sys.exit()
 
     if not os.path.exists(args.model_dir):
         print(f" ## Error: Can't find model directory: {args.model_dir}")
         sys.exit()
 
     required_files = ["config.json",
-                      "tokenizer.model",
+                      ["tokenizer.model", "tokenizer.json"],
                       "*.safetensors"]
 
     for filename in required_files:
-
-        path = os.path.join(args.model_dir, filename)
-        matches = glob.glob(path)
-        if len(matches) == 0:
+        if isinstance(filename, str):
+            filename = [filename]
+        all_matches = []
+        for file in filename:
+            path = os.path.join(args.model_dir, file)
+            matches = glob.glob(path)
+            all_matches += matches
+        if len(all_matches) == 0:
             print(f" ## Error: Cannot find {filename} in {args.model_dir}")
             sys.exit()
 
 
-def init(args, quiet = False, allow_auto_split = False):
+def init(args,
+         quiet: bool = False,
+         allow_auto_split: bool = False,
+         skip_load: bool = False,
+         benchmark: bool = False,
+         max_batch_size: int = None,
+         max_output_len: int = None):
 
     # Create config
 
     config = ExLlamaV2Config()
     config.model_dir = args.model_dir
+    config.fasttensors = hasattr(args, "fast_safetensors") and args.fast_safetensors
     config.prepare()
 
     # Set config options
 
     if args.length: config.max_seq_len = args.length
-    config.scale_pos_emb = args.rope_scale
-    config.scale_alpha_value = args.rope_alpha
+    if args.rope_scale: config.scale_pos_emb = args.rope_scale
+    if args.rope_alpha: config.scale_alpha_value = args.rope_alpha
     config.no_flash_attn = args.no_flash_attn
+    if args.experts_per_token: config.num_experts_per_token = args.experts_per_token
+
+    if max_batch_size: config.max_batch_size = max_batch_size
+    config.max_output_len = max_output_len
 
     # Set low-mem options
 
     if args.low_mem: config.set_low_mem()
+    if args.load_q4: config.load_in_q4 = True
 
     # Load model
     # If --gpu_split auto, return unloaded model. Model must be loaded with model.load_autosplit() supplying cache
     # created in lazy mode
 
     model = ExLlamaV2(config)
 
     split = None
     if args.gpu_split and args.gpu_split != "auto":
         split = [float(alloc) for alloc in args.gpu_split.split(",")]
 
-    if args.gpu_split != "auto":
+    if args.gpu_split != "auto" and not skip_load:
         if not quiet: print(" -- Loading model...")
+        t = time.time()
         model.load(split)
+        t = time.time() - t
+        if benchmark and not quiet:
+            print(f" -- Loaded model in {t:.4f} seconds")
     else:
         assert allow_auto_split, "Auto split not allowed."
 
     # Load tokenizer
 
     if not quiet: print(" -- Loading tokenizer...")
 
     tokenizer = ExLlamaV2Tokenizer(config)
 
-    return model, tokenizer
+    return model, tokenizer
```

## exllamav2/module.py

```diff
@@ -1,134 +1,219 @@
+from __future__ import annotations
 import torch
 import torch.nn as nn
 from exllamav2.config import ExLlamaV2Config
-from safetensors import safe_open
+from exllamav2.fasttensors import STFile
 
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.model import ExLlamaV2
+    from exllamav2.lora import ExLlamaV2Lora
 
-def _torch_device(idx):
+def _torch_device(idx: int) -> str:
     if idx == -1: return "cpu"
     return f"cuda:{idx}"
 
 
-def _tsize(st, key):
-
-    tslice = st.get_slice(key)
-    shape = tslice.get_shape()
-    numel = 1
-    for x in shape: numel *= x
-    dtype = tslice.get_dtype()
-    if dtype == "I32": return numel * 4
-    elif dtype == "I16": return numel * 2
-    elif dtype == "F16": return numel * 2
-    elif dtype == "BF16": return numel * 2
-    elif dtype == "F32": return numel * 4
-    else: raise ValueError(f"Unexpected datatype {dtype}: {key}")
-
-
 class ExLlamaV2Module:
 
-    model = None
     config: ExLlamaV2Config
     key: str
+    alt_key: str | None
     device_idx: int
     footprint: int
+    submodules: list[ExLlamaV2Module]
+    assumed_footprint: int
 
-    def __init__(self, model, key):
+    def __init__(self,
+                 model: ExLlamaV2,
+                 key: str):
 
         self.model = model
         self.key = key
+        self.alt_key = None
         self.footprint = -1
+        self.submodules = []
 
 
-    def device(self):
+    def numel(self): raise(NotImplementedError())
+    def load(self): raise(NotImplementedError())
+    def unload(self): raise(NotImplementedError())
+    def scratch_space_fixed(self): raise(NotImplementedError())
+    def scratch_space(self): raise(NotImplementedError())
+
+    def forward(self,
+                hidden_states,
+                cache = None,
+                attn_params = None,
+                past_len = None,
+                intermediates = None,
+                loras = None):
+        raise(NotImplementedError())
 
+
+    def device(self) -> str:
         return _torch_device(self.device_idx)
 
 
-    def load_multi(self, keys, measure = False):
+    def load_multi(self,
+                   key: str,
+                   keys: list[str],
+                   measure: bool = False) -> int | dict[str: torch.Tensor]:
 
         tensors = {}
         submap = {}
         submap_i = {}
         size = 0
 
+        # key = self.key if override_key is None else override_key
+
         for k in keys:
-            ck = self.key + "." + k
+            ck = key + "." + k
             if ck in self.model.config.tensor_file_map:
                 submap[k] = self.model.config.tensor_file_map[ck]
 
         for k, v in submap.items():
             if v not in submap_i:
                 submap_i[v] = []
             submap_i[v].append(k)
 
         for v, ks in submap_i.items():
-            with safe_open(v, framework="pt", device="cpu") as st:
-                for k in ks:
-                    if measure:
-                        size += _tsize(st, self.key + "." + k)
-                    else:
-                        tensors[k] = st.get_tensor(self.key + "." + k).to(self.device())
+            stfile = STFile.open(v, fast = self.model.config.fasttensors, keymap = self.model.config.arch.keymap)
+            for k in ks:
+                if measure:
+                    size += stfile.measure(key + "." + k)
+                else:
+                    tensors[k] = stfile.get_tensor(key + "." + k, device = self.device())
 
         return size if measure else tensors
 
 
-    def load_weight(self):
+    def load_weight(self,
+                    override_key: str | None = None):
+
+        if override_key is not None:
+            keys = [override_key]
+        else:
+            keys = [self.key]
+            if self.alt_key is not None:
+                keys += [self.alt_key]
 
-        # EXL2
+        for key in keys:
 
-        if self.key + ".q_weight" in self.model.config.tensor_file_map:
-            qtensors = self.load_multi(["q_weight", "q_invperm", "q_scale", "q_scale_max", "q_groups", "q_perm"])
-            qtensors["q_perm"] = torch.argsort(qtensors["q_invperm"]).to(torch.int)
-            return qtensors
+            # EXL2
+
+            if key + ".q_weight" in self.model.config.tensor_file_map:
+                qtensors = self.load_multi(key, ["q_weight", "q_invperm", "q_scale", "q_scale_max", "q_groups", "q_perm", "bias"])
+                qtensors["q_perm"] = torch.argsort(qtensors["q_invperm"]).to(torch.int)
+                return qtensors
 
-        # GPTQ
+            # GPTQ
 
-        if self.key + ".qweight" in self.model.config.tensor_file_map:
-            qtensors = self.load_multi(["qweight", "qzeros", "scales", "g_idx"])
-            return qtensors
+            if key + ".qweight" in self.model.config.tensor_file_map:
+                qtensors = self.load_multi(key, ["qweight", "qzeros", "scales", "g_idx", "bias"])
+                if "bias" in qtensors and torch.all(qtensors["bias"].eq(0)):
+                    del qtensors["bias"]
+                qtensors["scales"] = qtensors["scales"].half()
+                return qtensors
 
-        # Torch
+            # Torch
 
-        if self.key + ".weight" in self.model.config.tensor_file_map:
-            tensor = self.load_multi(["weight"])["weight"]
-            tensor = tensor.half()
-            return nn.Parameter(tensor)
+            if key + ".weight" in self.model.config.tensor_file_map:
+                if key + ".bias" in self.model.config.tensor_file_map:
+                    tensors = self.load_multi(key, ["weight", "bias"])
+                    tensor = tensors["weight"].half()
+                    bias = tensors["bias"].half()
+                    if self.model.config.arch.orig_weights_transposed and len(tensor.shape) == 2:
+                        tensor = tensor.T
+                    return nn.Parameter(tensor), nn.Parameter(bias)
+                else:
+                    tensors = self.load_multi(key, ["weight"])
+                    tensor = tensors["weight"].half()
+                    # if self.model.config.arch.orig_weights_transposed:
+                    #     tensor = tensor.T
+                    return nn.Parameter(tensor)
 
-        # No weights found for key
+            # No weights found for key
 
         return None
 
 
-    def weight_footprint(self):
+    def load_weight_fused(self,
+                          f_key: str,
+                          f_beg: int,
+                          f_end: int,
+                          in_feat: int,
+                          out_feat: int):
+
+        res = []
+        for key in [f_key, f_key + ".weight", f_key + ".bias"]:
+
+            filename = self.model.config.tensor_file_map.get(key)
+            if not filename: continue
+
+            stfile = STFile.open(filename, fast = self.model.config.fasttensors, keymap = self.model.config.arch.keymap)
+            # tensor = stfile.get_tensor(key, device = self.device()).half()
+            tensor = stfile.get_tensor(key, device = "cpu", cached = True, out_dtype = torch.half)
+            if self.model.config.arch.orig_weights_transposed and len(tensor.shape) == 2:
+                tensor = tensor.T
+            tensor = tensor[f_beg:f_end]
+            if not key.endswith(".bias"):
+                if in_feat != out_feat and \
+                    tensor.shape[1] == out_feat and \
+                    tensor.shape[0] == in_feat:
+                    tensor = tensor.T
+            tensor = tensor.contiguous().to(self.device())
+            res.append(nn.Parameter(tensor))
+
+        if len(res) == 2: return res[0], res[1]
+        if len(res) == 1: return res[0]
+        return None
+
+
+    def weight_footprint(self) -> int:
 
         if self.footprint == -1:
 
-            # EXL2
+            keys = [self.key]
+            if self.alt_key is not None:
+                keys += [self.alt_key]
 
-            if self.key + ".q_weight" in self.model.config.tensor_file_map:
-                self.footprint = self.load_multi(["q_weight", "q_invperm", "q_scale", "q_scale_max", "q_groups", "q_perm", "q_perm"], measure = True)
+            for key in keys:
 
-            # GPTQ
+                # EXL2
 
-            elif self.key + ".qweight" in self.model.config.tensor_file_map:
-                self.footprint = self.load_multi(["qweight", "qzeros", "scales", "g_idx"], measure = True)
+                if key + ".q_weight" in self.model.config.tensor_file_map:
+                    self.footprint = self.load_multi(key, ["q_weight", "q_invperm", "q_scale", "q_scale_max", "q_groups", "q_perm", "q_perm", "bias"], measure = True)
 
-            # Torch
+                # GPTQ
+
+                elif key + ".qweight" in self.model.config.tensor_file_map:
+                    self.footprint = self.load_multi(key, ["qweight", "qzeros", "scales", "g_idx", "bias"], measure = True)
 
-            elif self.key + ".weight" in self.model.config.tensor_file_map:
-                self.footprint = self.load_multi(["weight"], measure = True)
+                # Torch
+
+                elif key + ".weight" in self.model.config.tensor_file_map:
+                    self.footprint = self.load_multi(key, ["weight", "bias"], measure = True)
+
+                if self.footprint != -1: break
 
             # Error
 
-            else: raise ValueError("Unknown tensor type: " + self.key)
+            if self.footprint == -1:
+                # raise ValueError("Unknown tensor type: " + self.key)
+                return self.assumed_footprint
 
         return self.footprint
 
 
-    def set_device_idx(self, idx):
-
+    def set_device_idx(self, idx: int):
         self.device_idx = idx
 
 
-    def is_quant(self):
+    def is_quant(self) -> bool:
         return False
+
+
+    def reload(self):
+        self.unload()
+        self.load()
```

## exllamav2/rmsnorm.py

```diff
@@ -1,79 +1,128 @@
+from __future__ import annotations
 import torch
 from torch import nn
 from exllamav2.module import ExLlamaV2Module
-from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
+from exllamav2.ext import exllamav2_ext as ext_c
 
-class ExLlamaV2RMSNorm(ExLlamaV2Module):
+from typing import TYPE_CHECKING
+if TYPE_CHECKING:
+    from exllamav2.model import ExLlamaV2
 
-    weight: nn.Parameter or None = None
-    variance_epsilon: float
+class ExLlamaV2RMSNorm(ExLlamaV2Module):
 
     name: str = "RMSNorm"
 
+    weight: nn.Parameter | None
+    bias: nn.Parameter | None
+    variance_epsilon: float
+
 
     def __init__(self, model, key):
-        if model.config.architecture == "Yi":
-            key = key.replace(".input_layernorm", ".ln1")
-            key = key.replace(".post_attention_layernorm", ".ln2")
         super().__init__(model, key)
 
+        self.weight = None
+        self.bias = None
+        self.variance_epsilon = 1e-6
+
 
     def load(self):
 
+
         w = self.load_weight()
-        assert isinstance(w, nn.Parameter)
 
-        self.weight = w
-        self.variance_epsilon = self.model.config.rms_norm_eps
+        if isinstance(w, tuple):
+            self.weight = w[0]
+            self.bias = w[1]
+        else:
+            self.weight = w
+            self.bias = None
+
+        assert isinstance(self.weight, nn.Parameter)
+        assert self.bias is None, "RMSNorm does not support bias"
+        # or isinstance(self.bias, nn.Parameter)
+
+        self.variance_epsilon = self.model.config.norm_eps
+
+        # Gemma adds 1 to the norm tensor for some reason
+        if self.model.config.arch.norm_constant_bias != 0:
+            self.weight += self.model.config.arch.norm_constant_bias
 
 
     def unload(self):
 
         if self.weight is not None:
             del self.weight
             self.weight = None
 
+        if self.bias is not None:
+            del self.bias
+            self.bias = None
+
+
+    def numel(self):
+
+        return 0
+        # return self.weight.numel()
+
+
+    def get_weight(self) -> torch.Tensor:
 
-    def get_weight(self):
+        # Make sure to return the original weight tensor for Gemma
+        if self.model.config.arch.norm_constant_bias != 0:
+            return self.weight.data - self.model.config.arch.norm_constant_bias
 
         return self.weight.data
 
 
-    def weight_footprint(self):
+    def weight_footprint(self) -> int:
 
         hidden_size = self.model.config.hidden_size
         return hidden_size * 2
 
 
-    def scratch_space_fixed(self):
+    def scratch_space_fixed(self) -> int:
 
         return 0
 
 
-    def scratch_space(self):
+    def scratch_space(self) -> int:
 
         return 0
 
 
-    def forward(self, hidden_states, cache = None, attn_mask = None, past_len = None, intermediates = False, loras = None):
+    def forward(self,
+                hidden_states: torch.Tensor,
+                cache = None,
+                attn_params = None,
+                past_len = None,
+                intermediates: bool = False,
+                loras = None,
+                **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
         output_shape = hidden_states.shape
         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])
         norm = torch.empty_like(hidden_states)
         ext_c.rms_norm(hidden_states, self.weight, norm, self.variance_epsilon)
         hidden_states = norm.view(output_shape)
 
         if intermediates:
             return {"hidden_states": hidden_states}
         else:
             return hidden_states
 
 
-    def forward_torch(self, hidden_states, cache = None, attn_mask = None, past_len = None, intermediates = False):
+    def forward_torch(self,
+                      hidden_states: torch.Tensor,
+                      cache = None,
+                      attn_params = None,
+                      past_len = None,
+                      intermediates: bool = False,
+                      loras = None,
+                      **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
         hidden_states[hidden_states == -float('inf')] = -65504.0
         hidden_states[hidden_states == float('inf')] = 65504.0
 
         variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim = True)
         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
         hidden_states = hidden_states.to(self.weight.dtype)
```

## exllamav2/util.py

```diff
@@ -1,9 +1,129 @@
+from __future__ import annotations
+from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn
 import gc
 import torch
+import time
+
+
+class Timer:
+    def __enter__(self):
+        self.start_time = time.time()
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self.end_time = time.time()
+        self.interval = self.end_time - self.start_time
+
+
+class SeqTensor:
+
+    PAGE_SIZE = 256
+
+    tensor: torch.Tensor
+    seq_dim: int
+    seq_len: int
+    seq_cap: int
+
+    def __init__(
+        self,
+        shape: tuple,
+        dtype: torch.dtype,
+        seq_dim: int,
+        device: torch.device = "cpu",
+        init_cap: int = -1
+    ):
+        if seq_dim < 0: seq_dim = len(shape) + seq_dim
+        self.seq_dim = seq_dim
+        self.seq_len = 0
+        if init_cap == -1:
+            init_cap = self.PAGE_SIZE
+        else:
+            init_cap = (init_cap // self.PAGE_SIZE + 1) * self.PAGE_SIZE
+        shape = list(shape)
+        shape[seq_dim] = self.seq_cap = init_cap
+        shape = tuple(shape)
+        self.tensor = torch.empty(shape, dtype = dtype, device = device)
+
+    def __len__(self):
+        return self.seq_len
+
+    def __bool__(self):
+        return self.seq_len > 0
+
+    @staticmethod
+    def from_tensor(tensor: torch.Tensor, seq_dim: int):
+        s = SeqTensor(tensor.shape, tensor.dtype, seq_dim, tensor.device, init_cap = tensor.shape[seq_dim])
+        s.append(tensor)
+        return s
+
+    def clone(self, drop: int | None = None):
+        if drop and drop <= self.seq_len:
+            return SeqTensor.from_tensor(self.torch_slice(None, self.seq_len - drop), self.seq_dim)
+        else:
+            return SeqTensor.from_tensor(self.torch(), self.seq_dim)
+
+    def clear(self):
+        self.seq_len = 0
+
+    def set(self, new_data: SeqTensor | torch.tensor | None = None):
+        self.clear()
+        self.append(new_data)
+
+    def append(self, new_data: SeqTensor | torch.tensor | None):
+        if new_data is None: return
+        if isinstance(new_data, SeqTensor):
+            new_data = new_data.torch()
+        new_len = new_data.shape[self.seq_dim]
+        end_pos = self.seq_len + new_len
+        if end_pos >= self.seq_cap:
+            new_cap = (end_pos // self.PAGE_SIZE + 1) * self.PAGE_SIZE
+            grow_shape = list(new_data.shape)
+            grow_shape[self.seq_dim] = new_cap - self.seq_cap
+            grow_shape = tuple(grow_shape)
+            grow_tensor = torch.empty(grow_shape, dtype = self.tensor.dtype, device = self.tensor.device)
+            self.tensor = torch.cat((self.tensor, grow_tensor), dim = self.seq_dim)
+            self.seq_cap = new_cap
+        s = self.tensor.narrow(self.seq_dim, self.seq_len, end_pos - self.seq_len)
+        s.copy_(new_data)
+        self.seq_len += new_len
+
+    def truncate(self, new_len: int):
+        assert new_len <= self.seq_len
+        self.seq_len = new_len
+
+    def torch(self):
+        s = self.tensor.narrow(self.seq_dim, 0, self.seq_len)
+        return s
+
+    def slice(self, a: int | None, b: int | None):
+        return SeqTensor.from_tensor(self.torch_slice(a, b), self.seq_dim)
+
+    def torch_slice(self, a: int | None, b: int | None):
+        if a is None and b is None:
+            return self.torch()
+        elif b is None:
+            s = self.tensor.narrow(self.seq_dim, a, self.seq_len - a)
+        elif a is None:
+            s = self.tensor.narrow(self.seq_dim, 0, b)
+        else:
+            s = self.tensor.narrow(self.seq_dim, a, b - a)
+        return s
+
+
+def get_basic_progress():
+    progress = Progress(
+        TextColumn("[progress.description]{task.description}"),
+        BarColumn(bar_width = None),
+        "[progress.percentage]{task.percentage:>3.0f}%",
+        TimeElapsedColumn(),
+        TimeRemainingColumn(),
+    )
+    return progress
+
 
 def list_live_tensors():
 
     tensors = {}
     gc.collect()
     torch.cuda.empty_cache()
```

## exllamav2/version.py

```diff
@@ -1 +1 @@
-__version__ = "0.0.9"
+__version__ = "0.1.0"
```

## exllamav2/exllamav2_ext/config.h

```diff
@@ -1,13 +1,18 @@
 #ifndef _config_h
 #define _config_h
 
-#define MAX_Q_GEMM_ROWS 50
+#define MAX_Q_GEMM_ROWS 32
+#define MAX_Q_GEMM_ROWS_KERNEL 4
+#define MAX_Q_GEMM_WEIGHTS 4  // must be <= MAX_Q_GEMM_ROWS_KERNEL
 
 #define QMODE_2BIT 1
 #define QMODE_3BIT 1
 #define QMODE_4BIT 1
 #define QMODE_5BIT 1
-#define QMODE_6BIT 0
+#define QMODE_6BIT 1
 #define QMODE_8BIT 0
 
+#define USE_AVX2
+//#define PROFILING
+
 #endif
```

## exllamav2/exllamav2_ext/cpp/quantize_func.cpp

```diff
@@ -1,9 +1,10 @@
 #include "quantize_func.h"
 #include "../cuda/quantize.cuh"
+#include <c10/cuda/CUDAGuard.h>
 
 void quantize_range
 (
     torch::Tensor quant,
     torch::Tensor scale,
     torch::Tensor out_q,
     float qzero,
@@ -13,47 +14,75 @@
     torch::Tensor error,
     int a,
     int b
 )
 {
     int columns = weights.size(1);
     int hcolumns = hessian_inv.size(1);
+    TORCH_CHECK(hcolumns == weights.size(0), "H shape mismatch")
+
+    const at::cuda::OptionalCUDAGuard device_guard(device_of(weights));
 
     for (int c = a; c < b; c++)
     {
-        quantize_cuda
+        fused_quantize_adjust_cuda
         (
-            ((const float*) weights.data_ptr()) + c * columns,
-            ((float*) quant.data_ptr()) + c * columns,
+            (const float*) weights.data_ptr(),
+            (float*) quant.data_ptr(),
             (const float*) scale.data_ptr(),
-            out_q.device().is_meta() ? NULL : ((uint16_t*) out_q.data_ptr()) + c * columns,
-            1,
-            columns,
-            qzero,
-            maxq
-        );
-
-        adjust_error_row_cuda
-        (
+            out_q.device().is_meta() ? NULL : (uint16_t*) out_q.data_ptr(),
             (const float*) hessian_inv.data_ptr(),
             (float*) error.data_ptr(),
-            (const float*) weights.data_ptr(),
-            (const float*) quant.data_ptr(),
-            c,
+            c,          // row
+            hcolumns,   // rows
             columns,
-            hcolumns
+            qzero,
+            maxq
         );
 
         vv_mul_sub_cuda
         (
-            ((const float*) hessian_inv.data_ptr()) + c * hcolumns + c,
-            ((const float*) error.data_ptr()) + c * columns,
-            ((float*) weights.data_ptr()) + c * columns,
+            ((const float*) hessian_inv.data_ptr()) + (uint64_t)c * (uint64_t)hcolumns + (uint64_t)c,
+            ((const float*) error.data_ptr()) + (uint64_t)c * (uint64_t)columns,
+            ((float*) weights.data_ptr()) + (uint64_t)c * (uint64_t)columns,
             b - c,
             columns
         );
     }
 
     torch::Tensor x = hessian_inv.slice(0, a, b).slice(1, b).transpose(0, 1);
     torch::Tensor y = error.slice(0, a, b);
     weights.slice(0, b).addmm_(x, y, 1.0f, -1.0f);
 }
+
+void quantize_range_inplace
+(
+    torch::Tensor weights,
+    torch::Tensor scale,
+    torch::Tensor out_q,
+    float qzero,
+    float maxq,
+    int a,
+    int b
+)
+{
+    int columns = weights.size(1);
+    int rows = weights.size(0);
+
+    const at::cuda::OptionalCUDAGuard device_guard(device_of(weights));
+
+    for (int c = a; c < b; c++)
+    {
+        quantize_rtn_cuda
+        (
+            (float*) weights.data_ptr(),
+            (const float*) scale.data_ptr(),
+            out_q.device().is_meta() ? NULL : (uint16_t*) out_q.data_ptr(),
+            c,          // row
+            rows,       // rows
+            columns,
+            qzero,
+            maxq
+        );
+    }
+}
+
```

## exllamav2/exllamav2_ext/cpp/quantize_func.h

```diff
@@ -18,8 +18,19 @@
     torch::Tensor hessian_inv,
     torch::Tensor weights,
     torch::Tensor error,
     int a,
     int b
 );
 
+void quantize_range_inplace
+(
+    torch::Tensor weights,
+    torch::Tensor scale,
+    torch::Tensor out_q,
+    float qzero,
+    float maxq,
+    int a,
+    int b
+);
+
 #endif
```

## exllamav2/exllamav2_ext/cpp/sampling.cpp

```diff
@@ -1,124 +1,240 @@
 #include "sampling.h"
 #include "util.h"
+#include "algorithm"
 #include <math.h>
 #include <vector>
 #include <queue>
 #include <utility>
+#include "avx2_target.h"
+#include "sampling_avx2.h"
+#include "profiling.h"
 
 const int top_k_heap_threshold = 500;
 
-bool* g_rep_mask = NULL;
-int g_vocab_size = 0;
+//bool* g_rep_mask = NULL;
+//int g_vocab_size = 0;
 
+// Repetition penalty
+
+AVX2_TARGET_OPTIONAL
 void apply_rep_penalty_cpu
 (
     const int vocab_size,
     const uint64_t* sequence,
     const float penalty_max,
     const int sustain,
     const int decay,
+    const float alpha_frequency,
+    const float alpha_presence,
     const int seq_len,
     float* logits
 )
 {
-    if (vocab_size != g_vocab_size)
+    profile_start("apply_rep_penalty_cpu");
+
+    // Map of which logits have already had penalties applied
+
+//    if (vocab_size > g_vocab_size)
+//    {
+//        if (g_rep_mask) free(g_rep_mask);
+//        g_vocab_size = vocab_size;
+//        g_rep_mask = (bool*) malloc(g_vocab_size * sizeof(bool));
+//    }
+//    memset(g_rep_mask, 0, g_vocab_size * sizeof(bool));
+    bool* g_rep_mask = (bool*) calloc(vocab_size, sizeof(bool));
+
+    // Penalties to apply
+
+    float rep_p = penalty_max;          // Multiplicative penalty, as in HF repetition penalty
+    float freq_p = alpha_frequency;     // Additive frequency penalty, as in OAI spec
+    float pres_p = alpha_presence;      // Additive presence penalty, as in OAI spec
+
+    // Change in penalties over the "decay" range of the context
+
+    float d_rep_p = 0.0f;
+    float d_freq_p = 0.0f;
+    float d_pres_p = 0.0f;
+    if (decay)
     {
-        if (g_rep_mask) free(g_rep_mask);
-        g_vocab_size = vocab_size;
-        g_rep_mask = (bool*) malloc(g_vocab_size * sizeof(bool));
+        d_rep_p = (1.0f - rep_p) / (float) decay;
+        d_freq_p = (0.0f - freq_p) / (float) decay;
+        d_pres_p = (0.0f - pres_p) / (float) decay;
     }
 
-    memset(g_rep_mask, 0, g_vocab_size * sizeof(bool));
+    // "sustain" length, range of the context over which penalties are fixed
+
+    int sust = sustain == -1 ? seq_len : sustain;
 
-    float v = penalty_max;
-    float dv = decay ? (1.0f - penalty_max) / (float) decay : 0.0f;
+    // First token of the penalty range, including decay
 
-    int s = sustain == -1 ? seq_len : sustain;
-    int beg = seq_len - s - decay;
+    int beg = seq_len - sust - decay;
     if (beg < 0) beg = 0;
 
+    // Iter over context, backwards
+
     for (int i = seq_len; i > beg;)
     {
         uint64_t t = sequence[--i];
-        if (!g_rep_mask[t])
+        if (t < vocab_size)
+        {
+
+            // If t has not been encountered before, apply rep_p and pres_p
+
+            if (!g_rep_mask[t])
+            {
+                if (logits[t] > 0.0) logits[t] /= rep_p;  // Multiplicative penalty
+                else logits[t] *= rep_p;
+
+                logits[t] -= pres_p;  // Additive penalty
+
+                g_rep_mask[t] = true;  // Only once per logit
+            }
+
+            // Apply freq_p penalty for every time a token is encountered, so the total additive penalty is count * freq_p
+
+            logits[t] -= freq_p;
+        }
+
+        // If we're in the "decay" range, reduce penalties for every token
+
+        if (--sust < 0)
         {
-            if (logits[t] > 0.0) logits[t] /= v;
-            else logits[t] *= v;
-            g_rep_mask[t] = true;
+            rep_p += d_rep_p;
+            freq_p += d_freq_p;
+            pres_p += d_pres_p;
         }
-        if (--s < 0) v += dv;
     }
+
+    free(g_rep_mask);
+    profile_stop();
 }
 
-void softmax_cpu
+AVX2_TARGET_OPTIONAL
+void softmax_cpu_nonavx2
 (
     const int vocab_size,
     const float temperature,
     const float* logits,
     const bool* logits_filter,
+    const float exponent,
     float* output
 )
 {
+    profile_start("softmax_cpu");
+
     float esum = 0.0f;
     float itemp = 1.0f / temperature;
-    float maxl = 0.0f;
+    float maxl = -1e38;
 
-    #pragma unroll(32)
     for (int i = 0; i < vocab_size; i++)
     {
         if (!logits_filter[i]) continue;
         maxl = fmaxf(logits[i], maxl);
     }
-    maxl *= itemp;
 
-    #pragma unroll(32)
     for (int i = 0; i < vocab_size; i++)
     {
         if (!logits_filter[i]) continue;
-        float e = expf(logits[i] * itemp - maxl);
+        float l = logits[i] - maxl;
+        if (exponent == 2.0f)
+            l *= -l;
+        else if (exponent != 1.0f)
+            l = -powf(fabs(l), exponent);
+        float e = expf(l * itemp);
         output[i] = e;
         esum += e;
     }
+
     float isum = 1.0f / esum;
 
-    #pragma unroll(32)
     for (int i = 0; i < vocab_size; i++)
     {
-        if (logits_filter[i])
-            output[i] *= isum;
-        else
-            output[i] = 0.0f;
+        if (logits_filter[i]) output[i] *= isum;
+        else output[i] = 0.0f;
     }
 
+    profile_stop();
+
 //    printf("Softmax:");
 //    float summ = 0.0f;
 //    for (int i = 0; i < vocab_size; i++)
 //    {
 //        if (logits_filter[i])
 //        {
-//            printf("%d, %f\n", i, output[i]);
 //            summ += output[i];
+//            if (output[i] < 1e-5) continue;
+//            printf("%d, %f\n", i, output[i]);
 //        }
 //    }
-//    printf("sum: %f\n", summ);
+//    printf("sum: %f\n\n", summ);
+}
+
+void softmax_cpu
+(
+    const int vocab_size,
+    const float temperature,
+    const float* logits,
+    const bool* logits_filter,
+    const float exponent,
+    float* output
+)
+{
+    if (is_avx2_supported())
+        return softmax_cpu_avx2(vocab_size, temperature, logits, logits_filter, exponent, output);
+    else
+        return softmax_cpu_nonavx2(vocab_size, temperature, logits, logits_filter, exponent, output);
 }
 
+AVX2_TARGET_OPTIONAL
 int post_softmax_temperature
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
-    float temperature
+    float temperature,
+    float min_temp = 0,
+    float max_temp = 0.0f,
+    float temp_exponent = 1.0f
 )
 {
+    profile_start("post_softmax_temperature");
+
+    if (max_temp > min_temp)
+    {
+        // Calculate entropy of the softmax probabilities
+
+        float entropy = 0.0f;
+        for (int i = 0; i < num_candidates; ++i)
+        {
+            float prob = temp_probs[i];
+            if (prob > 0.0f) entropy -= prob * logf(prob);  // Ensure no log(0)
+        }
+
+        // Calculate maximum possible entropy
+
+        float max_entropy = -logf(1.0f / num_candidates);
+
+        // Guard against division by zero
+
+        if (max_entropy == 0.0f) max_entropy = 1.0f;
+
+        // Normalize the entropy
+
+        float normalized_entropy = entropy / max_entropy;
+
+        // Map the normalized entropy to the desired temperature range using the power function
+
+        temperature = min_temp + (max_temp - min_temp) * powf(normalized_entropy, temp_exponent);
+    }
+
 //    printf("---- pre\n");
 //    for (int i = 0; i < num_candidates; ++i)
 //        DBGIF(i, temp_probs[i]);
-
+    
     float psum = 0.0f;
     float itemp = 1.0f / temperature;
     for (int i = 0; i < num_candidates; ++i)
     {
         float p = powf(temp_probs[i], itemp);
         psum += p;
         temp_probs[i] = p;
@@ -131,51 +247,35 @@
 //    printf("---- post\n");
 //    DBGF(temperature);
 //    printf("----\n");
 //    for (int i = 0; i < num_candidates; ++i)
 //        DBGIF(i, temp_probs[i]);
 //    printf("\n");
 
+    profile_stop();
     return num_candidates;
 }
 
-
+AVX2_TARGET_OPTIONAL
 void normalize_cpu
 (
     const int num_candidates,
     float* probs
 )
 {
+    profile_start("normalize_cpu");
+
     float sum = 0.0f;
     #pragma unroll(32)
     for (int i = 0; i < num_candidates; i++) sum += probs[i];
     float isum = 1.0f / sum;
     #pragma unroll(32)
     for (int i = 0; i < num_candidates; i++) probs[i] *= isum;
-}
-
-int greedy_sample
-(
-    const int num_candidates,
-    const float* probs,
-    const bool* logits_filter
-)
-{
-    int maxidx = -1;
-    float max = -1e38;
 
-    for(int i = 1; i < num_candidates; i++)
-    {
-        if (logits_filter[i] && (maxidx == -1 || probs[i] > max))
-        {
-            max = probs[i];
-            maxidx = i;
-        }
-    }
-    return maxidx;
+    profile_stop();
 }
 
 template <typename T>
 inline void swap(T &a, T &b)
 {
     T temp = a;
     a = b;
@@ -189,14 +289,15 @@
 
 inline bool cmp_desc(const float& a, const float& b)
 {
     return a < b;
 }
 
 template <bool (*cmp_func)(const float&, const float&)>
+AVX2_TARGET_OPTIONAL
 void quicksort_with_idx
 (
     float* arr,
     int* idx,
     int low,
     int high,
     int max_index
@@ -284,14 +385,15 @@
         quicksort_with_idx<cmp_func>(arr, idx, low, pos - 1, max_index);
     if (max_index == 0 || pos <= max_index)
         quicksort_with_idx<cmp_func>(arr, idx, pos + 1, high, max_index);
 }
 
 // Discard tiny probabilities, improves performance when temperature is very low
 
+AVX2_TARGET_OPTIONAL
 int pre_sort_descending
 (
     const int num_candidates,
     float* arr,
     int* idx
 )
 {
@@ -308,14 +410,15 @@
         i++;
         j--;
     }
 
     return i;
 }
 
+AVX2_TARGET_OPTIONAL
 int sort_descending
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     int max_index
 )
@@ -326,27 +429,48 @@
 //    int m = (max_index == 0 ? num_candidates : max_index);
 //    for (int i = 0; i < m; i++) printf("%i - %f \n", temp_indices[i], temp_probs[i] * 10000.0);
 //    for (int i = 0; i < m - 1; i++) if (temp_probs[i] < temp_probs[i + 1] - 2e-8) DBGI(i);
 
     return pre;
 }
 
+AVX2_TARGET_OPTIONAL
 int top_k_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     int top_k
 )
 {
-    //TIME_START;
+    profile_start("top_k_cpu");
+
+    // Special case greedy sampling
+
+    if (top_k == 1)
+    {
+        int maxidx = -1;
+        float max = -1e38;
+
+        for(int i = 0; i < num_candidates; i++)
+        {
+            if (maxidx == -1 || temp_probs[i] > max)
+            {
+                max = temp_probs[i];
+                maxidx = i;
+            }
+        }
+
+        swap<float>(temp_probs[0], temp_probs[maxidx]);
+        swap<int>(temp_indices[0], temp_indices[maxidx]);
+    }
 
     // Use min-heap for lower values of K
 
-    if (top_k <= top_k_heap_threshold)
+    else if (top_k <= top_k_heap_threshold)
     {
         std::priority_queue<std::pair<float, int>, std::vector<std::pair<float, int>>, std::greater<std::pair<float, int>>> min_heap;
 
         for (int i = 0; i < top_k; ++i) min_heap.push({temp_probs[i], temp_indices[i]});
 
         for (int i = top_k; i < num_candidates; i++)
         {
@@ -370,30 +494,30 @@
     // For larger values, quicksort is still faster
 
     else
     {
         sort_descending(num_candidates, temp_probs, temp_indices, top_k);
     }
 
-    //TIME_STOP;
-
+    profile_stop();
     return top_k;
 }
 
+AVX2_TARGET_OPTIONAL
 int top_p_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float top_p
 )
 {
-    std::priority_queue<std::pair<float, int>, std::vector<std::pair<float, int>>, std::greater<std::pair<float, int>>> min_heap;
+    profile_start("top_p_cpu");
 
-    //TIME_START;
+    std::priority_queue<std::pair<float, int>, std::vector<std::pair<float, int>>, std::greater<std::pair<float, int>>> min_heap;
 
     float min_p = 1e-6;
 
     float sum = 0.0f;
     for (int i = 0; i < num_candidates; i++)
     {
         if (temp_probs[i] < min_p) continue;
@@ -415,19 +539,19 @@
     {
         j--;
         temp_probs[j] = min_heap.top().first;
         temp_indices[j] = min_heap.top().second;
         min_heap.pop();
     }
 
-    //TIME_STOP;
-
+    profile_stop();
     return k;
 }
 
+AVX2_TARGET_OPTIONAL
 int keep_threshold
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float threshold
 )
@@ -445,45 +569,71 @@
             i++;
         }
         j--;
     }
     return i;
 }
 
+int top_a_cpu
+(
+    const int num_candidates,
+    float* temp_probs,
+    int* temp_indices,
+    float top_a
+)
+{
+    profile_start("top_a_cpu");
+
+    // Find top probability
+    float top_prob = temp_probs[0];
+    for (int i = 1; i < num_candidates; i++)
+        if (temp_probs[i] > top_prob) top_prob = temp_probs[i];
+
+    // Calculate the threshold
+    float threshold = top_a * top_prob * top_prob;
+
+    // Use the keep_threshold function to keep only probabilities above the threshold
+    int n = keep_threshold(num_candidates, temp_probs, temp_indices, threshold);
+
+    profile_stop();
+    return n;
+}
+
+AVX2_TARGET_OPTIONAL
 int min_p_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float min_p
 )
 {
-    //TIME_START;
+    profile_start("min_p_cpu");
 
     float top_prob = temp_probs[0];
     for (int i = 1; i < num_candidates; i++)
         if (temp_probs[i] > top_prob) top_prob = temp_probs[i];
 
     float threshold = top_prob * min_p;
     int n = keep_threshold(num_candidates, temp_probs, temp_indices, threshold);
 
-    //TIME_STOP;
-
+    profile_stop();
     return n;
 }
 
+AVX2_TARGET_OPTIONAL
 int tfs_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float tfs
 )
 {
-    //TIME_START;
+    profile_start("tfs_cpu");
 
     if (num_candidates < 3) return num_candidates;  // Discrete 2nd derivative undefined
 
     // 2nd derivative of sorted probs
 
     int nc = sort_descending(num_candidates, temp_probs, temp_indices, num_candidates);
 
@@ -511,28 +661,30 @@
     // Center distribution on the cutoff point
 
     k++;
 
     //TIME_STOP;
 
     free(derivative);
+    profile_stop();
     return k;
 }
 
+AVX2_TARGET_OPTIONAL
 int mirostat_pre_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float mirostat_mu,
     float mirostat_tau,
     float mirostat_eta
 )
 {
-    //TIME_START;
+    profile_start("mirostat_pre_cpu");
 
     // If mu not yet initialized, initialize here
 
     float mu = mirostat_mu;
     if (mu == 0.0f) mu = mirostat_tau * 2.0f;
 
     // Discard tokens with surprise greater than mu
@@ -540,124 +692,140 @@
     int nc = sort_descending(num_candidates, temp_probs, temp_indices, num_candidates);
 
     float target_prob = powf(2, -mu);
     int k = 1;
     for (; k < nc; k++)
         if (temp_probs[k] < target_prob) break;
 
-    //TIME_STOP;
-
+    profile_stop();
     return k;
 }
 
 float mirostat_post_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float mirostat_mu,
     float mirostat_tau,
     float mirostat_eta
 )
 {
+    profile_start("mirostat_post_cpu");
+
     // If mu not yet initializer, initialize here
 
     float mu = mirostat_mu;
     if (mu == 0.0f) mu = mirostat_tau * 2.0f;
 
     // Adjust mu based on probability of final choice
 
     float observed_surprise = -log2(temp_probs[0]);
     mu += mirostat_eta * (mirostat_tau - observed_surprise);
 
+    profile_stop();
     return mu;
 }
 
+AVX2_TARGET_OPTIONAL
 int typical_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float typical
 )
 {
-    //TIME_START;
+    profile_start("typical_cpu");
 
     const float epsilon = 1e-10;
 
-    float* temp = (float*) malloc(num_candidates * sizeof(float));
-    int* entropy_dev_order = (int*) malloc(num_candidates * sizeof(int));
-    int* temp_indices_2 = (int*) malloc(num_candidates * sizeof(int));
+    int r_candidates = pre_sort_descending(num_candidates, temp_probs, temp_indices);
+
+    float* temp = (float*) malloc(r_candidates * sizeof(float));
+    int* entropy_dev_order = (int*) malloc(r_candidates * sizeof(int));
+    int* temp_indices_2 = (int*) malloc(r_candidates * sizeof(int));
 
     float neg_entropy = 0.0f;
-    for (int i = 0; i < num_candidates; i++)
+    for (int i = 0; i < r_candidates; i++)
     {
         float x = temp_probs[i];
         float y = x + logf(x + epsilon);
         neg_entropy += x * y;
         temp[i] = y;  // temp = log_probs
     }
 
-    for (int i = 0; i < num_candidates; i++)
+    for (int i = 0; i < r_candidates; i++)
     {
         temp[i] = fabs(temp[i] - neg_entropy);  // temp = entropy_dev
         entropy_dev_order[i] = i;
     }
 
-    quicksort_with_idx<cmp_asc>(temp, entropy_dev_order, 0, num_candidates - 1, num_candidates);
+    quicksort_with_idx<cmp_asc>(temp, entropy_dev_order, 0, r_candidates - 1, r_candidates);
 
-    memcpy(temp, temp_probs, num_candidates * sizeof(float));  // temp = temp_probs
-    memcpy(temp_indices_2, temp_indices, num_candidates * sizeof(int));
+    memcpy(temp, temp_probs, r_candidates * sizeof(float));  // temp = temp_probs
+    memcpy(temp_indices_2, temp_indices, r_candidates * sizeof(int));
 
     float cumprob = 0.0f;
     int num = 0;
 
     while (true)
     {
         int j = entropy_dev_order[num];
         float p = temp[j];
         temp_probs[num] = p;
         temp_indices[num] = temp_indices_2[j];
 
         cumprob += p;
         if (cumprob >= typical) break;
         num++;
-        if (num >= num_candidates) break;
+        if (num >= r_candidates) break;
     }
 
     free(temp);
     free(entropy_dev_order);
     free(temp_indices_2);
 
-    //TIME_STOP;
-
     if (num == 0) num = 1;
+
+    profile_stop();
     return num;
 }
 
+AVX2_TARGET_OPTIONAL
 int multinomial_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float random
 )
 {
+//    printf("\n-----------------\n");
+//    int j = 0;
+//    for (int i = 0; i < num_candidates && j < 10; ++i)
+//    {
+//        if (temp_probs[i] < 1e-6) continue;
+//        DBGIF(i, temp_probs[i]);
+//        j++;
+//    }
+//    printf("-----------------\n");
+
+    profile_start("multinomial_cpu");
+
     int idx = 0;
     float accum = temp_probs[idx];
 
     while (true)
     {
         if (accum >= random) break;
         if (idx == num_candidates - 1) break;
         idx++;
         accum += temp_probs[idx];
     }
 
-    temp_probs[0] = temp_probs[idx];
-    temp_indices[0] = temp_indices[idx];
+    swap<float>(temp_probs[0], temp_probs[idx]);
+    swap<int>(temp_indices[0], temp_indices[idx]);
 
+    profile_stop();
     return 1;
-}
-
-
-
+}
```

## exllamav2/exllamav2_ext/cpp/sampling.h

```diff
@@ -1,46 +1,54 @@
 #ifndef _sampling_h
 #define _sampling_h
 
 #include <cstdlib>
 #include <cstring>
 #include <cstdint>
 #include <cstdio>
+#include <string>
+
+void profile_results();
+void profile_start(std::string stage);
+void profile_stop();
 
 void apply_rep_penalty_cpu
 (
     const int vocab_size,
     const uint64_t* sequence,
     const float penalty_max,
     const int sustain,
     const int decay,
+    const float alpha_frequency,
+    const float alpha_presence,
     const int seq_len,
     float* logits
 );
 
 void softmax_cpu
 (
     const int vocab_size,
     const float temperature,
     const float* logits,
     const bool* logits_filter,
+    const float exponent,
     float* output
 );
 
 void normalize_cpu
 (
     const int num_candidates,
     float* probs
 );
 
-int greedy_sample
+int pre_sort_descending
 (
     const int num_candidates,
-    const float* probs,
-    const bool* logits_filter
+    float* arr,
+    int* idx
 );
 
 int sort_descending
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
@@ -59,14 +67,22 @@
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float top_p
 );
 
+int top_a_cpu
+(
+    const int num_candidates,
+    float* temp_probs,
+    int* temp_indices,
+    float top_a
+);
+
 int min_p_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float min_p
 );
@@ -108,21 +124,22 @@
 );
 
 int post_softmax_temperature
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
-    float temperature
+    float temp,
+    float min_temp,
+    float max_temp,
+    float temp_exponent
 );
 
 int multinomial_cpu
 (
     const int num_candidates,
     float* temp_probs,
     int* temp_indices,
     float random
 );
 
 #endif
-
-
```

## exllamav2/exllamav2_ext/cpp/util.h

```diff
@@ -1,16 +1,20 @@
 #ifndef _util_h
 #define _util_h
 
 #include <chrono>
 
 #define DBGS(__x) printf("%s\n", __x)
-#define DBGI(__x) printf("%s: %i\n", #__x, __x)
-#define DBGI2(__x, __y) printf("%s, %s: %i, %i\n", #__x, #__y, __x, __y)
-#define DBGI3(__x, __y, __z) printf("%s, %s, %s: %i, %i, %i\n", #__x, #__y, #__z, __x, __y, __z)
+#define DBGI(__x) printf("%s: %li\n", #__x, __x)
+#define DBGI2(__x, __y) printf("%s, %s: %li, %li\n", #__x, #__y, __x, __y)
+#define DBGI3(__x, __y, __z) printf("%s, %s, %s: %li, %li, %li\n", #__x, #__y, #__z, __x, __y, __z)
+#define DBGX(__x) printf("%s: %lx\n", #__x, __x)
+#define DBGX2(__x, __y) printf("%s, %s: %lx, %lx\n", #__x, #__y, __x, __y)
+#define DBGX3(__x, __y, __z) printf("%s, %s, %s: %lx, %lx, %lx\n", #__x, #__y, #__z, __x, __y, __z)
+
 #define DBGF(__x) printf("%s: %f\n", #__x, __x)
 #define DBGF2(__x, __y) printf("%s, %s: %f, %f\n", #__x, #__y, __x, __y)
 #define DBGF3(__x, __y, __z) printf("%s, %s, %s: %f, %f, %f\n", #__x, #__y, #__z, __x, __y, __z)
 #define DBGIF(__x, __y) printf("%s, %s: %i, %f\n", #__x, #__y, __x, __y)
 
 #define TIME_START \
     auto start = std::chrono::high_resolution_clock::now()
@@ -18,8 +22,19 @@
 #define TIME_STOP \
     do { \
         auto stop = std::chrono::high_resolution_clock::now(); \
         auto duration_us = std::chrono::duration_cast<std::chrono::microseconds>(stop - start); \
         DBGI(duration_us); \
     } while (false)
 
+#define DIVIDE(x, size) (((x) + (size) - 1) / (size))
+
+// Some decluttering macros
+
+#define TORCH_CHECK_DTYPE(__x, __dtype) TORCH_CHECK((__x).dtype() == torch::__dtype, #__x " is incorrect datatype, must be " #__dtype)
+#define TORCH_CHECK_DTYPE_OPT(__x, __dtype) TORCH_CHECK((__x).device().is_meta() || (__x).dtype() == torch::__dtype, #__x " is incorrect datatype, must be " #__dtype)
+#define TORCH_CHECK_SHAPES(__x, __dim_x, __y, __dim_y, __scale_y) TORCH_CHECK((__x).size(__dim_x) == (__y).size(__dim_y) * __scale_y, #__x " and " #__y " have incompatible shapes")
+#define TORCH_CHECK_SHAPES_OPT(__x, __dim_x, __y, __dim_y, __scale_y) TORCH_CHECK((__x).device().is_meta() || (__x).size(__dim_x) == (__y).size(__dim_y) * __scale_y, #__x " and " #__y " have incompatible shapes")
+#define TORCH_CHECK_SHAPES_FULL(__x, __y) TORCH_CHECK((__x).sizes() == (__y).sizes(), #__x " and " #__y " have incompatible shapes")
+#define TORCH_CHECK_NUMEL(__x, __y) TORCH_CHECK((__x).numel() == (__y).numel(), #__x " and " #__y " have incompatible shapes")
+
 #endif
```

## exllamav2/exllamav2_ext/cuda/cache.cu

```diff
@@ -1,17 +1,17 @@
 #include "cache.cuh"
 
-// #if defined(CUDART_VERSION) && CUDART_VERSION >= 11080
-//
-// #include <cuda_fp8.h>
-
 #include "quant/qdq_util.cuh"
 #include "util.cuh"
+#include "compat.cuh"
 
 #define THREADS 32
+#define BLOCKSIZE_Q 256
+#define THREADS_Q (BLOCKSIZE_Q / 2)
+#define HADAMARD_Q4
 
 // The upper 8 bits of FP16 are equivalent to FP8 E5M2.
 //
 // The range of values typically cached seem to be in the range of +/- 16, with an exponent component (with bias) up to
 // about 20. Empirically, the MSE over the whole range of observed values in the K/V cache works out the same for E4M3
 // and E5M2. However, over 80% of values in the cache tensors fall within the range of -1..1, where E5M2 produces about
 // a 25% lower MSE.
@@ -26,15 +26,15 @@
 __device__ inline uint32_t decompress(uint32_t v)
 {
     uint32_t vh = (v & 0xff00) << 16;
     uint32_t vl = (v & 0x00ff) << 8;
     return vh | vl;
 }
 
-__global__ void nv_fp16_to_fp8
+__global__ void fp16_to_fp8_kernel
 (
     const half* __restrict__ pIn,
     unsigned char* __restrict__ pOut,
     int stride,
     int height,
     int min,
     int max
@@ -52,15 +52,15 @@
     uint32_t c1 = compress(in.y);
     uint32_t c2 = compress(in.z);
     uint32_t c3 = compress(in.w);
     int2 out = make_int2(c0 | (c1 << 16), c2 | (c3 << 16));
     *out_ptr = out;
 }
 
-__global__ void nv_fp8_to_fp16
+__global__ void fp8_to_fp16_kernel
 (
     const unsigned char* __restrict__ pIn,
     half* __restrict__ pOut,
     int stride,
     int height,
     int min,
     int max
@@ -78,51 +78,27 @@
     uint32_t c1 = decompress(in.x >> 16);
     uint32_t c2 = decompress(in.y);
     uint32_t c3 = decompress(in.y >> 16);
     int4 out = make_int4(c0, c1, c2, c3);
     *out_ptr = out;
 }
 
-// __global__ void nv_fp32_to_fp16(const float* pIn, half* pOut, int size)
-// {
-//     int i = blockIdx.x * blockDim.x + threadIdx.x;
-//     if (i < size) {
-//         pOut[i] = __float2half(pIn[i]);
-//     }
-// }
-
-// __global__ void nv_fp16_to_fp8_ref(const half* pIn, unsigned char *pOut, int size)
-// {
-//     int i = blockIdx.x * blockDim.x + threadIdx.x;
-//     if (i < size) {
-//         pOut[i] = __nv_cvt_halfraw_to_fp8(pIn[i], __NV_SATFINITE, __NV_E4M3);
-//     }
-// }
-//
-// __global__ void nv_fp8_to_fp16_ref(const unsigned char* pIn, half* pOut, int size)
-// {
-//     int i = blockIdx.x * blockDim.x + threadIdx.x;
-//     if (i < size) {
-//         pOut[i] = __nv_cvt_fp8_to_halfraw(pIn[i], __NV_E4M3);
-//     }
-// }
-
 void array_fp16_to_fp8_cuda(const half* pIn, unsigned char *pOut, int stride, int height, int offset, int width)
 {
     int min = offset;
     int max = offset + width;
     min = min / 8 * 8;
     max = min + (max - min + 7) / 8 * 8;
 
     dim3 blockDim, gridDim;
     blockDim.x = THREADS;
     gridDim.x = DIVIDE((max - min) / 8, THREADS);
     gridDim.y = height;
 
-    nv_fp16_to_fp8<<<gridDim, blockDim>>>(pIn, pOut, stride, height, min, max);
+    fp16_to_fp8_kernel<<<gridDim, blockDim>>>(pIn, pOut, stride, height, min, max);
     // cuda_check( cudaPeekAtLastError() );
 }
 
 void array_fp8_to_fp16_cuda(const unsigned char* pIn, half* pOut, int stride, int height, int offset, int width)
 {
     int min = offset;
     int max = offset + width;
@@ -130,32 +106,398 @@
     max = min + (max - min + 7) / 8 * 8;
 
     dim3 blockDim, gridDim;
     blockDim.x = THREADS;
     gridDim.x = DIVIDE((max - min) / 8, THREADS);
     gridDim.y = height;
 
-    nv_fp8_to_fp16<<<gridDim, blockDim>>>(pIn, pOut, stride, height, min, max);
+    fp8_to_fp16_kernel<<<gridDim, blockDim>>>(pIn, pOut, stride, height, min, max);
     // cuda_check( cudaPeekAtLastError() );
 }
 
-// void array_fp16_to_fp8_ref_cuda(const half* pIn, unsigned char *pOut, int size)
-// {
-//     const int threads = 512;
-//     int blocks = DIVIDE(size / 1, threads);
-//     nv_fp16_to_fp8_ref<<<blocks, threads>>>(pIn, pOut, size);
-// }
-//
-// void array_fp8_to_fp16_ref_cuda(const unsigned char* pIn, half* pOut, int size)
-// {
-//     const int threads = 512;
-//     int blocks = DIVIDE(size / 1, threads);
-//     nv_fp8_to_fp16_ref<<<blocks, threads>>>(pIn, pOut, size);
-// }
+// Q4
 
-// #else
-//
-// void array_fp16_to_fp8_cuda(const half* pIn, unsigned char *pOut, int size) { }
-//
-// void array_fp8_to_fp16_cuda(const unsigned char* pIn, half* pOut, int size) { }
-//
-// #endif
+__device__ void fp16_to_q4
+(
+    int t,
+    const half* in,
+    unsigned char* out,
+    half* scales,
+    int block_offset
+)
+{
+    const half2* in2 = (const half2*) (in + block_offset);
+    __shared__ uint32_t q_buffer[BLOCKSIZE_Q / 8];
+    __shared__ half s_buffer[BLOCKSIZE_Q / 32];
+
+    half2 w2 = in2[t];
+    half2 o = w2;
+
+    // Perform hadamard transform on two interleaved 32-element groups. Don't scale output by 1/sqrt(32) here, instead
+    // scale by 1/32 when dequantizing
+
+    #ifdef HADAMARD_Q4
+
+        for (int i = 1; i < 32; i <<= 1)
+        {
+            half2 pw2 = __shfl_xor_sync(0xffffffff, w2, i);
+            uint32_t* w2i = reinterpret_cast<uint32_t*>(&w2);
+            int32_t sfm = -static_cast<int32_t>(t & i) >> 31;
+            *w2i ^= (sfm & 0x80008000);
+            w2 = __hadd2(w2, pw2);
+        }
+
+    #endif
+
+    // Max abs value for lane_id 0..15, 16..31
+
+    half2 absmax2 = __habs2(w2);
+    half absmax = __hmax(__low2half(absmax2), __high2half(absmax2));
+    absmax = __hmax(absmax, __shfl_xor_sync(0xffffffff, absmax, 8));
+    absmax = __hmax(absmax, __shfl_xor_sync(0xffffffff, absmax, 4));
+    absmax = __hmax(absmax, __shfl_xor_sync(0xffffffff, absmax, 2));
+    absmax = __hmax(absmax, __shfl_xor_sync(0xffffffff, absmax, 1));
+    absmax2 = __half2half2(absmax);
+
+    // Normalize
+
+    half2 c_8 = __half2half2(__float2half_rn(8));
+    half c_i = __float2half_rn(1.0f / 8.0f);
+
+    w2 = __h2div(w2, absmax2);
+    w2 = __hfma2(w2, c_8, c_8);
+
+    // Quantize & pack
+
+    int q0 = clamp(__half2int_rn(__low2half(w2)), 0, 15);
+    int q1 = clamp(__half2int_rn(__high2half(w2)), 0, 15);
+    uint32_t q = q0 | (q1 << 4);
+
+    q |= (__shfl_down_sync(0x55555555, q, 1) << 8);
+    q |= (__shfl_down_sync(0x11111111, q, 2) << 16);
+    if (t % 4 == 0) q_buffer[t / 4] = q;
+    if (t % 16 == 0) s_buffer[t / 16] = __hmul(absmax, c_i);
+    __syncthreads();
+
+    // Store
+
+    int4* pq = (int4*) q_buffer;
+    int4* ps = (int4*) s_buffer;
+    int4* out_q = (int4*) (out + block_offset / 2);
+    int4* out_s = (int4*) (scales + block_offset / 32);
+
+    if (t < BLOCKSIZE_Q / 32) out_q[t] = pq[t];
+    if (t < BLOCKSIZE_Q / 256) out_s[t] = ps[t];
+}
+
+__device__ void q4_to_fp16
+(
+    int t,
+    const unsigned char* in,
+    const half* scales,
+    half* out,
+    int block_offset
+)
+{
+    __shared__ uint32_t q_buffer[BLOCKSIZE_Q / 8];
+    __shared__ half s_buffer[BLOCKSIZE_Q / 32];
+
+    // Fetch
+
+    int4* in_q = (int4*) (in + block_offset / 2);
+    int4* in_s = (int4*) (scales + block_offset / 32);
+    int4* pq = (int4*) q_buffer;
+    int4* ps = (int4*) s_buffer;
+
+    if (t < BLOCKSIZE_Q / 32) pq[t] = in_q[t];
+    if (t < BLOCKSIZE_Q / 256) ps[t] = in_s[t];
+    __syncthreads();
+
+    // Get scale
+
+    half scale = s_buffer[t / 16];
+    half2 scale2 = __half2half2(scale);
+
+    // Dequantize
+
+    int shift0 = (t % 4) * 8;
+    int shift1 = shift0 + 4;
+    uint32_t q = q_buffer[t / 4];
+    int q0 = ((int) ((q >> shift0) & 0x0f)) - 8;
+    int q1 = ((int) ((q >> shift1) & 0x0f)) - 8;
+
+    half w0 = __int2half_rn(q0);
+    half w1 = __int2half_rn(q1);
+    half2 w2 = __halves2half2(w0, w1);
+    w2 = __hmul2(w2, scale2);
+
+    // Perform hadamard transform on two interleaved 32-element groups. Skipped scaling when quantizing, so result
+    // is scaled by 1/32 here
+
+    #ifdef HADAMARD_Q4
+
+        for (int i = 1; i < 32; i <<= 1)
+        {
+            half2 pw2 = __shfl_xor_sync(0xffffffff, w2, i);
+            uint32_t* w2i = reinterpret_cast<uint32_t*>(&w2);
+            int32_t sfm = -static_cast<int32_t>(t & i) >> 31;
+            *w2i ^= (sfm & 0x80008000);
+            w2 = __hadd2(w2, pw2);
+        }
+        w2 = __hmul2(w2, __float2half2_rn(1.0f/32.0f));
+
+    #endif
+
+    // Store
+
+    half2* out2 = (half2*) (out + block_offset);
+    out2[t] = w2;
+}
+
+// -------------- FP16 -> Q4
+
+__global__ void fp16_to_q4_kv_paged_kernel
+(
+    const half* __restrict__ k_in,
+    unsigned char* __restrict__ k_out,
+    half* __restrict__ k_scales,
+    const half* __restrict__ v_in,
+    unsigned char* __restrict__ v_out,
+    half* __restrict__ v_scales,
+    const int* __restrict__ cache_seqlens,
+    const int* __restrict__ block_table,
+    int pages_per_seq,
+    int page_size,
+    int dim,
+    int q_len
+)
+{
+    int t = threadIdx.x;
+    const half* in = blockIdx.z ? v_in : k_in;
+    half* scales = blockIdx.z ? v_scales : k_scales;
+    unsigned char* out = blockIdx.z ? v_out : k_out;
+
+    int x = blockIdx.x;
+    int y = blockIdx.y;
+
+    int page = block_table[pages_per_seq * y + x];
+    int seqlen = cache_seqlens[y];
+    int vx_a = page_size * x;
+    int px_a = seqlen - vx_a;
+    int px_b = px_a + q_len;
+    px_a = max(px_a, 0);
+    px_b = min(px_b, page_size);
+
+    int block_a = (page * page_size + px_a) * dim;
+    int block_b = (page * page_size + px_b) * dim;
+
+    for (int i = block_a; i < block_b; i += BLOCKSIZE_Q)
+    {
+//        if (!t) DBGI2(y, i);
+        fp16_to_q4(t, in, out, scales, i);
+    }
+}
+
+__global__ void fp16_to_q4_kv_kernel
+(
+    const half* __restrict__ k_in,
+    unsigned char* __restrict__ k_out,
+    half* __restrict__ k_scales,
+    const half* __restrict__ v_in,
+    unsigned char* __restrict__ v_out,
+    half* __restrict__ v_scales,
+    int offset,
+    int stride
+)
+{
+    int t = threadIdx.x;
+    const half* in = blockIdx.z ? v_in : k_in;
+    unsigned char* out = blockIdx.z ? v_out : k_out;
+    half* scales = blockIdx.z ? v_scales : k_scales;
+    int block_offset = (offset + blockIdx.y * stride + blockIdx.x * BLOCKSIZE_Q);
+
+    fp16_to_q4(t, in, out, scales, block_offset);
+}
+
+void array_fp16_to_q4_kv_paged_cuda
+(
+    const half* k_in,
+    unsigned char* k_out,
+    half* k_scales,
+    const half* v_in,
+    unsigned char* v_out,
+    half* v_scales,
+    int batch_size,
+    int dim,
+    int pages_per_seq,
+    const int* cache_seqlens,
+    const int* block_table,
+    int page_size,
+    int q_len
+)
+{
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_Q;
+    gridDim.x = pages_per_seq;
+    gridDim.y = batch_size;
+    gridDim.z = v_in ? 2 : 1;
+
+    fp16_to_q4_kv_paged_kernel<<<gridDim, blockDim>>>
+    (
+        k_in,
+        k_out,
+        k_scales,
+        v_in,
+        v_out,
+        v_scales,
+        cache_seqlens,
+        block_table,
+        pages_per_seq,
+        page_size,
+        dim,
+        q_len
+    );
+}
+
+void array_fp16_to_q4_kv_cuda
+(
+    const half* k_in,
+    unsigned char* k_out,
+    half* k_scales,
+    const half* v_in,
+    unsigned char* v_out,
+    half* v_scales,
+    int stride,
+    int height,
+    int offset,
+    int width
+)
+{
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_Q;
+    gridDim.x = width / BLOCKSIZE_Q;
+    gridDim.y = height;
+    gridDim.z = v_in ? 2 : 1;
+
+    fp16_to_q4_kv_kernel<<<gridDim, blockDim>>>(k_in, k_out, k_scales, v_in, v_out, v_scales, offset, stride);
+}
+
+// --------------- Q4 -> FP16
+
+__global__ void q4_to_fp16_kv_paged_kernel
+(
+    const unsigned char* __restrict__ k_in,
+    const half* __restrict__ k_scales,
+    half* __restrict__ k_out,
+    const unsigned char* __restrict__ v_in,
+    const half* __restrict__ v_scales,
+    half* __restrict__ v_out,
+    const int* __restrict__ cache_seqlens,
+    const int* __restrict__ block_table,
+    int pages_per_seq,
+    int page_size,
+    int dim
+)
+{
+    int t = threadIdx.x;
+    const unsigned char* in = blockIdx.z ? v_in : k_in;
+    const half* scales = blockIdx.z ? v_scales : k_scales;
+    half* out = blockIdx.z ? v_out : k_out;
+
+    int x = blockIdx.x;
+    int y = blockIdx.y;
+    int page = block_table[pages_per_seq * y + x];
+    int seqlen = cache_seqlens[y];
+    int vx_a = page_size * x;
+    int vx_b = min(vx_a + page_size, seqlen);
+    int vnum = max(vx_b - vx_a, 0);
+    int block_a = (page * page_size) * dim;
+    int block_b = (page * page_size + vnum) * dim;
+
+    for (int i = block_a; i < block_b; i += BLOCKSIZE_Q)
+    {
+//        if (!t) DBGI2(y, i);
+        q4_to_fp16(t, in, scales, out, i);
+    }
+}
+
+__global__ void q4_to_fp16_kv_kernel
+(
+    const unsigned char* __restrict__ k_in,
+    const half* __restrict__ k_scales,
+    half* __restrict__ k_out,
+    const unsigned char* __restrict__ v_in,
+    const half* __restrict__ v_scales,
+    half* __restrict__ v_out,
+    int offset,
+    int stride
+)
+{
+    int t = threadIdx.x;
+    const unsigned char* in = blockIdx.z ? v_in : k_in;
+    const half* scales = blockIdx.z ? v_scales : k_scales;
+    half* out = blockIdx.z ? v_out : k_out;
+    int block_offset = (offset + blockIdx.y * stride + blockIdx.x * BLOCKSIZE_Q);
+
+    q4_to_fp16(t, in, scales, out, block_offset);
+}
+
+void array_q4_to_fp16_kv_paged_cuda
+(
+    const unsigned char* k_in,
+    const half* k_scales,
+    half* k_out,
+    const unsigned char* v_in,
+    const half* v_scales,
+    half* v_out,
+    int batch_size,
+    int dim,
+    int pages_per_seq,
+    const int* cache_seqlens,
+    const int* block_table,
+    int page_size
+)
+{
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_Q;
+    gridDim.x = pages_per_seq;
+    gridDim.y = batch_size;
+    gridDim.z = v_in ? 2 : 1;
+
+    q4_to_fp16_kv_paged_kernel<<<gridDim, blockDim>>>
+    (
+        k_in,
+        k_scales,
+        k_out,
+        v_in,
+        v_scales,
+        v_out,
+        cache_seqlens,
+        block_table,
+        pages_per_seq,
+        page_size,
+        dim
+    );
+}
+
+void array_q4_to_fp16_kv_cuda
+(
+    const unsigned char* k_in,
+    const half* k_scales,
+    half* k_out,
+    const unsigned char* v_in,
+    const half* v_scales,
+    half* v_out,
+    int stride,
+    int height,
+    int offset,
+    int width
+)
+{
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_Q;
+    gridDim.x = width / BLOCKSIZE_Q;
+    gridDim.y = height;
+    gridDim.z = v_in ? 2 : 1;
+
+    q4_to_fp16_kv_kernel<<<gridDim, blockDim>>>(k_in, k_scales, k_out, v_in, v_scales, v_out, offset, stride);
+}
```

## exllamav2/exllamav2_ext/cuda/cache.cuh

```diff
@@ -2,13 +2,75 @@
 #define _cache_cuh
 
 #include <cuda_runtime.h>
 #include <cuda_fp16.h>
 #include <cstdint>
 #include <cstdio>
 
-void array_fp16_to_fp8_cuda(const half* pIn, unsigned char *pOut, int stride, int height, int offset, int width);
+void array_fp16_to_fp8_cuda(const half* pIn, unsigned char* pOut, int stride, int height, int offset, int width);
 void array_fp8_to_fp16_cuda(const unsigned char* pIn, half* pOut, int stride, int height, int offset, int width);
+
+void array_fp16_to_q4_kv_cuda
+(
+    const half* k_in,
+    unsigned char* k_out,
+    half* k_scales,
+    const half* v_in,
+    unsigned char* v_out,
+    half* v_scales,
+    int stride,
+    int height,
+    int offset,
+    int width
+);
+
+void array_q4_to_fp16_kv_cuda
+(
+    const unsigned char* k_in,
+    const half* k_scales,
+    half* k_out,
+    const unsigned char* v_in,
+    const half* v_scales,
+    half* v_out,
+    int stride,
+    int height,
+    int offset,
+    int width
+);
+
+void array_fp16_to_q4_kv_paged_cuda
+(
+    const half* k_in,
+    unsigned char* k_out,
+    half* k_scales,
+    const half* v_in,
+    unsigned char* v_out,
+    half* v_scales,
+    int batch_size,
+    int dim,
+    int pages_per_seq,
+    const int* cache_seqlens,
+    const int* block_table,
+    int page_size,
+    int q_len
+);
+
+void array_q4_to_fp16_kv_paged_cuda
+(
+    const unsigned char* k_in,
+    const half* k_scales,
+    half* k_out,
+    const unsigned char* v_in,
+    const half* v_scales,
+    half* v_out,
+    int batch_size,
+    int dim,
+    int pages_per_seq,
+    const int* cache_seqlens,
+    const int* block_table,
+    int page_size
+);
+
 // void array_fp16_to_fp8_ref_cuda(const half* pIn, unsigned char *pOut, int size);
 // void array_fp8_to_fp16_ref_cuda(const unsigned char* pIn, half* pOut, int size);
 
 #endif
```

## exllamav2/exllamav2_ext/cuda/compat.cuh

```diff
@@ -49,8 +49,65 @@
 #if __CUDA_ARCH__ < 600 || defined(USE_ROCM)
 __device__ __forceinline__ void atomicAdd(half2* address, half2 val) { atomicAdd_half2(address, val); }
 #endif
 
 #endif
 #endif
 
+// Approximate tanh
+
+__forceinline__ __device__ float copysignf_pos(float a, float b)
+{
+    float r;
+    r = __int_as_float(__float_as_int(a) | (__float_as_int(b) & 0x80000000));
+    return r;
+}
+
+#if defined(USE_ROCM) || (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 750 || CUDART_VERSION < 11000))
+
+__inline__ __device__ float tanh_opt(float x)
+{
+    const float exp_val = -1.f * fabs(2 * x);
+    return copysignf_pos((1.0f - __expf(exp_val)) / (__expf(exp_val) + 1.0f), x);
+}
+
+#else
+
+__inline__ __device__ float tanh_opt(float x)
+{
+    float r;
+    asm("tanh.approx.f32 %0,%1; \n\t" : "=f"(r) : "f"(x));
+    return r;
+}
+
+#endif
+
+// ROCm redefinitions
+
+#if defined(USE_ROCM)
+
+#define __shfl_xor_sync(mask, var, laneMask) __shfl_xor(var, laneMask)
+#define __shfl_down_sync(mask, var, laneMask) __shfl_down(var, laneMask)
+
+__device__ __forceinline__ __half2 __compat_h2rcp(__half2 x)
+{
+    return __halves2half2
+    (
+         hrcp(__low2half(x)),
+         hrcp(__high2half(x))
+    );
+}
+#define h2rcp __compat_h2rcp
+
+__device__ __forceinline__ __half2 __compat_hmax2(__half2 x, __half2 y)
+{
+    return __halves2half2
+    (
+        __hmax(__low2half(x), __low2half(y)),
+        __hmax(__high2half(x), __high2half(y))
+    );
+}
+#define __hmax2 __compat_hmax2
+
+#endif
+
 #endif
```

## exllamav2/exllamav2_ext/cuda/h_gemm.cu

```diff
@@ -138,15 +138,14 @@
     half* __restrict__ c,
     bool clear
 )
 {
     int m = blockIdx.y * W_THREADS_M + threadIdx.y;
     int n = blockIdx.x * W_THREADS_N + threadIdx.x;
 
-    if (n >= size_n) return;
     if (m >= size_m) return;
 
     MatrixView_half a_(a, size_m, size_k);
     MatrixView_half b_(b, size_k, size_n);
     MatrixView_half_rw c_(c, size_m, size_n);
 
     half* c_ptr = c_.item_ptr(m, n);
@@ -156,14 +155,16 @@
 
     if (t < size_k)
     {
         read_a[t] = a_.item(m, t);
     }
     __syncthreads();
 
+    if (n >= size_n) return;
+
     half r = {};
 
     for (int k = 0; k < size_k; ++k)
     {
         half item_a = read_a[k];
         half item_b = b_.item(k, n);
         r = __hfma(item_a, item_b, r);
```

## exllamav2/exllamav2_ext/cuda/h_gemm.cuh

```diff
@@ -18,9 +18,22 @@
     const half* a,
     const half* b,
     half* c,
     const float alpha,
     const float beta
 );
 
+void h_gemm_cublas
+(
+    cublasHandle_t cublas_handle,
+    const int size_m,
+    const int size_n,
+    const int size_k,
+    const half* a,
+    const half* b,
+    half* c,
+    const float alpha,
+    const float beta
+);
+
 #endif
```

## exllamav2/exllamav2_ext/cuda/q_attn.cu

```diff
@@ -1,10 +1,12 @@
 #include "q_attn.cuh"
 #include "q_gemm.cuh"
 #include "rms_norm.cuh"
+#include "layer_norm.cuh"
+#include "head_norm.cuh"
 #include "rope.cuh"
 #include "util.cuh"
 #include "lora.cuh"
 
 const int THREADS_X = 32;
 const int THREADS_Y = 1;
 const int THREADS_Z = 4;
@@ -66,14 +68,16 @@
         value_cache_ptr += cache_stride[0] / BLOCKSIZE_X;
     }
 }
 
 QAttn::QAttn
 (
     half* _layernorm,
+    half* _layernorm_bias,
+    bool _layernorm_is_rms,
     float _norm_epsilon,
     QMatrix* _q_proj,
     QMatrix* _k_proj,
     QMatrix* _v_proj,
     QMatrix* _o_proj,
     half* _temp_state,
 //     half* _temp_q,
@@ -81,17 +85,23 @@
 //     half* _temp_v,
     half* _temp_dq,
     int _max_rows,
     int _hidden_size,
     int _num_heads,
     int _num_kv_heads,
     int _head_dim,
-    int _max_seq_len
+    int _max_seq_len,
+    bool _has_residual,
+    int _rope_style,
+    half* _q_norm,
+    half* _k_norm
 ):
     layernorm(_layernorm),
+    layernorm_bias(_layernorm_bias),
+    layernorm_is_rms(_layernorm_is_rms),
     norm_epsilon(_norm_epsilon),
     q_proj(_q_proj),
     k_proj(_k_proj),
     v_proj(_v_proj),
     o_proj(_o_proj),
     temp_state(_temp_state),
 //     temp_q(_temp_q),
@@ -99,61 +109,100 @@
 //     temp_v(_temp_v),
     temp_dq(_temp_dq),
     max_rows(_max_rows),
     hidden_size(_hidden_size),
     num_heads(_num_heads),
     num_kv_heads(_num_kv_heads),
     head_dim(_head_dim),
-    max_seq_len(_max_seq_len)
+    max_seq_len(_max_seq_len),
+    has_residual(_has_residual),
+    rope_style(_rope_style),
+    q_norm(_q_norm),
+    k_norm(_k_norm)
 {
 }
 
 QAttn::~QAttn()
 {
 }
 
 void QAttn::forward_cuda_1
 (
     cublasHandle_t cublas_handle,
     half* x,
     int batch_size,
     int q_len,
     int past_len,
-    const uint32_t* past_lens,
+    const int32_t* past_lens,
     half* temp_q,
     half* temp_k,
     half* temp_v,
     const half* sin,
     const half* cos,
     const std::vector<uintptr_t>& loras,
     half* lora_temp
 )
 {
-    rms_norm_cuda(x, layernorm, temp_state, norm_epsilon, q_len * batch_size, hidden_size);
+    half* norm_state = x;
 
-    gemm_half_q_half_cuda(cublas_handle, temp_state, q_proj, temp_q, q_len * batch_size, q_proj->width, hidden_size, true, temp_dq);
-    gemm_half_q_half_cuda(cublas_handle, temp_state, k_proj, temp_k, q_len * batch_size, k_proj->width, hidden_size, true, temp_dq);
-    gemm_half_q_half_cuda(cublas_handle, temp_state, v_proj, temp_v, q_len * batch_size, v_proj->width, hidden_size, true, temp_dq);
-
-    apply_loras_cuda(cublas_handle, q_proj_lora, loras, q_proj, temp_state, temp_q, lora_temp, q_len * batch_size);
-    apply_loras_cuda(cublas_handle, k_proj_lora, loras, k_proj, temp_state, temp_k, lora_temp, q_len * batch_size);
-    apply_loras_cuda(cublas_handle, v_proj_lora, loras, v_proj, temp_state, temp_v, lora_temp, q_len * batch_size);
+    if (layernorm)
+    {
+        if (layernorm_is_rms)
+            rms_norm_cuda(x, layernorm, temp_state, norm_epsilon, q_len * batch_size, hidden_size);
+        else
+            layer_norm_cuda(x, layernorm, layernorm_bias, temp_state, norm_epsilon, q_len * batch_size, hidden_size);
+        norm_state = temp_state;
+    }
+
+    gemm_half_q_half_cuda(cublas_handle, norm_state, q_proj, temp_q, q_len * batch_size, q_proj->width, hidden_size, true, temp_dq);
+    gemm_half_q_half_cuda(cublas_handle, norm_state, k_proj, temp_k, q_len * batch_size, k_proj->width, hidden_size, true, temp_dq);
+    gemm_half_q_half_cuda(cublas_handle, norm_state, v_proj, temp_v, q_len * batch_size, v_proj->width, hidden_size, true, temp_dq);
+
+    if (q_norm)
+        head_norm_cuda(temp_q, q_norm, NULL, temp_q, norm_epsilon, q_len * batch_size, num_heads, head_dim);
+
+    if (k_norm)
+        head_norm_cuda(temp_k, k_norm, NULL, temp_k, norm_epsilon, q_len * batch_size, num_kv_heads, head_dim);
 
-    rope_cuda(temp_q, sin, cos, batch_size, q_len * num_heads,    head_dim, num_heads,    past_len, past_lens);
-    rope_cuda(temp_k, sin, cos, batch_size, q_len * num_kv_heads, head_dim, num_kv_heads, past_len, past_lens);
+    apply_loras_cuda(cublas_handle, q_proj_lora, loras, q_proj, norm_state, temp_q, lora_temp, q_len * batch_size);
+    apply_loras_cuda(cublas_handle, k_proj_lora, loras, k_proj, norm_state, temp_k, lora_temp, q_len * batch_size);
+    apply_loras_cuda(cublas_handle, v_proj_lora, loras, v_proj, norm_state, temp_v, lora_temp, q_len * batch_size);
+
+//    rope_cuda(temp_q, sin, cos, batch_size, q_len * num_heads,    head_dim, num_heads,    past_len, past_lens);
+//    rope_cuda(temp_k, sin, cos, batch_size, q_len * num_kv_heads, head_dim, num_kv_heads, past_len, past_lens);
+
+    if (rope_style != ROPE_STYLE_NONE)
+    {
+        rope_cuda_qk
+        (
+            temp_q,
+            temp_k,
+            sin,
+            cos,
+            batch_size,
+            q_len * num_heads,
+            q_len * num_kv_heads,
+            head_dim,
+            num_heads,
+            num_kv_heads,
+            past_len,
+            past_lens,
+            rope_style == ROPE_STYLE_NEOX
+        );
+    }
 }
 
 void QAttn::forward_cuda_2
 (
     cublasHandle_t cublas_handle,
     const half* attn_output,
     half* hidden_state,
     int q_len,
     int batch_size,
     const std::vector<uintptr_t>& loras,
     half* lora_temp
 )
 {
-    gemm_half_q_half_cuda(cublas_handle, attn_output, o_proj, hidden_state, q_len * batch_size, o_proj->width, hidden_size, false, temp_dq);
+    gemm_half_q_half_cuda(cublas_handle, attn_output, o_proj, hidden_state, q_len * batch_size, o_proj->width, o_proj->height, !has_residual, temp_dq);
 
     apply_loras_cuda(cublas_handle, o_proj_lora, loras, o_proj, attn_output, hidden_state, lora_temp, q_len * batch_size);
 }
```

## exllamav2/exllamav2_ext/cuda/q_attn.cuh

```diff
@@ -5,21 +5,30 @@
 #include <cuda_fp16.h>
 #include <cstdint>
 #include <cstdio>
 #include <ATen/cuda/CUDAContext.h>
 
 #include "q_matrix.cuh"
 
+#define ROPE_STYLE_NONE 0
+#define ROPE_STYLE_GPTJ 1
+#define ROPE_STYLE_NEOX 2
+
 class QAttn
 {
 public:
 
     half* layernorm;
+    half* layernorm_bias;
+    bool layernorm_is_rms;
     float norm_epsilon;
 
+    half* q_norm;
+    half* k_norm;
+
     QMatrix* q_proj;
     QMatrix* k_proj;
     QMatrix* v_proj;
     QMatrix* o_proj;
 
     half* temp_state;
 //     half* temp_q;
@@ -36,17 +45,22 @@
     int max_seq_len;
 
     std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> q_proj_lora;
     std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> k_proj_lora;
     std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> v_proj_lora;
     std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> o_proj_lora;
 
+    bool has_residual;
+    int rope_style;
+
     QAttn
     (
         half* _layernorm,
+        half* _layermorm_bias,
+        bool _layernorm_is_rms,
         float _norm_epsilon,
         QMatrix* _q_proj,
         QMatrix* _k_proj,
         QMatrix* _v_proj,
         QMatrix* _o_proj,
         half* _temp_state,
 //         half* _temp_q,
@@ -54,27 +68,31 @@
 //         half* _temp_v,
         half* _temp_dq,
         int _max_rows,
         int _hidden_size,
         int _num_heads,
         int _num_kv_heads,
         int _head_dim,
-        int _max_seq_len
+        int _max_seq_len,
+        bool _has_residual,
+        int _rope_style,
+        half* _q_norm,
+        half* _k_norm
     );
 
     ~QAttn();
 
     void forward_cuda_1
     (
         cublasHandle_t cublas_handle,
         half* x,
         int batch_size,
         int q_len,
         int past_len,
-        const uint32_t* past_lens,
+        const int32_t* past_lens,
         half* temp_q,
         half* temp_k,
         half* temp_v,
         const half* sin,
         const half* cos,
         const std::vector<uintptr_t>& loras,
         half* lora_temp
```

## exllamav2/exllamav2_ext/cuda/q_gemm.cu

```diff
@@ -1,104 +1,191 @@
 #include "q_gemm.cuh"
 #include "util.cuh"
-#include "matrix_view.cuh"
 #include "../config.h"
 
-#include "quant/qdq_2.cuh"
-#include "quant/qdq_3.cuh"
-#include "quant/qdq_4.cuh"
-#include "quant/qdq_5.cuh"
-#include "quant/qdq_6.cuh"
-#include "quant/qdq_8.cuh"
-
-#define BLOCK_KN_SIZE 128
-#define BLOCK_M_SIZE_MAX 8
-#define MAX_GROUPS_IN_BLOCK (BLOCK_KN_SIZE / 32)
 #define CLEAR_N_SIZE 256
 
-#include "q_gemm_kernel.cuh"
-#include "q_gemm_kernel_gptq.cuh"
+#include "comp_units/kernel_select.cuh"
+#include "q_gemm_autotune.cuh"
+#include "h_add.cuh"
 
 void gemm_half_q_half_cuda_part
 (
     const half* a,
     QMatrix* b,
     half* c,
     int size_m,
     int size_n,
     int size_k,
     int m_count,
-    bool clear
+    bool clear,
+    const half* r_weights,
+    int r_weights_stride,
+    bool mul_r_weights
 )
 {
     if (!b->is_gptq)
     {
+        int block_kn_size;
+        bool measure;
+        AT_Result* atr;
+        cudaEvent_t start, stop;
+
+        bool use_autotune = true;
+
+        if (!use_autotune)
+        {
+            block_kn_size = at_get_fallback_blocksize(b->device, size_m, size_n, size_k);
+        }
+        else
+        {
+            // Only autotune up to EXL2_BLOCK_M_SIZE_MAX
+
+            if (size_m > EXL2_BLOCK_M_SIZE_MAX)
+            {
+                atr = at_get_top(b->device, size_k, size_n);
+                if (atr && atr->best) block_kn_size = atr->best;
+                else block_kn_size = at_get_fallback_blocksize(b->device, size_m, size_n, size_k);
+                measure = false;
+            }
+
+            // Use autotuned size or prepare measurement
+
+            else
+            {
+                atr = at_get(b->device, size_m, size_k, size_n);
+                if (atr->best)
+                {
+                    block_kn_size = atr->best;
+                    measure = false;
+                }
+                else
+                {
+                    measure = true;
+                    int c32 = atr->timings_32.size();
+                    int c64 = atr->timings_64.size();
+                    if (c32 + c64 == AT_NUM_MEASURE)
+                    {
+                        at_select(atr);
+                        block_kn_size = atr->best;
+                        measure = false;
+                    }
+                    else
+                    {
+                        block_kn_size = c32 < c64 ? 32 : 64;
+                        measure = true;
+                    }
+                }
+            }
+        }
+
+        // Prepare kernel
+
         dim3 blockDim, gridDim;
-        blockDim.x = BLOCK_KN_SIZE;
+        blockDim.x = block_kn_size;
         blockDim.y = 1;
         blockDim.z = 1;
-        gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
+        gridDim.x = DIVIDE(size_n, block_kn_size * 4);
         gridDim.y = DIVIDE(size_m, m_count);
-        gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
+        gridDim.z = DIVIDE(size_k, block_kn_size);
+
+        int max_m = min(EXL2_BLOCK_M_SIZE_MAX, size_m);
 
-        fp_gemm_half_q_half_kernel kernel = pick_gemm_half_q_half_kernel(true, m_count);
+        fp_gemm_half_q_half_kernel kernel = pick_gemm_half_q_half_kernel(max_m, b->kernel_p, r_weights != NULL, mul_r_weights, block_kn_size);
+        if (!kernel) return;
+
+        // Measurement events
+
+        if (measure)
+        {
+            cudaEventCreate(&start);
+            cudaEventCreate(&stop);
+            cudaEventRecord(start);
+        }
+
+        // Launch kernel
 
         kernel<<<gridDim, blockDim>>>
         (
             a,
             b->cuda_q_weight,
             b->cuda_q_scale,
             b->cuda_q_scale_max,
             c,
             size_m,
             size_n,
             size_k,
             b->groups,
-            b->groupsize,
+            b->cuda_q_group_map,
             b->cuda_q_perm,
             b->rows_8,
             b->rows_6,
             b->rows_5,
             b->rows_4,
             b->rows_3,
             b->rows_2,
-            clear
+            clear,
+            r_weights,
+            r_weights_stride
         );
+
+        // Finish measurement
+
+        if (measure)
+        {
+            cudaEventRecord(stop);
+            cudaEventSynchronize(stop);
+            float timing = 0.0f;
+            cudaEventElapsedTime(&timing, start, stop);
+            cudaEventDestroy(start);
+            cudaEventDestroy(stop);
+            if (block_kn_size == 32) atr->timings_32.push_back(timing);
+            if (block_kn_size == 64) atr->timings_64.push_back(timing);
+        }
     }
+
+    // GPTQ kernel
+
     else
     {
         dim3 blockDim, gridDim;
-        blockDim.x = BLOCK_KN_SIZE;
+        blockDim.x = GPTQ_BLOCK_KN_SIZE;
         blockDim.y = 1;
         blockDim.z = 1;
-        gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);
-        gridDim.y = DIVIDE(size_m, m_count);
-        gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);
-
-        fp_gemm_half_q_half_gptq_kernel kernel = pick_gemm_half_q_half_gptq_kernel(true, m_count);
-
-//         DBGX((uint64_t) b->cuda_q_perm);
-//         DBGI(b->rows_4);
-//         DBGI(b->height);
+        gridDim.x = DIVIDE(size_n, GPTQ_BLOCK_KN_SIZE * 4);
+        gridDim.y = DIVIDE(size_m, GPTQ_BLOCK_M_SIZE_MAX);
+        gridDim.z = DIVIDE(size_k, GPTQ_BLOCK_KN_SIZE);
+
+        int max_m = min(GPTQ_BLOCK_M_SIZE_MAX, size_m);
+
+        fp_gemm_half_q_half_gptq_kernel kernel = pick_gemm_half_q_half_gptq_kernel(GPTQ_BLOCK_M_SIZE_MAX, r_weights != NULL, mul_r_weights);
+        if (!kernel) return;
+
+//         DBGX((uint64_t) r_weights);
+//         if (r_weights)
+//             print_global_mem(r_weights, 1, 1, 1);
+//         DBGI(r_weights_stride);
 
         kernel<<<gridDim, blockDim>>>
         (
             a,
             b->cuda_q_weight,
             b->cuda_gptq_qzeros,
             b->cuda_gptq_scales,
             c,
             size_m,
             size_n,
             size_k,
             b->groups,
-            b->groupsize,
+            b->gptq_groupsize,
             b->cuda_q_perm,
             b->rows_4,
-            clear
+            clear,
+            r_weights,
+            r_weights_stride
         );
     }
 }
 
 void gemm_half_q_half_cuda
 (
     cublasHandle_t cublas_handle,
@@ -106,80 +193,94 @@
     QMatrix* b,
     half* c,
     int size_m,
     int size_n,
     int size_k,
     bool clear,
     half* temp_dq,
-    bool force_cuda
+    bool force_cuda,
+    const half* r_weights,
+    const int r_weights_stride,
+    bool mul_r_weights
 )
 {
-    if (size_m > MAX_Q_GEMM_ROWS && !force_cuda)
-    {
-        //printf("cublas\n");
-
-        // Reconstruct FP16 matrix, then cuBLAS
+    // Here we force CUDA matmul for matrices that are too big to dequantize. This is necessary for the
+    // extremely large output layers of some models. Splitting along K and dequantizing/multiplying in
+    // chunks would work also except the remapping of EXL2 matrices complicates it.
+    //
+    // TODO: Finish the chunking stuff
 
-        if (!temp_dq) temp_dq = b->temp_dq;
-        b->reconstruct(temp_dq);
+    int row_step = (b->max_dq_rows / 128) * 128;
 
-        //cublasSetMathMode(cublas_handle, CUBLAS_TENSOR_OP_MATH);
+    if (size_m > MAX_Q_GEMM_ROWS && !force_cuda && size_k <= row_step)
+    {
+        int row_b = 0;
+        if (row_step == 0) row_step = size_k;
 
-        const half alpha = __float2half(1.0f);
-        const half beta = clear ? __float2half(0.0f) : __float2half(1.0f);
-        cublasHgemm(cublas_handle,
-                    CUBLAS_OP_N,
-                    CUBLAS_OP_N,
-                    size_n, size_m, size_k,
-                    &alpha, temp_dq, size_n,
-                            a,       size_k,
-                    &beta,  c,       size_n);
+        while (row_b < size_k)
+        {
+            int row_a = row_b;
+            row_b += row_step;
+            row_b = min(row_b, size_k);
+            int chunk_k = row_b - row_a;
+
+            // Reconstruct FP16 matrix, then cuBLAS
+
+            if (!temp_dq) temp_dq = b->temp_dq;
+            b->reconstruct(temp_dq, row_a, row_b);
+
+            const half alpha = __float2half(1.0f);
+            const half beta = (clear && row_a == 0) ? __float2half(0.0f) : __float2half(1.0f);
+            cublasHgemm(cublas_handle,
+                        CUBLAS_OP_N,
+                        CUBLAS_OP_N,
+                        size_n, size_m, chunk_k,
+                        &alpha, temp_dq,   size_n,
+                                a + row_a, size_k,
+                        &beta,  c,         size_n);
+
+//            cublasHgemm(cublas_handle,
+//                        CUBLAS_OP_N,
+//                        CUBLAS_OP_N,
+//                        size_n, size_m, size_k,
+//                        &alpha, temp_dq, size_n,
+//                                a,       size_k,
+//                        &beta,  c,       size_n);
+        }
 
         //const float alpha = 1.0f;
         //const float beta = clear ? 0.0f : 1.0f;
         //cublasSgemmEx(cublas_handle,
-        //              CUBLAS_OP_N,
-        //              CUBLAS_OP_N,
-        //              size_n, size_m, size_k,
-        //              &alpha, temp_dq, CUDA_R_16F, size_n,
-        //                      a,       CUDA_R_16F, size_k,
-        //              &beta,  c,       CUDA_R_16F, size_n);
+        //             CUBLAS_OP_N,
+        //             CUBLAS_OP_N,
+        //             size_n, size_m, size_k,
+        //             &alpha, temp_dq, CUDA_R_16F, size_n,
+        //                     a,       CUDA_R_16F, size_k,
+        //             &beta,  c,       CUDA_R_16F, size_n);
 
         //const float alpha = 1.0f;
         //const float beta = clear ? 0.0f : 1.0f;
         //cublasGemmEx(cublas_handle,
         //             CUBLAS_OP_N, CUBLAS_OP_N,
         //             size_n, size_m, size_k,
         //             &alpha, temp_dq, CUDA_R_16F, size_n,
         //                     a,       CUDA_R_16F, size_k,
         //             &beta,  c,       CUDA_R_16F, size_n,
         //             CUDA_R_16F, CUBLAS_GEMM_DFALT_TENSOR_OP);
     }
     else
     {
-        //printf("cuda\n");
-
         // Quantized matmul
 
-        //if (clear) clear_tensor_cuda(c, size_m, size_n);
-
-        int max_chunks = size_m / BLOCK_M_SIZE_MAX;
-        int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;
-        int last_chunk_size = size_m - last_chunk;
-
-        if (max_chunks)
-        {
-            gemm_half_q_half_cuda_part(a, b, c, last_chunk, size_n, size_k, BLOCK_M_SIZE_MAX, clear);
-        }
-
-        if (last_chunk_size)
-        {
-            gemm_half_q_half_cuda_part(a + last_chunk * size_k, b, c + last_chunk * size_n, last_chunk_size, size_n, size_k, last_chunk_size, clear);
-        }
+        int block_m_size_max = b->is_gptq ? GPTQ_BLOCK_M_SIZE_MAX : EXL2_BLOCK_M_SIZE_MAX;
+        int block_m = min(size_m, block_m_size_max);
+        gemm_half_q_half_cuda_part(a, b, c, size_m, size_n, size_k, block_m, clear, r_weights, r_weights_stride, mul_r_weights);
     }
+
+    if (b->cuda_bias) cuda_vector_add_(c, b->cuda_bias, size_m, size_n);
 }
 
 __global__ void clear_kernel
 (
     half* __restrict__ c,
     const int size_m,
     const int size_n
@@ -195,15 +296,14 @@
 void clear_tensor_cuda
 (
     half* c,
     int size_m,
     int size_n
 )
 {
-    return;
-    dim3 blockDim, gridDim;
-    blockDim.x = CLEAR_N_SIZE;
-    blockDim.y = 1;
-    gridDim.x = DIVIDE(size_n / 8, CLEAR_N_SIZE);
-    gridDim.y = size_m;
-    clear_kernel<<<gridDim, blockDim>>>(c, size_m, size_n);
+//     dim3 blockDim, gridDim;
+//     blockDim.x = CLEAR_N_SIZE;
+//     blockDim.y = 1;
+//     gridDim.x = DIVIDE(size_n / 8, CLEAR_N_SIZE);
+//     gridDim.y = size_m;
+//     clear_kernel<<<gridDim, blockDim>>>(c, size_m, size_n);
 }
```

## exllamav2/exllamav2_ext/cuda/q_gemm.cuh

```diff
@@ -16,15 +16,18 @@
     QMatrix* b,
     half* c,
     int size_m,
     int size_n,
     int size_k,
     bool clear = false,
     half* reconstruct = NULL,
-    bool force_cuda = false
+    bool force_cuda = false,
+    const half* r_weights = NULL,
+    const int r_weights_stride = 0,
+    bool mul_r_weights = false
 );
 
 void clear_tensor_cuda
 (
     half* c,
     int size_m,
     int size_n
```

## exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh

```diff
@@ -1,8 +1,21 @@
 #include "compat.cuh"
+#include "../config.h"
+#include "matrix_view.cuh"
+#include "quant/qdq_2.cuh"
+#include "quant/qdq_3.cuh"
+#include "quant/qdq_4.cuh"
+#include "quant/qdq_5.cuh"
+#include "quant/qdq_6.cuh"
+#include "quant/qdq_8.cuh"
+
+#define EXL2_BLOCK_M_SIZE_MAX MAX_Q_GEMM_ROWS_KERNEL
+
+//#define EXL2_BLOCK_KN_SIZE 32
+//#define EXL2_MAX_GROUPS_IN_BLOCK (EXL2_BLOCK_KN_SIZE / 32)
 
 __forceinline__ __device__ half2 dot22_8(half2(&dq)[4], const half* a_ptr, const half2 g_result, const half qs_h)
 {
     half2 result = {};
     const half2* a2_ptr = (const half2*)a_ptr;
     #pragma unroll
     for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);
@@ -53,127 +66,199 @@
     const half2* a2_ptr = (const half2*)a_ptr;
     #pragma unroll
     for (int i = 0; i < 16; i += 1) result = __hfma2(dq[i], *a2_ptr++, result);
     float result_f = __half2float(__low2half(result)) + __half2float(__high2half(result));
     return fma(result_f, qs_f, g_result);
 }
 
+__forceinline__ __device__ half dot22_8_h(half2(&dq)[4], const half* a_ptr, const half g_result, const half qs_h)
+{
+    // Use FP32 accumulator to avoid potential overflow since unscaled weights are in the range -128..127
+
+    float result = {};
+    #pragma unroll
+    for (int i = 0; i < 4; i++)
+    {
+        half2 w01 = dq[i];
+        float w0 = __low2float(w01);
+        float w1 = __high2float(w01);
+        float x0 = __half2float(*a_ptr++);
+        float x1 = __half2float(*a_ptr++);
+        result = fma(w0, x0, result);
+        result = fma(w1, x1, result);
+    }
+    float qs = __half2float(qs_h);
+    result *= qs;
+    half result_h = __float2half_rn(result);
+    return __hadd(result_h, g_result);
+}
+
+__forceinline__ __device__ half dot22_16_h(half2(&dq)[8], const half* a_ptr, const half g_result, const half qs_h)
+{
+    half2 result = {};
+    const half2* a2_ptr = (const half2*)a_ptr;
+    #pragma unroll
+    for (int i = 0; i < 8; i++) result = __hfma2(dq[i], *a2_ptr++, result);
+    half result_h = __hadd(__low2half(result), __high2half(result));
+    return __hfma(result_h, qs_h, g_result);
+}
+
+__forceinline__ __device__ half dot22_32_h(half2(&dq)[16], const half* a_ptr, const half g_result, const half qs_h)
+{
+    half2 result = {};
+    const half2* a2_ptr = (const half2*)a_ptr;
+    #pragma unroll
+    for (int i = 0; i < 16; i += 1) result = __hfma2(dq[i], *a2_ptr++, result);
+    half result_h = __hadd(__low2half(result), __high2half(result));
+    return __hfma(result_h, qs_h, g_result);
+}
 
 
 typedef void (*fp_gemm_half_q_half_kernel)
 (
     const half*,
     const uint32_t*,
     const uint32_t*,
     const half*,
     half*,
     const int,
     const int,
     const int,
     const int,
-    const int,
+    const uint16_t*,
     const uint16_t*,
     const int,
     const int,
     const int,
     const int,
     const int,
     const int,
-    const bool
+    const bool,
+    const half*,
+    const int
 );
 
-template <bool first_block, int m_count>
+template <int m_count, int kernel_p, bool use_r_weights, bool mul_r_weights, int block_kn_size>
 __global__ void gemm_half_q_half_kernel
 (
     const half*      __restrict__ a,
     const uint32_t*  __restrict__ b_q_weight,
     const uint32_t*  __restrict__ b_q_scale,
     const half*      __restrict__ b_q_scale_max,
     half*            __restrict__ c,
     const int size_m,
     const int size_n,
     const int size_k,
     const int groups,
-    const int groupsize,
+    const uint16_t* __restrict__ b_q_group_map,
     const uint16_t* __restrict__ b_q_perm,
     const int rows_8,
     const int rows_6,
     const int rows_5,
     const int rows_4,
     const int rows_3,
     const int rows_2,
-    const bool clear
+    const bool clear,
+    const half* r_weights,
+    const int r_weights_stride
 )
 {
     MatrixView_half a_(a, size_m, size_k);
     MatrixView_half_rw c_(c, size_m, size_n);
     MatrixView_q4_row b_q_scale_(b_q_scale, groups, size_n);
 
     int t = threadIdx.x;
 
     // Block
 
-    int offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
+//    const int m_count = EXL2_BLOCK_M_SIZE_MAX;
+
+    int offset_n = blockIdx.x * block_kn_size * 4;
     int offset_m = blockIdx.y * m_count;
-    int offset_k = blockIdx.z * BLOCK_KN_SIZE;
+    int offset_k = blockIdx.z * block_kn_size;
+
+    int m_count_min = min(size_m - offset_m, m_count);
 
-    int end_n = min(offset_n + BLOCK_KN_SIZE * 4, size_n);
-    int end_m = min(offset_m + m_count, size_m);
-    int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);
+    int end_n = min(offset_n + block_kn_size * 4, size_n);
+    int end_m = min(offset_m + m_count_min, size_m);
+    int end_k = min(offset_k + block_kn_size, size_k);
     int n = offset_n + t * 4;
 
+    // Read weights
+
+    half_uint16 weights[MAX_Q_GEMM_WEIGHTS];
+    if constexpr (use_r_weights)
+    {
+        uint16_t any_w = 0;
+        const half* w_ptr = r_weights;
+        for (int m = 0; m < m_count_min; ++m)
+        {
+            weights[m].as_half = *w_ptr;
+            w_ptr += r_weights_stride;
+            any_w |= weights[m].as_uint16;
+        }
+        if (!any_w) return;  // Early exit if all weights are zero -- does not zero output (!!!)
+    }
+
     // Preload block_a
 
-    __shared__ half block_a[m_count][BLOCK_KN_SIZE];
+    __shared__ half block_a[m_count][block_kn_size];
 
     if (offset_k + t < end_k)
     {
-        for (int m = 0; m < m_count; ++m)
+        for (int m = 0; m < m_count_min; ++m)
         {
             const half* a_ptr = a_.item_ptr(offset_m + m, 0);
             half* block_a_ptr = block_a[m];
             half a0 = a_ptr[b_q_perm[offset_k + t]];
+//            half a0 = a_ptr[offset_k + t];
             block_a_ptr[t] = a0;
         }
     }
 
     // Clear
 
     if (n >= size_n) return;
 
     if (clear && blockIdx.z == 0) // && (threadIdx.x & 1) == 0)
     {
-        for (int m = 0; m < m_count; m++)
+        for (int m = 0; m < m_count_min; m++)
             *((uint64_t*) c_.item_ptr(offset_m + m, n)) = 0;
     }
 
     __syncthreads();
 
     // Find initial group
 
-    int group = offset_k / groupsize;
+    //int group = offset_k / groupsize;
+    int group = b_q_group_map[offset_k * 2];
+
+//    if (offset_m == 0 && t == 0)
+//        DBGI2(offset_k, group);
 
     // Preload scales
 
-    float scales[MAX_GROUPS_IN_BLOCK][4];
+    half scales[block_kn_size / 32][4];
 
-    int groups_in_block = DIVIDE((end_k - offset_k), groupsize);
-    for (int g = 0; g < groups_in_block; g++)
+    //int groups_in_block = DIVIDE((end_k - offset_k), groupsize);
+    int temp_k = offset_k;
+    for (int g = 0; temp_k < end_k; g++)
     {
         int qscales[4];
         b_q_scale_.item4(qscales, group + g, n);
         qscales[0]++;
         qscales[1]++;
         qscales[2]++;
         qscales[3]++;
-        float maxscale = __half2float(b_q_scale_max[group + g]);
-        scales[g][0] = __int2float_rn(qscales[0] * qscales[0]) * maxscale;
-        scales[g][1] = __int2float_rn(qscales[1] * qscales[1]) * maxscale;
-        scales[g][2] = __int2float_rn(qscales[2] * qscales[2]) * maxscale;
-        scales[g][3] = __int2float_rn(qscales[3] * qscales[3]) * maxscale;
+        half maxscale = b_q_scale_max[group + g];
+        scales[g][0] = __hmul(__int2half_rn(qscales[0] * qscales[0]), maxscale);
+        scales[g][1] = __hmul(__int2half_rn(qscales[1] * qscales[1]), maxscale);
+        scales[g][2] = __hmul(__int2half_rn(qscales[2] * qscales[2]), maxscale);
+        scales[g][3] = __hmul(__int2half_rn(qscales[3] * qscales[3]), maxscale);
+        temp_k += b_q_group_map[temp_k * 2 + 1];
     }
 
     // a, b offset
 
     int pre_rows_8 = min(rows_8, offset_k);
     int pre_rows_6 = offset_k > rows_8 ? min(rows_6, offset_k) - rows_8 : 0;
     int pre_rows_5 = offset_k > rows_6 ? min(rows_5, offset_k) - rows_6 : 0;
@@ -186,44 +271,45 @@
     qk += pre_rows_5 / 32 * 5;
     qk += pre_rows_4 / 32 * 4;
     qk += pre_rows_3 / 32 * 3;
     qk += pre_rows_2 / 32 * 2;
 
     const uint32_t* b_ptr = b_q_weight + qk * size_n + n;
     const half* a_ptr = &block_a[0][0];
-    int a_stride = BLOCK_KN_SIZE;
+    int a_stride = block_kn_size;
 
     // Initial group
 
     int scales_idx = 0;
-    float qs_f0 = scales[scales_idx][0];
-    float qs_f1 = scales[scales_idx][1];
-    float qs_f2 = scales[scales_idx][2];
-    float qs_f3 = scales[scales_idx][3];
-    int nextgroup = offset_k + groupsize;
+    half qs_h0 = scales[scales_idx][0];
+    half qs_h1 = scales[scales_idx][1];
+    half qs_h2 = scales[scales_idx][2];
+    half qs_h3 = scales[scales_idx][3];
+    int nextgroup = offset_k + b_q_group_map[offset_k * 2 + 1];
 
     // Column result
 
-    float block_c[m_count][4] = {};
+    half block_c[m_count][4] = {};
 
     // Dequantize groups
 
     int k = offset_k;
 
+    if constexpr (kernel_p & 0b10000000) {
     while (k < rows_8 && k < end_k)
     {
         if (k == nextgroup)
         {
             group++;
             scales_idx++;
-            qs_f0 = scales[scales_idx][0];
-            qs_f1 = scales[scales_idx][1];
-            qs_f2 = scales[scales_idx][2];
-            qs_f3 = scales[scales_idx][3];
-            nextgroup += groupsize;
+            qs_h0 = scales[scales_idx][0];
+            qs_h1 = scales[scales_idx][1];
+            qs_h2 = scales[scales_idx][2];
+            qs_h3 = scales[scales_idx][3];
+            nextgroup += b_q_group_map[k * 2 + 1];
         }
 
         #pragma unroll
         for (int j = 0; j < 4; j++)
         {
             int4 load_int4[2];
             load_int4[0] = *((int4*) b_ptr); b_ptr += size_n;
@@ -231,37 +317,39 @@
 
             half2 dq[4][4];
             dequant_8bit_8(load_int4[0].x, load_int4[1].x, dq[0], size_n);
             dequant_8bit_8(load_int4[0].y, load_int4[1].y, dq[1], size_n);
             dequant_8bit_8(load_int4[0].z, load_int4[1].z, dq[2], size_n);
             dequant_8bit_8(load_int4[0].w, load_int4[1].w, dq[3], size_n);
 
-            for (int m = 0; m < m_count; m++)
+            for (int m = 0; m < m_count_min; m++)
             {
-                block_c[m][0] = dot22_8_f(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_f0);
-                block_c[m][1] = dot22_8_f(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_f1);
-                block_c[m][2] = dot22_8_f(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_f2);
-                block_c[m][3] = dot22_8_f(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_f3);
+                if constexpr (use_r_weights) { if (!weights[m].as_uint16) continue; }
+                block_c[m][0] = dot22_8_h(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_h0);
+                block_c[m][1] = dot22_8_h(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_h1);
+                block_c[m][2] = dot22_8_h(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_h2);
+                block_c[m][3] = dot22_8_h(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_h3);
             }
             a_ptr += 8;
         }
         k += 32;
-    }
+    }}
 
+    if constexpr (kernel_p & 0b00100000) {
     while (k < rows_6 && k < end_k)
     {
         if (k == nextgroup)
         {
             group++;
             scales_idx++;
-            qs_f0 = scales[scales_idx][0];
-            qs_f1 = scales[scales_idx][1];
-            qs_f2 = scales[scales_idx][2];
-            qs_f3 = scales[scales_idx][3];
-            nextgroup += groupsize;
+            qs_h0 = scales[scales_idx][0];
+            qs_h1 = scales[scales_idx][1];
+            qs_h2 = scales[scales_idx][2];
+            qs_h3 = scales[scales_idx][3];
+            nextgroup += b_q_group_map[k * 2 + 1];
         }
 
         #pragma unroll
         for (int j = 0; j < 2; j++)
         {
             int4 load_int4[3];
             load_int4[0] = *((int4*) b_ptr); b_ptr += size_n;
@@ -270,37 +358,39 @@
 
             half2 dq[4][8];
             dequant_6bit_16(load_int4[0].x, load_int4[1].x, load_int4[2].x, dq[0], size_n);
             dequant_6bit_16(load_int4[0].y, load_int4[1].y, load_int4[2].y, dq[1], size_n);
             dequant_6bit_16(load_int4[0].z, load_int4[1].z, load_int4[2].z, dq[2], size_n);
             dequant_6bit_16(load_int4[0].w, load_int4[1].w, load_int4[2].w, dq[3], size_n);
 
-            for (int m = 0; m < m_count; m++)
+            for (int m = 0; m < m_count_min; m++)
             {
-                block_c[m][0] = dot22_16_f(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_f0);
-                block_c[m][1] = dot22_16_f(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_f1);
-                block_c[m][2] = dot22_16_f(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_f2);
-                block_c[m][3] = dot22_16_f(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_f3);
+                if constexpr (use_r_weights) { if (!weights[m].as_uint16) continue; }
+                block_c[m][0] = dot22_16_h(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_h0);
+                block_c[m][1] = dot22_16_h(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_h1);
+                block_c[m][2] = dot22_16_h(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_h2);
+                block_c[m][3] = dot22_16_h(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_h3);
             }
             a_ptr += 16;
         }
         k += 32;
-    }
+    }}
 
+    if constexpr (kernel_p & 0b00010000) {
     while (k < rows_5 && k < end_k)
     {
         if (k == nextgroup)
         {
             group++;
             scales_idx++;
-            qs_f0 = scales[scales_idx][0];
-            qs_f1 = scales[scales_idx][1];
-            qs_f2 = scales[scales_idx][2];
-            qs_f3 = scales[scales_idx][3];
-            nextgroup += groupsize;
+            qs_h0 = scales[scales_idx][0];
+            qs_h1 = scales[scales_idx][1];
+            qs_h2 = scales[scales_idx][2];
+            qs_h3 = scales[scales_idx][3];
+            nextgroup += b_q_group_map[k * 2 + 1];
         }
 
         #pragma unroll
         for (int j = 0; j < 1; j++)
         {
             int4 load_int4[5];
             load_int4[0] = *((int4*) b_ptr); b_ptr += size_n;
@@ -311,75 +401,79 @@
 
             half2 dq[4][16];
             dequant_5bit_32(load_int4[0].x, load_int4[1].x, load_int4[2].x, load_int4[3].x, load_int4[4].x, dq[0], size_n);
             dequant_5bit_32(load_int4[0].y, load_int4[1].y, load_int4[2].y, load_int4[3].y, load_int4[4].y, dq[1], size_n);
             dequant_5bit_32(load_int4[0].z, load_int4[1].z, load_int4[2].z, load_int4[3].z, load_int4[4].z, dq[2], size_n);
             dequant_5bit_32(load_int4[0].w, load_int4[1].w, load_int4[2].w, load_int4[3].w, load_int4[4].w, dq[3], size_n);
 
-            for (int m = 0; m < m_count; m++)
+            for (int m = 0; m < m_count_min; m++)
             {
-                block_c[m][0] = dot22_32_f(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_f0);
-                block_c[m][1] = dot22_32_f(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_f1);
-                block_c[m][2] = dot22_32_f(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_f2);
-                block_c[m][3] = dot22_32_f(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_f3);
+                if constexpr (use_r_weights) { if (!weights[m].as_uint16) continue; }
+                block_c[m][0] = dot22_32_h(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_h0);
+                block_c[m][1] = dot22_32_h(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_h1);
+                block_c[m][2] = dot22_32_h(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_h2);
+                block_c[m][3] = dot22_32_h(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_h3);
             }
             a_ptr += 32;
         }
 
         k += 32;
-    }
+    }}
 
+    if constexpr (kernel_p & 0b00001000) {
     while (k < rows_4 && k < end_k)
     {
         if (k == nextgroup)
         {
             group++;
             scales_idx++;
-            qs_f0 = scales[scales_idx][0];
-            qs_f1 = scales[scales_idx][1];
-            qs_f2 = scales[scales_idx][2];
-            qs_f3 = scales[scales_idx][3];
-            nextgroup += groupsize;
+            qs_h0 = scales[scales_idx][0];
+            qs_h1 = scales[scales_idx][1];
+            qs_h2 = scales[scales_idx][2];
+            qs_h3 = scales[scales_idx][3];
+            nextgroup += b_q_group_map[k * 2 + 1];
         }
 
         #pragma unroll
         for (int j = 0; j < 4; j++)
         {
             int4 load_int4[1];
             load_int4[0] = *((int4*) b_ptr); b_ptr += size_n;
 
             half2 dq[4][4];
             dequant_4bit_8(load_int4[0].x, dq[0], size_n);
             dequant_4bit_8(load_int4[0].y, dq[1], size_n);
             dequant_4bit_8(load_int4[0].z, dq[2], size_n);
             dequant_4bit_8(load_int4[0].w, dq[3], size_n);
 
-            for (int m = 0; m < m_count; m++)
+            for (int m = 0; m < m_count_min; m++)
             {
-                block_c[m][0] = dot22_8_f(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_f0);
-                block_c[m][1] = dot22_8_f(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_f1);
-                block_c[m][2] = dot22_8_f(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_f2);
-                block_c[m][3] = dot22_8_f(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_f3);
+                if constexpr (use_r_weights) { if (!weights[m].as_uint16) continue; }
+                block_c[m][0] = dot22_8_h(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_h0);
+                block_c[m][1] = dot22_8_h(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_h1);
+                block_c[m][2] = dot22_8_h(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_h2);
+                block_c[m][3] = dot22_8_h(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_h3);
             }
             a_ptr += 8;
         }
         k += 32;
-    }
+    }}
 
+    if constexpr (kernel_p & 0b00000100) {
     while (k < rows_3 && k < end_k)
     {
         if (k == nextgroup)
         {
             group++;
             scales_idx++;
-            qs_f0 = scales[scales_idx][0];
-            qs_f1 = scales[scales_idx][1];
-            qs_f2 = scales[scales_idx][2];
-            qs_f3 = scales[scales_idx][3];
-            nextgroup += groupsize;
+            qs_h0 = scales[scales_idx][0];
+            qs_h1 = scales[scales_idx][1];
+            qs_h2 = scales[scales_idx][2];
+            qs_h3 = scales[scales_idx][3];
+            nextgroup += b_q_group_map[k * 2 + 1];
         }
 
         #pragma unroll
         for (int j = 0; j < 1; j++)
         {
             int4 load_int4[3];
             load_int4[0] = *((int4*) b_ptr); b_ptr += size_n;
@@ -388,97 +482,82 @@
 
             half2 dq[4][16];
             dequant_3bit_32(load_int4[0].x, load_int4[1].x, load_int4[2].x, dq[0], size_n);
             dequant_3bit_32(load_int4[0].y, load_int4[1].y, load_int4[2].y, dq[1], size_n);
             dequant_3bit_32(load_int4[0].z, load_int4[1].z, load_int4[2].z, dq[2], size_n);
             dequant_3bit_32(load_int4[0].w, load_int4[1].w, load_int4[2].w, dq[3], size_n);
 
-            for (int m = 0; m < m_count; m++)
+            for (int m = 0; m < m_count_min; m++)
             {
-                block_c[m][0] = dot22_32_f(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_f0);
-                block_c[m][1] = dot22_32_f(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_f1);
-                block_c[m][2] = dot22_32_f(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_f2);
-                block_c[m][3] = dot22_32_f(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_f3);
+                if constexpr (use_r_weights) { if (!weights[m].as_uint16) continue; }
+                block_c[m][0] = dot22_32_h(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_h0);
+                block_c[m][1] = dot22_32_h(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_h1);
+                block_c[m][2] = dot22_32_h(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_h2);
+                block_c[m][3] = dot22_32_h(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_h3);
             }
             a_ptr += 32;
         }
         k += 32;
-    }
+    }}
 
+    if constexpr (kernel_p & 0b00000010) {
     while (k < rows_2 && k < end_k)
     {
         if (k == nextgroup)
         {
             group++;
             scales_idx++;
-            qs_f0 = scales[scales_idx][0];
-            qs_f1 = scales[scales_idx][1];
-            qs_f2 = scales[scales_idx][2];
-            qs_f3 = scales[scales_idx][3];
-            nextgroup += groupsize;
+            qs_h0 = scales[scales_idx][0];
+            qs_h1 = scales[scales_idx][1];
+            qs_h2 = scales[scales_idx][2];
+            qs_h3 = scales[scales_idx][3];
+            nextgroup += b_q_group_map[k * 2 + 1];
         }
 
         #pragma unroll
-        for (int j = 0; j < 2; j++)
+        for (int j = 0; j < 1; j++)
         {
             int4 load_int4[1];
             load_int4[0] = *((int4*) b_ptr); b_ptr += size_n;
 
             half2 dq[4][8];
             dequant_2bit_16(load_int4[0].x, dq[0], size_n);
             dequant_2bit_16(load_int4[0].y, dq[1], size_n);
             dequant_2bit_16(load_int4[0].z, dq[2], size_n);
             dequant_2bit_16(load_int4[0].w, dq[3], size_n);
 
-            for (int m = 0; m < m_count; m++)
+            for (int m = 0; m < m_count_min; m++)
             {
-                block_c[m][0] = dot22_16_f(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_f0);
-                block_c[m][1] = dot22_16_f(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_f1);
-                block_c[m][2] = dot22_16_f(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_f2);
-                block_c[m][3] = dot22_16_f(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_f3);
+                if constexpr (use_r_weights) { if (!weights[m].as_uint16) continue; }
+                block_c[m][0] = dot22_16_h(dq[0], a_ptr + m * a_stride, block_c[m][0], qs_h0);
+                block_c[m][1] = dot22_16_h(dq[1], a_ptr + m * a_stride, block_c[m][1], qs_h1);
+                block_c[m][2] = dot22_16_h(dq[2], a_ptr + m * a_stride, block_c[m][2], qs_h2);
+                block_c[m][3] = dot22_16_h(dq[3], a_ptr + m * a_stride, block_c[m][3], qs_h3);
             }
 
             a_ptr += 16;
         }
-        k += 32;
-    }
+        k += 16;
+    }}
 
     // Accumulate column sums in c
 
-    for (int m = 0; m < m_count; m++)
+    for (int m = 0; m < m_count_min; m++)
     {
         half2* out = (half2*)c_.item_ptr(offset_m + m, n);
-        half2 result01 = __halves2half2(__float2half_rn(block_c[m][0]), __float2half_rn(block_c[m][1]));
-        half2 result23 = __halves2half2(__float2half_rn(block_c[m][2]), __float2half_rn(block_c[m][3]));
+        half2 result01 = __halves2half2(block_c[m][0], block_c[m][1]);
+        half2 result23 = __halves2half2(block_c[m][2], block_c[m][3]);
+
+        if constexpr (mul_r_weights)
+        {
+            half2 w_mul2 = __half2half2(weights[m].as_half);
+            result01 = __hmul2(result01, w_mul2);
+            result23 = __hmul2(result23, w_mul2);
+        }
+
         atomicAdd(out    , result01);
         atomicAdd(out + 1, result23);
+//        *out = result01;
+//        *(out + 1) = result23;
     }
 }
 
-fp_gemm_half_q_half_kernel pick_gemm_half_q_half_kernel(bool first_block, const int m_count)
-{
-    #if BLOCK_M_SIZE_MAX >= 1
-    if (m_count == 1) return gemm_half_q_half_kernel<true, 1>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 2
-    if (m_count == 2) return gemm_half_q_half_kernel<true, 2>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 3
-    if (m_count == 3) return gemm_half_q_half_kernel<true, 3>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 4
-    if (m_count == 4) return gemm_half_q_half_kernel<true, 4>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 5
-    if (m_count == 5) return gemm_half_q_half_kernel<true, 5>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 6
-    if (m_count == 6) return gemm_half_q_half_kernel<true, 6>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 7
-    if (m_count == 7) return gemm_half_q_half_kernel<true, 7>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 8
-    if (m_count == 8) return gemm_half_q_half_kernel<true, 8>;
-    #endif
-    return NULL;
-}
```

## exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh

```diff
@@ -1,8 +1,15 @@
 #include "compat.cuh"
+#include "../config.h"
+#include "matrix_view.cuh"
+#include "quant/qdq_4.cuh"
+
+#define GPTQ_BLOCK_KN_SIZE 128
+#define GPTQ_BLOCK_M_SIZE_MAX MAX_Q_GEMM_ROWS_KERNEL
+#define GPTQ_MAX_GROUPS_IN_BLOCK (GPTQ_BLOCK_KN_SIZE / 32)
 
 __forceinline__ __device__ half2 dot22_8(half2(&dq)[4], const half* a_ptr, const half2 g_result)
 {
     half2 result = {};
     const half2* a2_ptr = (const half2*)a_ptr;
     #pragma unroll
     for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);
@@ -14,75 +21,111 @@
     half2 result = {};
     const half2* a2_ptr = (const half2*)a_ptr;
     #pragma unroll
     for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);
     return __half2float(__low2half(result)) + __half2float(__high2half(result));
 }
 
+__forceinline__ __device__ half2 dot22_8_h2(half2(&dq)[4], const half* a_ptr)
+{
+    half2 result = {};
+    const half2* a2_ptr = (const half2*)a_ptr;
+    #pragma unroll
+    for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);
+    return result;
+}
+
+__forceinline__ __device__ float add_halves(half2 h)
+{
+    return __low2float(h) + __high2float(h);
+}
+
 typedef void (*fp_gemm_half_q_half_gptq_kernel)
 (
     const half*,
     const uint32_t*,
     const uint32_t*,
     const half*,
     half*,
     const int,
     const int,
     const int,
     const int,
     const int,
     const uint16_t*,
     const int,
-    const bool
+    const bool,
+    const half*,
+    const int
 );
 
-template <bool first_block, int m_count>
+template <int m_count, bool use_r_weights, bool mul_r_weights>
 __global__ void gemm_half_q_half_gptq_kernel
 (
     const half* __restrict__ a,
     const uint32_t* __restrict__ b_q_weight,
     const uint32_t* __restrict__ b_gptq_qzeros,
     const half* __restrict__ b_gptq_scales,
     half* __restrict__ c,
     const int size_m,
     const int size_n,
     const int size_k,
     const int groups,
     const int groupsize,
     const uint16_t* __restrict__ b_q_perm,
     const int rows_4,
-    const bool clear
+    const bool clear,
+    const half* r_weights,
+    const int r_weights_stride
 )
 {
     MatrixView_half a_(a, size_m, size_k);
     MatrixView_half_rw c_(c, size_m, size_n);
     MatrixView_q4_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
     MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
 
     int t = threadIdx.x;
 
     // Block
 
-    int offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;
+    int offset_n = blockIdx.x * GPTQ_BLOCK_KN_SIZE * 4;
     int offset_m = blockIdx.y * m_count;
-    int offset_k = blockIdx.z * BLOCK_KN_SIZE;
+    int offset_k = blockIdx.z * GPTQ_BLOCK_KN_SIZE;
+
+    int m_count_min = min(size_m - offset_m, m_count);
 
-    int end_n = min(offset_n + BLOCK_KN_SIZE * 4, size_n);
-    int end_m = min(offset_m + m_count, size_m);
-    int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);
+    int end_n = min(offset_n + GPTQ_BLOCK_KN_SIZE * 4, size_n);
+    int end_m = min(offset_m + m_count_min, size_m);
+    int end_k = min(offset_k + GPTQ_BLOCK_KN_SIZE, size_k);
 
     int n = offset_n + t * 4;
 
+    // Read weights
+
+    half_uint16 weights[MAX_Q_GEMM_WEIGHTS];
+    if constexpr (use_r_weights)
+    {
+        uint16_t any_w = 0;
+        const half* w_ptr = r_weights;
+        for (int m = 0; m < m_count_min; ++m)
+        {
+            weights[m].as_half = *w_ptr;
+            w_ptr += r_weights_stride;
+            any_w |= weights[m].as_uint16;
+        }
+        if (!any_w) return;  // Early exit if all weights are zero -- does not zero output (!!!)
+    }
+
     // Preload block_a
 
-    __shared__ half block_a[m_count][BLOCK_KN_SIZE];
+    __shared__ half block_a[m_count][GPTQ_BLOCK_KN_SIZE];
 
     if (offset_k + t < end_k)
     {
-        for (int m = 0; m < m_count; ++m)
+        for (int m = 0; m < m_count_min; ++m)
         {
             const half* a_ptr = a_.item_ptr(offset_m + m, 0);
             half* block_a_ptr = block_a[m];
 
             half a0;
             if (b_q_perm) a0 = a_ptr[b_q_perm[offset_k + t]];
             else a0 = a_ptr[offset_k + t];
@@ -92,15 +135,15 @@
 
     // Zero output
 
     if (n >= size_n) return;
 
     if (clear && blockIdx.z == 0) // && (threadIdx.x & 1) == 0)
     {
-        for (int m = 0; m < m_count; m++)
+        for (int m = 0; m < m_count_min; m++)
             *((uint64_t*)c_.item_ptr(offset_m + m, n)) = 0;
     }
 
     __syncthreads();
 
     // Find initial group
 
@@ -109,46 +152,46 @@
 
     // a, b offset
 
     int qk = offset_k / (32 / 4);
 
     const uint32_t* b_ptr = b_q_weight + qk * size_n + n;
     const half* a_ptr = &block_a[0][0];
-    int a_stride = BLOCK_KN_SIZE;
+    int a_stride = GPTQ_BLOCK_KN_SIZE;
 
     // Initial group
 
     int zeros[4];
-    float scales[4];
+    half2 scales[4];
     half2 z1z16[4][2];
     half2 y1y16[4][2];
     b_gptq_qzeros_.item4(zeros, group, n);
-    b_gptq_scales_.item4_f(scales, group, n);
+    b_gptq_scales_.item4_h2(scales, group, n);
     dequant_4bit_8_prep_zero(zeros[0] + 1, z1z16[0], y1y16[0]);
     dequant_4bit_8_prep_zero(zeros[1] + 1, z1z16[1], y1y16[1]);
     dequant_4bit_8_prep_zero(zeros[2] + 1, z1z16[2], y1y16[2]);
     dequant_4bit_8_prep_zero(zeros[3] + 1, z1z16[3], y1y16[3]);
 
 //    __syncthreads();
 
     // Column result
 
-    float block_c[m_count][4] = {};
+    half2 block_c[m_count][4] = {};
 
     // Dequantize and multiply
 
     int k = offset_k;
     while (k < end_k)
     {
         if (k == nextgroup)
         {
             group++;
             nextgroup += groupsize;
             b_gptq_qzeros_.item4(zeros, group, n);
-            b_gptq_scales_.item4_f(scales, group, n);
+            b_gptq_scales_.item4_h2(scales, group, n);
             dequant_4bit_8_prep_zero(zeros[0] + 1, z1z16[0], y1y16[0]);
             dequant_4bit_8_prep_zero(zeros[1] + 1, z1z16[1], y1y16[1]);
             dequant_4bit_8_prep_zero(zeros[2] + 1, z1z16[2], y1y16[2]);
             dequant_4bit_8_prep_zero(zeros[3] + 1, z1z16[3], y1y16[3]);
         }
 
         #pragma unroll
@@ -160,60 +203,45 @@
             half2 dq[4][4];
             dequant_4bit_8_gptq(load_int4.x, dq[0], z1z16[0], y1y16[0], size_n, false);
             dequant_4bit_8_gptq(load_int4.y, dq[1], z1z16[1], y1y16[1], size_n, false);
             dequant_4bit_8_gptq(load_int4.z, dq[2], z1z16[2], y1y16[2], size_n, false);
             dequant_4bit_8_gptq(load_int4.w, dq[3], z1z16[3], y1y16[3], size_n, false);
 
             #pragma unroll
-            for (int m = 0; m < m_count; m++)
+            for (int m = 0; m < m_count_min; m++)
             {
-                block_c[m][0] = fma(dot22_8_f(dq[0], a_ptr + m * a_stride), scales[0], block_c[m][0]);
-                block_c[m][1] = fma(dot22_8_f(dq[1], a_ptr + m * a_stride), scales[1], block_c[m][1]);
-                block_c[m][2] = fma(dot22_8_f(dq[2], a_ptr + m * a_stride), scales[2], block_c[m][2]);
-                block_c[m][3] = fma(dot22_8_f(dq[3], a_ptr + m * a_stride), scales[3], block_c[m][3]);
+                if constexpr (use_r_weights) { if (!weights[m].as_uint16) continue; }
+                block_c[m][0] = __hfma2(dot22_8_h2(dq[0], a_ptr + m * a_stride), scales[0], block_c[m][0]);
+                block_c[m][1] = __hfma2(dot22_8_h2(dq[1], a_ptr + m * a_stride), scales[1], block_c[m][1]);
+                block_c[m][2] = __hfma2(dot22_8_h2(dq[2], a_ptr + m * a_stride), scales[2], block_c[m][2]);
+                block_c[m][3] = __hfma2(dot22_8_h2(dq[3], a_ptr + m * a_stride), scales[3], block_c[m][3]);
             }
 
             b_ptr += size_n;
             a_ptr += 8;
         }
 
         k += 32;
     }
 
-    for (int m = 0; m < m_count; m++)
+    for (int m = 0; m < m_count_min; m++)
     {
         half2 *out = (half2*) c_.item_ptr(offset_m + m, n);
-        half2 result01 = __halves2half2(__float2half_rn(block_c[m][0]), __float2half_rn(block_c[m][1]));
-        half2 result23 = __halves2half2(__float2half_rn(block_c[m][2]), __float2half_rn(block_c[m][3]));
+        half result0 = __hadd(__low2half(block_c[m][0]), __high2half(block_c[m][0]));
+        half result1 = __hadd(__low2half(block_c[m][1]), __high2half(block_c[m][1]));
+        half result2 = __hadd(__low2half(block_c[m][2]), __high2half(block_c[m][2]));
+        half result3 = __hadd(__low2half(block_c[m][3]), __high2half(block_c[m][3]));
+        half2 result01 = __halves2half2(result0, result1);
+        half2 result23 = __halves2half2(result2, result3);
+
+        if constexpr (mul_r_weights)
+        {
+            half2 w_mul2 = __half2half2(weights[m].as_half);
+            result01 = __hmul2(result01, w_mul2);
+            result23 = __hmul2(result23, w_mul2);
+        }
+
         atomicAdd(out    , result01);
         atomicAdd(out + 1, result23);
     }
 }
 
-fp_gemm_half_q_half_gptq_kernel pick_gemm_half_q_half_gptq_kernel(bool first_block, const int m_count)
-{
-    #if BLOCK_M_SIZE_MAX >= 1
-    if (m_count == 1) return gemm_half_q_half_gptq_kernel<true, 1>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 2
-    if (m_count == 2) return gemm_half_q_half_gptq_kernel<true, 2>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 3
-    if (m_count == 3) return gemm_half_q_half_gptq_kernel<true, 3>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 4
-    if (m_count == 4) return gemm_half_q_half_gptq_kernel<true, 4>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 5
-    if (m_count == 5) return gemm_half_q_half_gptq_kernel<true, 5>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 6
-    if (m_count == 6) return gemm_half_q_half_gptq_kernel<true, 6>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 7
-    if (m_count == 7) return gemm_half_q_half_gptq_kernel<true, 7>;
-    #endif
-    #if BLOCK_M_SIZE_MAX >= 8
-    if (m_count == 8) return gemm_half_q_half_gptq_kernel<true, 8>;
-    #endif
-    return NULL;
-}
```

## exllamav2/exllamav2_ext/cuda/q_matrix.cu

```diff
@@ -5,14 +5,16 @@
 #include "quant/qdq_2.cuh"
 #include "quant/qdq_3.cuh"
 #include "quant/qdq_4.cuh"
 #include "quant/qdq_5.cuh"
 #include "quant/qdq_6.cuh"
 #include "quant/qdq_8.cuh"
 
+#include "cache.cuh"
+
 #define BLOCK_KN_SIZE 128
 
 #define THREADS_X 32
 #define THREADS_Y 32
 
 // Shuffle quantized data on load
 
@@ -53,68 +55,91 @@
 
     uint32_t* _q_weight,
     uint16_t* _q_perm,
     uint16_t* _q_invperm,
     uint32_t* _q_scale,
     half* _q_scale_max,
     uint16_t* _q_groups,
+    uint16_t* _q_group_map,
 
     uint32_t* _gptq_qzeros,
     half* _gptq_scales,
     uint32_t* _gptq_g_idx,
 
-    half* _temp_dq
+    half* _bias,
+
+    half* _temp_dq,
+    const int _max_dq_rows
 ) :
     device(_device),
     height(_height),
     width(_width),
     groups(_groups),
-    temp_dq(_temp_dq)
+    temp_dq(_temp_dq),
+    max_dq_rows(_max_dq_rows)
 {
     cudaSetDevice(device);
 
     failed = false;
 
     cuda_q_weight = _q_weight;
     cuda_q_perm = _q_perm;
     cuda_q_invperm = _q_invperm;
     cuda_q_scale = _q_scale;
     cuda_q_scale_max = _q_scale_max;
     cuda_q_groups = _q_groups;
+    cuda_q_group_map = _q_group_map;
     cuda_gptq_qzeros = _gptq_qzeros;
     cuda_gptq_scales = _gptq_scales;
+    cuda_bias = _bias;
 
     is_gptq = (_gptq_qzeros != NULL);
 
-    groupsize = 1;
-    while (groupsize * groups < height) groupsize *= 2;
+    if (is_gptq)
+    {
+        gptq_groupsize = 1;
+        while (gptq_groupsize * groups < height) gptq_groupsize *= 2;
+    }
 
     // Create group map
 
     rows_8 = 0;
     rows_6 = 0;
     rows_5 = 0;
     rows_4 = 0;
     rows_3 = 0;
     rows_2 = 0;
+    kernel_p = 0;
 
     if (!is_gptq)
     {
         uint16_t* cpu_q_groups = (uint16_t*)calloc(groups * 2, sizeof(uint16_t));
         cudaMemcpy(cpu_q_groups, cuda_q_groups, groups * 2 * sizeof(uint16_t), cudaMemcpyDeviceToHost);
 
+        int row = 0;
         for (int i = 0; i < groups; i++)
         {
             int bits = cpu_q_groups[i * 2];
-            if (bits == 8) rows_8 += groupsize;
-            if (bits == 6) rows_6 += groupsize;
-            if (bits == 5) rows_5 += groupsize;
-            if (bits == 4) rows_4 += groupsize;
-            if (bits == 3) rows_3 += groupsize;
-            if (bits == 2) rows_2 += groupsize;
+            kernel_p |= (1 << (bits - 1));
+
+            int rows;
+            if (i < groups - 1)
+            {
+                int qrows = cpu_q_groups[i * 2 + 3] - cpu_q_groups[i * 2 + 1];
+                rows = qrows * 32 / bits;
+            }
+            else rows = height - row;
+
+            if (bits == 8) rows_8 += rows;
+            if (bits == 6) rows_6 += rows;
+            if (bits == 5) rows_5 += rows;
+            if (bits == 4) rows_4 += rows;
+            if (bits == 3) rows_3 += rows;
+            if (bits == 2) rows_2 += rows;
+            row += rows;
         }
 
         free(cpu_q_groups);
 
         rows_6 += rows_8;
         rows_5 += rows_6;
         rows_4 += rows_5;
@@ -134,14 +159,21 @@
                 failed = true;
                 //printf("FAIL\n");
                 return;
             }
         }
     }
 
+//     DBGI(rows_8);
+//     DBGI(rows_6);
+//     DBGI(rows_5);
+//     DBGI(rows_4);
+//     DBGI(rows_3);
+//     DBGI(rows_2);
+
     // Shuffle quantized data
 
     dim3 blockDim, gridDim;
     blockDim.x = THREADS_X;
     blockDim.y = 1;
     gridDim.x = DIVIDE(width, THREADS_X);
     gridDim.y = 1;
@@ -163,15 +195,17 @@
     const half* __restrict__ b_gptq_scales,
     //const uint16_t* __restrict__ b_q_groups,
     const int size_k,
     const int size_n,
     const int groupsize,
     const int groups,
     half* __restrict__ b,
-    const int rows_4
+    const int rows_4,
+    const int row_a,
+    const int row_b
 )
 {
     MatrixView_half_rw b_(b, size_k, size_n);
     MatrixView_q4_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);
     MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);
 
     int offset_k = BLOCK_KN_SIZE * blockIdx.y;
@@ -186,14 +220,16 @@
 
     if (b_q_perm)
     {
         if (offset_k + t < size_k)
             perm[t] = b_q_perm[offset_k + t];
     }
 
+    __syncthreads();
+
     // Column
 
     int n = offset_n + t * 4;
     if (n >= size_n) return;
 
     // Find initial group
 
@@ -215,16 +251,14 @@
     b_gptq_qzeros_.item4(zeros, group, n);
     b_gptq_scales_.item4_h2(scales, group, n);
     dequant_4bit_8_prep_zero(zeros[0] + 1, z1z16[0], y1y16[0]);
     dequant_4bit_8_prep_zero(zeros[1] + 1, z1z16[1], y1y16[1]);
     dequant_4bit_8_prep_zero(zeros[2] + 1, z1z16[2], y1y16[2]);
     dequant_4bit_8_prep_zero(zeros[3] + 1, z1z16[3], y1y16[3]);
 
-    __syncthreads();
-
     int k = offset_k;
     int lk = 0;
 
     while (k < end_k)
     {
         if (k == nextgroup)
         {
@@ -279,26 +313,28 @@
 
 __global__ void reconstruct_kernel
 (
     const uint32_t* __restrict__ b_q_weight,
     const uint16_t* __restrict__ b_q_perm,
     const uint32_t* __restrict__ b_q_scale,
     const half* __restrict__ b_q_scale_max,
-    //const uint16_t* __restrict__ b_q_groups,
+    const uint16_t* __restrict__ b_q_group_map,
     const int size_k,
     const int size_n,
-    const int groupsize,
+    //const int groupsize,
     const int groups,
     half* __restrict__ b,
     const int rows_8,
     const int rows_6,
     const int rows_5,
     const int rows_4,
     const int rows_3,
-    const int rows_2
+    const int rows_2,
+    const int row_a,
+    const int row_b
 )
 {
     MatrixView_half_rw b_(b, size_k, size_n);
     MatrixView_q4_row b_q_scale_(b_q_scale, groups, size_n);
 
     int offset_k = BLOCK_KN_SIZE * blockIdx.y;
     int offset_n = BLOCK_KN_SIZE * blockIdx.x;
@@ -313,15 +349,16 @@
     // Column
 
     int n = offset_n + t;
     if (n >= size_n) return;
 
     // Find initial group
 
-    int group = offset_k / groupsize;
+    // int group = offset_k / groupsize;
+    int group = b_q_group_map[offset_k * 2];
 
     int pre_rows_8 = min(rows_8, offset_k);
     int pre_rows_6 = offset_k > rows_8 ? min(rows_6, offset_k) - rows_8 : 0;
     int pre_rows_5 = offset_k > rows_6 ? min(rows_5, offset_k) - rows_6 : 0;
     int pre_rows_4 = offset_k > rows_5 ? min(rows_4, offset_k) - rows_5 : 0;
     int pre_rows_3 = offset_k > rows_4 ? min(rows_3, offset_k) - rows_4 : 0;
     int pre_rows_2 = offset_k > rows_3 ? min(rows_2, offset_k) - rows_3 : 0;
@@ -333,25 +370,25 @@
     qk += pre_rows_3 / 32 * 3;
     qk += pre_rows_2 / 32 * 2;
 
     const uint32_t* b_ptr = b_q_weight + qk * size_n + n;
 
     half qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]);
     half2 qs_h2 = __halves2half2(qs_h, qs_h);
-    int nextgroup = offset_k + groupsize;
+    int nextgroup = offset_k + b_q_group_map[offset_k * 2 + 1];
 
     int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);
     int k = offset_k;
     int lk = 0;
 
     __syncthreads();
 
     while (k < rows_8 && k < end_k)
     {
-        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += groupsize; qs_h2 = __halves2half2(qs_h, qs_h); }
+        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += b_q_group_map[k * 2 + 1]; qs_h2 = __halves2half2(qs_h, qs_h); }
         for (int p = 0; p < 4; p++)
         {
             half2 dq[4];
             uint32_t q_0 = *b_ptr; b_ptr += size_n;
             uint32_t q_1 = *b_ptr; b_ptr += size_n;
             dequant_8bit_8(q_0, q_1, dq, size_n);
             for (int j = 0; j < 4; j++) dq[j] = __hmul2(dq[j], qs_h2);
@@ -359,15 +396,15 @@
             for (int j = 0; j < 8; j++) b_.set(perm[lk++], n, dqh[j]);
         }
         k += 32;
     }
 
     while (k < rows_6 && k < end_k)
     {
-        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += groupsize; qs_h2 = __halves2half2(qs_h, qs_h); }
+        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += b_q_group_map[k * 2 + 1]; qs_h2 = __halves2half2(qs_h, qs_h); }
         for (int p = 0; p < 2; p++)
         {
             half2 dq[8];
             uint32_t q_0 = *b_ptr; b_ptr += size_n;
             uint32_t q_1 = *b_ptr; b_ptr += size_n;
             uint32_t q_2 = *b_ptr; b_ptr += size_n;
             dequant_6bit_16(q_0, q_1, q_2, dq, size_n);
@@ -376,15 +413,15 @@
             for (int j = 0; j < 16; j++) b_.set(perm[lk++], n, dqh[j]);
         }
         k += 32;
     }
 
     while (k < rows_5 && k < end_k)
     {
-        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += groupsize; qs_h2 = __halves2half2(qs_h, qs_h); }
+        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += b_q_group_map[k * 2 + 1]; qs_h2 = __halves2half2(qs_h, qs_h); }
         for (int p = 0; p < 1; p++)
         {
             half2 dq[16];
             uint32_t q_0 = *b_ptr; b_ptr += size_n;
             uint32_t q_1 = *b_ptr; b_ptr += size_n;
             uint32_t q_2 = *b_ptr; b_ptr += size_n;
             uint32_t q_3 = *b_ptr; b_ptr += size_n;
@@ -395,30 +432,30 @@
             for (int j = 0; j < 32; j++) b_.set(perm[lk++], n, dqh[j]);
         }
         k += 32;
     }
 
     while (k < rows_4 && k < end_k)
     {
-        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += groupsize; qs_h2 = __halves2half2(qs_h, qs_h); }
+        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += b_q_group_map[k * 2 + 1]; qs_h2 = __halves2half2(qs_h, qs_h); }
         for (int p = 0; p < 4; p++)
         {
             half2 dq[4];
             uint32_t q_0 = *b_ptr; b_ptr += size_n;
             dequant_4bit_8(q_0, dq, size_n);
             for (int j = 0; j < 4; j++) dq[j] = __hmul2(dq[j], qs_h2);
             half* dqh = (half*) dq;
             for (int j = 0; j < 8; j++) b_.set(perm[lk++], n, dqh[j]);
         }
         k += 32;
     }
 
     while (k < rows_3 && k < end_k)
     {
-        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += groupsize; qs_h2 = __halves2half2(qs_h, qs_h); }
+        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += b_q_group_map[k * 2 + 1]; qs_h2 = __halves2half2(qs_h, qs_h); }
         for (int p = 0; p < 1; p++)
         {
             half2 dq[16];
             uint32_t q_0 = *b_ptr; b_ptr += size_n;
             uint32_t q_1 = *b_ptr; b_ptr += size_n;
             uint32_t q_2 = *b_ptr; b_ptr += size_n;
             dequant_3bit_32(q_0, q_1, q_2, dq, size_n);
@@ -427,74 +464,80 @@
             for (int j = 0; j < 32; j++) b_.set(perm[lk++], n, dqh[j]);
         }
         k += 32;
     }
 
     while (k < rows_2 && k < end_k)
     {
-        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += groupsize; qs_h2 = __halves2half2(qs_h, qs_h); }
-        for (int p = 0; p < 2; p++)
+        if (k == nextgroup) { group++; qs_h = dq_scale(b_q_scale_.item(group, n), b_q_scale_max[group]); nextgroup += b_q_group_map[k * 2 + 1]; qs_h2 = __halves2half2(qs_h, qs_h); }
+        for (int p = 0; p < 1; p++)
         {
             half2 dq[8];
             uint32_t q_0 = *b_ptr; b_ptr += size_n;
             dequant_2bit_16(q_0, dq, size_n);
             for (int j = 0; j < 8; j++) dq[j] = __hmul2(dq[j], qs_h2);
             half* dqh = (half*) dq;
             for (int j = 0; j < 16; j++) b_.set(perm[lk++], n, dqh[j]);
         }
-        k += 32;
+        k += 16;
     }
 }
 
-void QMatrix::reconstruct(half* out)
+void QMatrix::reconstruct(half* out, int row_a, int row_b)
 {
     dim3 blockDim, gridDim;
     blockDim.x = BLOCK_KN_SIZE;
     blockDim.y = 1;
-    gridDim.y = DIVIDE(height, BLOCK_KN_SIZE);
+    if (row_a == 0 && row_b == 0) row_b = height;
+
+    gridDim.y = DIVIDE(row_b - row_a, BLOCK_KN_SIZE);
 
     if (!is_gptq)
     {
         gridDim.x = DIVIDE(width, BLOCK_KN_SIZE);
         reconstruct_kernel<<<gridDim, blockDim>>>
         (
             cuda_q_weight,
             cuda_q_perm,
             cuda_q_scale,
             cuda_q_scale_max,
-            //cuda_q_groups,
+            cuda_q_group_map,
             height,
             width,
-            groupsize,
+            //groupsize,
             groups,
             out,
             rows_8,
             rows_6,
             rows_5,
             rows_4,
             rows_3,
-            rows_2
+            rows_2,
+            row_a,
+            row_b
         );
     }
     else
     {
         gridDim.x = DIVIDE(width, BLOCK_KN_SIZE * 4);
         reconstruct_gptq_kernel<<<gridDim, blockDim>>>
         (
             cuda_q_weight,
             cuda_q_perm,
             cuda_gptq_qzeros,
             cuda_gptq_scales,
             //const uint16_t* __restrict__ b_q_groups,
             height,
             width,
-            groupsize,
+            gptq_groupsize,
             groups,
             out,
-            rows_4
+            rows_4,
+            row_a,
+            row_b
         );
     }
 }
 
 __global__ void make_sequential_kernel
 (
     const uint32_t* __restrict__ w,
@@ -617,7 +660,132 @@
     cudaFree(cuda_new_qweight);
     free(cpu_g_idx_map);
     free(cpu_x_map);
     free(cpu_x_map_inv);
 
     return true;
 }
+
+// FP8/FP16 convert funcs
+
+#define BLOCKSIZE_F 8
+#define THREADS_F 32
+
+__global__ void matrix_fp8_to_fp16_kernel
+(
+    const uint8_t* __restrict__ in,
+    half* __restrict__ out
+)
+{
+    int block_offset = (blockIdx.x * BLOCKSIZE_F * THREADS_F + threadIdx.x * BLOCKSIZE_F);
+
+    const uint2* in2 = (const uint2*) (in + block_offset);
+    uint2 a = *in2;
+
+    uint4 b;
+    b.x = a.x & 0xff00ff00;
+    b.y = (a.x & 0x00ff00ff) << 8;
+    b.z = a.y & 0xff00ff00;
+    b.w = (a.y & 0x00ff00ff) << 8;
+
+    uint4* out4 = (uint4*) (out + block_offset);
+    *out4 = b;
+}
+
+__global__ void matrix_fp16_to_fp8_kernel
+(
+    const half* __restrict__ in,
+    uint8_t* __restrict__ out
+)
+{
+    int block_offset = (blockIdx.x * BLOCKSIZE_F * THREADS_F + threadIdx.x * BLOCKSIZE_F);
+
+    const uint4* in4 = (const uint4*) (in + block_offset);
+    uint4 a = *in4;
+
+    uint2* out2 = (uint2*) (out + block_offset);
+    uint2 b;
+    b.x = (a.x & 0xff00ff00) | ((a.y & 0xff00ff00) >> 8);
+    b.y = (a.z & 0xff00ff00) | ((a.w & 0xff00ff00) >> 8);
+    *out2 = b;
+}
+
+void matrix_fp8_to_fp16_cuda
+(
+    const uint8_t* in_ptr,
+    half* out_ptr,
+    int numel
+)
+{
+    if (numel % (BLOCKSIZE_F * THREADS_F))
+        printf(" ## matrix_fp8_to_fp16_cuda: numel() must be multiple of %d\n", BLOCKSIZE_F * THREADS_F);
+
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_F;
+    gridDim.x = numel / (BLOCKSIZE_F * THREADS_F);
+    matrix_fp8_to_fp16_kernel<<<gridDim, blockDim>>>(in_ptr, out_ptr);
+}
+
+void matrix_fp16_to_fp8_cuda
+(
+    const half* in_ptr,
+    uint8_t* out_ptr,
+    int numel
+)
+{
+    if (numel % (BLOCKSIZE_F * THREADS_F))
+        printf(" ## matrix_fp16_to_fp8_cuda: numel() must be multiple of %d\n", BLOCKSIZE_F * THREADS_F);
+
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_F;
+    gridDim.x = numel / (BLOCKSIZE_F * THREADS_F);
+    matrix_fp16_to_fp8_kernel<<<gridDim, blockDim>>>(in_ptr, out_ptr);
+}
+
+// Q4/FP16 convert funcs
+
+void matrix_q4_to_fp16_cuda
+(
+    const uint8_t* in_ptr,
+    const half* scales_ptr,
+    half* out_ptr,
+    int numel
+)
+{
+    array_q4_to_fp16_kv_cuda
+    (
+        in_ptr,
+        scales_ptr,
+        out_ptr,
+        NULL,
+        NULL,
+        NULL,
+        0,
+        1,
+        0,
+        numel
+    );
+}
+
+void matrix_fp16_to_q4_cuda
+(
+    const half* in_ptr,
+    uint8_t* out_ptr,
+    half* scales_ptr,
+    int numel
+)
+{
+    array_fp16_to_q4_kv_cuda
+    (
+        in_ptr,
+        out_ptr,
+        scales_ptr,
+        NULL,
+        NULL,
+        NULL,
+        0,
+        1,
+        0,
+        numel
+    );
+}
+
```

## exllamav2/exllamav2_ext/cuda/q_matrix.cuh

```diff
@@ -14,33 +14,37 @@
 
     int device;
     bool is_gptq;
 
     int height;
     int width;
     int groups;
-    int groupsize;
+    int gptq_groupsize;
 
     int rows_8;
     int rows_6;
     int rows_5;
     int rows_4;
     int rows_3;
     int rows_2;
+    int kernel_p;
 
     uint32_t* cuda_q_weight = NULL;
     uint16_t* cuda_q_perm = NULL;
     uint16_t* cuda_q_invperm = NULL;
     uint32_t* cuda_q_scale = NULL;
     half* cuda_q_scale_max = NULL;
     uint16_t* cuda_q_groups = NULL;
+    uint16_t* cuda_q_group_map = NULL;
     uint32_t* cuda_gptq_qzeros = NULL;
     half* cuda_gptq_scales = NULL;
+    half* cuda_bias = NULL;
 
     half* temp_dq;
+    int max_dq_rows;
 
     bool failed;
 
     QMatrix
     (
         const int _device,
         const int _height,
@@ -49,25 +53,59 @@
 
         uint32_t* _q_weight,
         uint16_t* _q_perm,
         uint16_t* _q_invperm,
         uint32_t* _q_scale,
         half* _q_scale_max,
         uint16_t* _q_groups,
+        uint16_t* _q_group_map,
 
         uint32_t* _gptq_qzeros,
         half* _gptq_scales,
         uint32_t* _gptq_g_idx,
 
-        half* _temp_dq
+        half* bias,
+
+        half* _temp_dq,
+        const int _max_dq_rows
     );
 
     ~QMatrix();
 
-    void reconstruct(half* out);
+    void reconstruct(half* out, int row_a = 0, int row_b = 0);
     bool make_sequential(const uint32_t* cpu_g_idx);
 
 private:
 
 };
 
+void matrix_q4_to_fp16_cuda
+(
+    const uint8_t* in_ptr,
+    const half* scales_ptr,
+    half* out_ptr,
+    int numel
+);
+
+void matrix_fp16_to_q4_cuda
+(
+    const half* in_ptr,
+    uint8_t* out_ptr,
+    half* scales_ptr,
+    int numel
+);
+
+void matrix_fp8_to_fp16_cuda
+(
+    const uint8_t* in_ptr,
+    half* out_ptr,
+    int numel
+);
+
+void matrix_fp16_to_fp8_cuda
+(
+    const half* in_ptr,
+    uint8_t* out_ptr,
+    int numel
+);
+
 #endif
```

## exllamav2/exllamav2_ext/cuda/q_mlp.cu

```diff
@@ -1,162 +1,270 @@
 #include "q_mlp.cuh"
 #include "q_gemm.cuh"
 #include "rms_norm.cuh"
+#include "layer_norm.cuh"
 #include "util.cuh"
 #include "matrix_view.cuh"
 #include "lora.cuh"
-
-#if defined(USE_ROCM)
-__device__ __forceinline__ __half2 __compat_h2rcp(__half2 x) {
-    return _Float16_2{static_cast<_Float16>(__builtin_amdgcn_rcph(static_cast<__half2_raw>(x).data.x)),
-        static_cast<_Float16>(__builtin_amdgcn_rcph(static_cast<__half2_raw>(x).data.y))};
-}
-#define h2rcp __compat_h2rcp
-#endif
+#include "quant/qdq_util.cuh"
+#include "../config.h"
+#include "compat.cuh"
 
 const int THREADS_X = 32;
 const int THREADS_Y = 4;
+
+#include "q_mlp_softmax.cuh"
+#include "q_mlp_activation.cuh"
+
 // const int MAX_DIMENSION = 8192;
 
-__device__ __forceinline__ half silu(half x)
+QMLP::QMLP
+(
+    half* _layernorm,
+    half* _layernorm_bias,
+    bool _layernorm_is_rms,
+    float _norm_epsilon,
+    QMatrix* _gate,
+    QMatrix* _up,
+    QMatrix* _down,
+    half* _temp_state,
+    half* _temp_a,
+    half* _temp_b,
+    half* _temp_dq,
+    int _max_rows,
+    bool _act_gelu,
+    bool _has_residual
+):
+    layernorm(_layernorm),
+    layernorm_bias(_layernorm_bias),
+    layernorm_is_rms(_layernorm_is_rms),
+    norm_epsilon(_norm_epsilon),
+    gate(_gate),
+    up(_up),
+    down(_down),
+    temp_state(_temp_state),
+    temp_a(_temp_a),
+    temp_b(_temp_b),
+    temp_dq(_temp_dq),
+    max_rows(_max_rows),
+    act_gelu(_act_gelu),
+    has_residual(_has_residual)
 {
-    half one = __float2half(1.0f);
-    half neg_x = __hneg(x);
-    half e = hexp(neg_x);
-    half sum = __hadd(one, e);
-    half r = hrcp(sum);
-    half result = __hmul(x, r);
-    return result;
 }
 
-__device__ __forceinline__ half2 silu(half2 x)
-{
-    half2 one = __float2half2_rn(1.0f);
-    half2 neg_x = __hneg2(x);
-    half2 e = h2exp(neg_x);
-    half2 sum = __hadd2(one, e);
-    half2 r = h2rcp(sum);
-    half2 result = __hmul2(x, r);
-    return result;
+QMLP::~QMLP() {
 }
 
-typedef void (*fp_silu_mul_kernel)
-(
-    half*,
-    const half*,
-    const int,
-    const int
-);
-
-template <bool use_half2>
-__global__ void silu_mul_kernel
+void QMLP::forward_
 (
-    half* __restrict__ x,
-    const half* __restrict__ y,
-    const int height,
-    const int width
+    cublasHandle_t cublas_handle,
+    half* x,
+    int rows,
+    int columns,
+    const std::vector<uintptr_t>& loras,
+    half* lora_temp
 )
 {
-    MatrixView_half_rw x_(x, height, width);
-    MatrixView_half y_(y, height, width);
+    bool use_half2 = true;
+    int intermediate_size = up->width;
+
+    // Activation kernel dims
+
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_X;
+    blockDim.y = THREADS_Y;
+    gridDim.x = DIVIDE(up->width, THREADS_X) / (use_half2 ? 2 : 1);
+    gridDim.y = DIVIDE(rows, THREADS_Y);
 
-    int column = (THREADS_X * blockIdx.x + threadIdx.x); if constexpr (use_half2) column *= 2;
-    int row = THREADS_Y * blockIdx.y + threadIdx.y;
-    if (row >= height) return;
+    // Layernorm
 
-    // silu(x) * y
+    half* norm_state = x;
 
-    if constexpr (use_half2)
+    if (layernorm)
     {
-        half2 one = __half2half2(__float2half(1.0f));
+        if (layernorm_is_rms)
+            rms_norm_cuda(x, layernorm, temp_state, norm_epsilon, rows, columns);
+        else
+            layer_norm_cuda(x, layernorm, layernorm_bias, temp_state, norm_epsilon, rows, columns);
+        norm_state = temp_state;
+    }
+
+    // Up proj with gate
 
-        half2 x_item = x_.item_half2(row, column);
-        half2 y_item = y_.item_half2(row, column);
+    if (gate)
+    {
+        gemm_half_q_half_cuda(cublas_handle, norm_state, gate, temp_a, rows, intermediate_size, columns, true, temp_dq);
+        gemm_half_q_half_cuda(cublas_handle, norm_state, up,   temp_b, rows, intermediate_size, columns, true, temp_dq);
 
-        x_item = silu(x_item);
-        x_item = __hmul2(x_item, y_item);
+        apply_loras_cuda(cublas_handle, gate_proj_lora, loras, gate, norm_state, temp_a, lora_temp, rows);
+        apply_loras_cuda(cublas_handle, up_proj_lora,   loras, up,   norm_state, temp_b, lora_temp, rows);
 
-        x_.set_half2(row, column, x_item);
+        fp_act_mul_kernel kernel = pick_act_mul_kernel(use_half2, false, act_gelu);
+        kernel<<<gridDim, blockDim>>>(temp_a, temp_b, rows, intermediate_size, NULL, 0);
     }
+
+    // Up proj without gate
+
     else
     {
-        half one = __float2half(1.0f);
-
-        half x_item = x_.item(row, column);
-        half y_item = y_.item(row, column);
+        gemm_half_q_half_cuda(cublas_handle, norm_state, up,   temp_a, rows, intermediate_size, columns, true, temp_dq);
 
-        x_item = silu(x_item);
-        x_item = __hmul(x_item, y_item);
+        apply_loras_cuda(cublas_handle, up_proj_lora,   loras, up,   norm_state, temp_a, lora_temp, rows);
 
-        x_.set(row, column, x_item);
+        fp_act_kernel kernel = pick_act_kernel(use_half2, false, act_gelu);
+        kernel<<<gridDim, blockDim>>>(temp_a, rows, intermediate_size, NULL, 0);
     }
-}
 
-fp_silu_mul_kernel pick_silu_mul_kernel(bool use_half2)
-{
-    if (use_half2) return silu_mul_kernel<true>;
-    else           return silu_mul_kernel<false>;
-};
+    // Down proj
 
+    gemm_half_q_half_cuda(cublas_handle, temp_a, down, x, rows, columns, intermediate_size, !has_residual, temp_dq);
 
-QMLP::QMLP
+    apply_loras_cuda(cublas_handle, down_proj_lora, loras, down, temp_a, x, lora_temp, rows);
+}
+
+
+QMoEMLP::QMoEMLP
 (
     half* _layernorm,
+    half* _layernorm_bias,
+    bool _layernorm_is_rms,
     float _norm_epsilon,
-    QMatrix* _gate,
-    QMatrix* _up,
-    QMatrix* _down,
+    half* _gate,
+    int _num_experts,
+    int _num_experts_per_token,
+    std::vector<QMatrix*>& _w1,
+    std::vector<QMatrix*>& _w2,
+    std::vector<QMatrix*>& _w3,
     half* _temp_state,
+    half* _temp_gathered_state,
     half* _temp_a,
     half* _temp_b,
+    half* _temp_logits,
     half* _temp_dq,
-    int _max_rows
+    int _max_rows,
+    int _hidden_dim,
+    bool _act_gelu
 ):
     layernorm(_layernorm),
+    layernorm_bias(_layernorm_bias),
+    layernorm_is_rms(_layernorm_is_rms),
     norm_epsilon(_norm_epsilon),
     gate(_gate),
-    up(_up),
-    down(_down),
+    num_experts(_num_experts),
+    num_experts_per_token(_num_experts_per_token),
+    w1(_w1),
+    w2(_w2),
+    w3(_w3),
     temp_state(_temp_state),
+    temp_gathered_state(_temp_gathered_state),
     temp_a(_temp_a),
     temp_b(_temp_b),
+    temp_logits(_temp_logits),
     temp_dq(_temp_dq),
-    max_rows(_max_rows)
+    max_rows(_max_rows),
+    hidden_dim(_hidden_dim),
+    act_gelu(_act_gelu)
 {
+//    for (int i = 0; i < num_experts; ++i)
+//    {
+//        std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> w1;
+//        std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> w2;
+//        std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> w3;
+//        w1_lora.push_back(w1);
+//        w2_lora.push_back(w2);
+//        w3_lora.push_back(w3);
+//    }
 }
 
-QMLP::~QMLP() {
+QMoEMLP::~QMoEMLP() {
 }
 
-void QMLP::forward_
+void QMoEMLP::forward_
 (
     cublasHandle_t cublas_handle,
     half* x,
     int rows,
-    int columns,
-    const std::vector<uintptr_t>& loras,
-    half* lora_temp
+    int columns
+//    const std::vector<uintptr_t>& loras,
+//    half* lora_temp
 )
 {
+    if (num_experts != 4 && num_experts != 8 && num_experts != 16)
+    {
+        printf(" ## num_experts must be 4, 8 or 16\n");
+        return;
+    }
+
     bool use_half2 = true;
-    int intermediate_size = gate->width;
 
-    rms_norm_cuda(x, layernorm, temp_state, norm_epsilon, rows, columns);
-    gemm_half_q_half_cuda(cublas_handle, temp_state, gate, temp_a, rows, intermediate_size, columns, true, temp_dq);
-    gemm_half_q_half_cuda(cublas_handle, temp_state, up,   temp_b, rows, intermediate_size, columns, true, temp_dq);
+    // Norm
+
+    if (layernorm_is_rms)
+        rms_norm_cuda(x, layernorm, temp_state, norm_epsilon, rows, columns);
+    else
+        layer_norm_cuda(x, layernorm, layernorm_bias, temp_state, norm_epsilon, rows, columns);
 
-    apply_loras_cuda(cublas_handle, gate_proj_lora, loras, gate, temp_state, temp_a, lora_temp, rows);
-    apply_loras_cuda(cublas_handle, up_proj_lora,   loras, up,   temp_state, temp_b, lora_temp, rows);
+    // Compute gate logits
+
+    half alpha_ = __float2half(1.0f);
+    half beta_ = __float2half(0.0f);
+    cublasHgemm(cublas_handle,
+                CUBLAS_OP_T, // gate is column-major
+                CUBLAS_OP_N,
+                num_experts, rows, hidden_dim,
+                &alpha_,
+                gate, hidden_dim,
+                temp_state, hidden_dim,
+                &beta_,
+                temp_logits, num_experts);
+
+    // Compute softmax filter to and normalize top-k outputs
 
     dim3 blockDim, gridDim;
-    blockDim.x = THREADS_X;
-    blockDim.y = THREADS_Y;
-    gridDim.x = DIVIDE(up->width, THREADS_X) / (use_half2 ? 2 : 1);
-    gridDim.y = DIVIDE(rows, THREADS_Y);
+    blockDim.x = WARPS;
+    blockDim.y = 1;
+    gridDim.x = 1;
+    gridDim.y = DIVIDE(rows, WARPS);
+    if (num_experts == 4)
+        softmax4_topk_norm_kernel<<<gridDim, blockDim>>>(temp_logits, rows, num_experts_per_token);
+    else if (num_experts == 8)
+        softmax8_topk_norm_kernel<<<gridDim, blockDim>>>(temp_logits, rows, num_experts_per_token);
+    else if (num_experts == 16)
+        softmax16_topk_norm_kernel<<<gridDim, blockDim>>>(temp_logits, rows, num_experts_per_token);
 
-    fp_silu_mul_kernel kernel = pick_silu_mul_kernel(use_half2);
-    kernel<<<gridDim, blockDim>>>(temp_a, temp_b, rows, intermediate_size);
+    // For small no. rows, execute all kernels but pass the routing weights. Rows with a weight of zero will skip dot
+    // product accum and kernels launched with only zero-weights will exit prematurely.
 
-    gemm_half_q_half_cuda(cublas_handle, temp_a, down, x, rows, columns, intermediate_size, false, temp_dq);
+    if (rows <= MAX_Q_GEMM_WEIGHTS)
+    {
+        int intermediate_size = w1[0]->width;
+        fp_act_mul_kernel kernel = pick_act_mul_kernel(use_half2, true, act_gelu);
 
-    apply_loras_cuda(cublas_handle, down_proj_lora, loras, down, temp_a, x, lora_temp, rows);
+        for (int i = 0; i < num_experts; i++)
+        {
+            gemm_half_q_half_cuda(cublas_handle, temp_state, w1[i], temp_a, rows, intermediate_size, columns, true, temp_dq, true, temp_logits + i, num_experts, false);
+            gemm_half_q_half_cuda(cublas_handle, temp_state, w3[i], temp_b, rows, intermediate_size, columns, true, temp_dq, true, temp_logits + i, num_experts, false);
+
+//            apply_loras_cuda(cublas_handle, w1_lora[i], loras, w1[i], temp_state, temp_a, lora_temp, rows);
+//            apply_loras_cuda(cublas_handle, w3_lora[i], loras, w3[i], temp_state, temp_b, lora_temp, rows);
+
+            blockDim.x = THREADS_X;
+            blockDim.y = THREADS_Y;
+            gridDim.x = DIVIDE(intermediate_size, THREADS_X) / (use_half2 ? 2 : 1);
+            gridDim.y = DIVIDE(rows, THREADS_Y);
+            kernel<<<gridDim, blockDim>>>(temp_a, temp_b, rows, intermediate_size, temp_logits + i, num_experts);
+
+            gemm_half_q_half_cuda(cublas_handle, temp_a, w2[i], x, rows, columns, intermediate_size, false, temp_dq, true, temp_logits + i, num_experts, true);
+
+//            apply_loras_cuda(cublas_handle, w2_lora[i], loras, w2[i], temp_a, x, lora_temp, rows);
+        }
+    }
+
+    // Gather larger number of rows in separate batches according to which experts they trigger, evaluate each MLP
+    // only on the affected rows and scale by routing weights while adding back directly onto the residual hidden state
+
+    else
+    {
+        printf(" ## ropws > %i not implemented\n", MAX_Q_GEMM_WEIGHTS);
+        DBGI(rows);
+    }
 }
```

## exllamav2/exllamav2_ext/cuda/q_mlp.cuh

```diff
@@ -10,14 +10,16 @@
 #include "q_matrix.cuh"
 
 class QMLP
 {
 public:
 
     half* layernorm;
+    half* layernorm_bias;
+    bool layernorm_is_rms;
     float norm_epsilon;
 
     QMatrix* gate;
     QMatrix* up;
     QMatrix* down;
 
     half* temp_state;
@@ -28,26 +30,33 @@
     int device;
     int max_rows;
 
     std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> gate_proj_lora;
     std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> up_proj_lora;
     std::unordered_map<uintptr_t, std::tuple<half*, half*, int>> down_proj_lora;
 
+    bool act_gelu;
+    bool has_residual;
+
     QMLP
     (
         half* _layernorm,
+        half* _layermorm_bias,
+        bool _layernorm_is_rms,
         float _norm_epsilon,
         QMatrix* _gate,
         QMatrix* _up,
         QMatrix* _down,
         half* _temp_state,
         half* _temp_a,
         half* _temp_b,
         half* _temp_dq,
-        int _max_rows
+        int _max_rows,
+        bool _act_gelu,
+        bool _has_residual
     );
 
     ~QMLP();
 
     void forward_
     (
         cublasHandle_t cublas_handle,
@@ -58,8 +67,81 @@
         half* lora_temp
     );
 
 private:
 
 };
 
+class QMoEMLP
+{
+public:
+
+    half* layernorm;
+    half* layernorm_bias;
+    bool layernorm_is_rms;
+    float norm_epsilon;
+
+    half* gate;
+    int num_experts;
+    int num_experts_per_token;
+
+    std::vector<QMatrix*> w1;
+    std::vector<QMatrix*> w2;
+    std::vector<QMatrix*> w3;
+
+    half* temp_state;
+    half* temp_gathered_state;
+    half* temp_a;
+    half* temp_b;
+    half* temp_logits;
+    half* temp_dq;
+
+    int device;
+    int max_rows;
+    int hidden_dim;
+
+    bool act_gelu;
+
+//    std::vector<std::unordered_map<uintptr_t, std::tuple<half*, half*, int>>> w1_lora;
+//    std::vector<std::unordered_map<uintptr_t, std::tuple<half*, half*, int>>> w2_lora;
+//    std::vector<std::unordered_map<uintptr_t, std::tuple<half*, half*, int>>> w3_lora;
+
+    QMoEMLP
+    (
+        half* _layernorm,
+        half* _layermorm_bias,
+        bool _layernorm_is_rms,
+        float _norm_epsilon,
+        half* _gate,
+        int _num_experts,
+        int _num_experts_per_token,
+        std::vector<QMatrix*>& w1,
+        std::vector<QMatrix*>& w2,
+        std::vector<QMatrix*>& w3,
+        half* _temp_state,
+        half* _temp_gathered_state,
+        half* _temp_a,
+        half* _temp_b,
+        half* _temp_logits,
+        half* _temp_dq,
+        int _max_rows,
+        int _hidden_dim,
+        bool _act_gelu
+    );
+
+    ~QMoEMLP();
+
+    void forward_
+    (
+        cublasHandle_t cublas_handle,
+        half* x,
+        int rows,
+        int columns
+//        const std::vector<uintptr_t>& loras,
+//        half* lora_temp
+    );
+
+private:
+
+};
+
 #endif
```

## exllamav2/exllamav2_ext/cuda/quantize.cu

```diff
@@ -1,14 +1,175 @@
 #include "quantize.cuh"
 #include "util.cuh"
 #include <curand_kernel.h>
 #include "compat.cuh"
 
 #define BLOCKSIZE_X 32
 #define BLOCKSIZE_Y 32
+#define MAX_P_GRID 64
+
+__global__ void quantize_rtn_kernel
+(
+    float*       __restrict__ weights,          // input  weights           [rows, columns]
+    const float* __restrict__ scale,            // input  scales            [1, columns]
+    uint16_t*    __restrict__ out_q,            // output qweights          [rows, columns]
+    int row,                                    // row index to quantize
+    int rows,                                   // num rows
+    int columns,                                // num columns
+    float qzero,                                // 2^(bits - 1)
+    float maxq                                  // (2^bits) - 1
+)
+{
+    int column = blockIdx.x * blockDim.x + threadIdx.x;
+    if (column >= columns) return;
+
+    uint64_t idx = (uint64_t)row * (uint64_t)columns + (uint64_t)column;
+
+    // Quantize
+
+    float x = weights[idx];
+    float s = scale[column];
+    x /= s;
+    x = rintf(x);
+    x += qzero;
+    x = clamp(x, 0.0f, maxq);
+
+    // Optionally save quant
+
+    if (out_q) out_q[idx] = static_cast<uint16_t>(x);
+
+    // Downcast while quantizing
+
+    half h_s = __float2half_rn(s);
+    half h_x = __float2half_rn(x);
+    half h_qzero = __float2half_rn(qzero);
+
+    // Dequantize
+
+    h_x = __hsub(h_x, h_qzero);
+    h_x = __hmul(h_x, h_s);
+    float q = __half2float(h_x);
+    weights[idx] = q;
+}
+
+void quantize_rtn_cuda
+(
+    float*          weights,
+    const float*    scale,
+    uint16_t*       out_q,
+    int row,
+    int rows,
+    int columns,
+    float qzero,
+    float maxq
+)
+{
+    dim3 threads(BLOCKSIZE_X, 1);
+    dim3 blocks(DIVIDE(columns, BLOCKSIZE_X), 1);
+
+    quantize_rtn_kernel<<<blocks, threads>>>
+    (
+        weights,
+        scale,
+        out_q,
+        row,
+        rows,
+        columns,
+        qzero,
+        maxq
+    );
+}
+
+__global__ void fused_quantize_adjust_kernel
+(
+    const float* __restrict__ weights,          // input  weights           [rows, columns]
+    float*       __restrict__ quant,            // output quantized weights [rows, columns]
+    const float* __restrict__ scale,            // input  scales            [1, columns]
+    uint16_t*    __restrict__ out_q,            // output qweights          [rows, columns]
+    const float* __restrict__ hessian_inv,      // input hessian            [rows, rows]
+    float*       __restrict__ error,            // output error             [rows, columns]
+    int row,                                    // row index to quantize
+    int rows,                                   // num rows
+    int columns,                                // num columns
+    float qzero,                                // 2^(bits - 1)
+    float maxq                                  // (2^bits) - 1
+)
+{
+    int column = blockIdx.x * blockDim.x + threadIdx.x;
+    if (column >= columns) return;
+
+    uint64_t idx = (uint64_t)row * (uint64_t)columns + (uint64_t)column;
+
+    // Quantize
+
+    float x = weights[idx];
+    float s = scale[column];
+    x /= s;
+    x = rintf(x);
+    x += qzero;
+    x = clamp(x, 0.0f, maxq);
+
+    // Optionally save quant
+
+    if (out_q) out_q[idx] = static_cast<uint16_t>(x);
+
+    // Downcast while quantizing
+
+    half h_s = __float2half_rn(s);
+    half h_x = __float2half_rn(x);
+    half h_qzero = __float2half_rn(qzero);
+
+    // Dequantize
+
+    h_x = __hsub(h_x, h_qzero);
+    h_x = __hmul(h_x, h_s);
+    float q = __half2float(h_x);
+    quant[idx] = q;
+
+    // Adjust error
+
+    uint64_t d_idx = (uint64_t)row * (uint64_t)rows + (uint64_t)row;
+    float d = hessian_inv[d_idx];  // H diagonal
+    float w = weights[idx];
+    error[idx] = (w - q) / d;
+}
+
+void fused_quantize_adjust_cuda
+(
+    const float*    weights,
+    float*          quant,
+    const float*    scale,
+    uint16_t*       out_q,
+    const float*    hessian_inv,
+    float*          error,
+    int row,
+    int rows,
+    int columns,
+    float qzero,
+    float maxq
+)
+{
+    dim3 threads(BLOCKSIZE_X, 1);
+    dim3 blocks(DIVIDE(columns, BLOCKSIZE_X), 1);
+
+    fused_quantize_adjust_kernel<<<blocks, threads>>>
+    (
+        weights,
+        quant,
+        scale,
+        out_q,
+        hessian_inv,
+        error,
+        row,
+        rows,
+        columns,
+        qzero,
+        maxq
+    );
+}
 
 __global__ void quantize_kernel
 (
     const float* __restrict__ input,
     float* __restrict__ output,
     const float* __restrict__ scale,
     uint16_t* __restrict__ out_q,
@@ -21,41 +182,40 @@
     int column = blockIdx.x * blockDim.x + threadIdx.x;
     int row = blockIdx.y * blockDim.y + threadIdx.y;
     if (column >= columns) return;
     if (row >= rows) return;
 
     // Quantize
 
-    float x = input[row * columns + column];
+    uint64_t idx = (uint64_t)row * (uint64_t)columns + (uint64_t)column;
+    float x = input[idx];
     float s = scale[column];
     x /= s;
     x = rintf(x);
     x += qzero;
     x = clamp(x, 0.0f, maxq);
 
     // Optionally save quant
 
     if (out_q)
     {
         uint16_t q = static_cast<uint16_t>(x);
-        out_q[row * columns + column] = q;
+        out_q[idx] = q;
     }
 
     half h_s = __float2half_rn(s);
     half h_x = __float2half_rn(x);
     half h_qzero = __float2half_rn(qzero);
 
     h_x = __hsub(h_x, h_qzero);
     h_x = __hmul(h_x, h_s);
 
     // Dequantize
 
-//    x -= qzero;
-//    x *= s;
-    output[row * columns + column] = __half2float(h_x);
+    output[idx] = __half2float(h_x);
 }
 
 void quantize_cuda
 (
     const float* input,
     float* output,
     const float* scale,
@@ -81,14 +241,53 @@
         rows,
         columns,
         qzero,
         maxq
     );
 }
 
+__global__ void adjust_error_row_kernel
+(
+    const float* __restrict__ hessian_inv,
+    float* __restrict__ error,
+    const float* __restrict__ weights,
+    const float* __restrict__ quant,
+    int c,
+    int columns,
+    int hcolumns
+)
+{
+    int column = blockIdx.x * blockDim.x + threadIdx.x;
+    if (column >= columns) return;
+
+    float d = hessian_inv[c * hcolumns + c];
+
+    int idx = c * columns + column;
+    float w = weights[idx];
+    float q = quant[idx];
+    error[idx] = (w - q) / d;
+}
+
+void adjust_error_row_cuda
+(
+    const float* hessian_inv,
+    float* error,
+    const float* weights,
+    const float* quant,
+    int c,
+    int columns,
+    int hcolumns
+)
+{
+    dim3 threads(BLOCKSIZE_X, 1);
+    dim3 blocks(DIVIDE(columns, BLOCKSIZE_X), 1);
+
+    adjust_error_row_kernel<<<blocks, threads>>>(hessian_inv, error, weights, quant, c, columns, hcolumns);
+}
+
 __global__ void quantize_err_kernel
 (
     const float* __restrict__ input,
     float* __restrict__ output,
     const float* __restrict__ scale,
     int rows,
     int columns,
@@ -96,44 +295,44 @@
     float maxq,
     float err_norm,
     float min_p,
     float max_p,
     int p_grid
 )
 {
-    int column = blockIdx.x * blockDim.x + threadIdx.x;
+    int column_ = blockIdx.x * blockDim.x + threadIdx.x;
+    int column = column_ * 4;
     int row = blockIdx.y * blockDim.y + threadIdx.y;
     if (column >= columns) return;
     if (row >= rows) return;
 
-    float w = input[row * columns + column];
+    float clamp_min = -qzero;
+    float clamp_max = maxq - qzero;
 
-    // Quantize
+    uint64_t idx = (uint64_t)row * (uint64_t)columns + (uint64_t)column;
+    float4 sc4 = *((float4*) (scale + column));
+    float4 w4 = *((float4*) (input + idx));
 
     for (int i = 0; i <= p_grid; i++)
     {
         float pi = __int2float_rn(i) / __int2float_rn(p_grid);
         float p = min_p * (1.0f - pi) + max_p * pi;
 
-        float x = w;
-        float s = scale[column] * p;
-        x /= s;
-        x = rintf(x);
-        x += qzero;
-        x = clamp(x, 0.0f, maxq);
-
-        // Dequantize
-
-        x -= qzero;
-        x *= s;
-
-        // Quantization error
+        float4 s4 = sc4;
+        s4.x *= p;
+        s4.y *= p;
+        s4.z *= p;
+        s4.w *= p;
+
+        float err = __powf(fabsf(clamp(rintf(w4.x / s4.x), clamp_min, clamp_max) * s4.x - w4.x), err_norm);
+        err +=      __powf(fabsf(clamp(rintf(w4.y / s4.y), clamp_min, clamp_max) * s4.y - w4.y), err_norm);
+        err +=      __powf(fabsf(clamp(rintf(w4.z / s4.z), clamp_min, clamp_max) * s4.z - w4.z), err_norm);
+        err +=      __powf(fabsf(clamp(rintf(w4.w / s4.w), clamp_min, clamp_max) * s4.w - w4.w), err_norm);
 
-        x = __powf(fabsf(x - w), err_norm);
-        atomicAdd(&output[i * 128 + column % 128], x);
+        atomicAdd(&output[i * 128 + column_ % 128], err);
     }
 }
 
 void quantize_err_cuda
 (
     const float* input,
     float* output,
@@ -145,15 +344,15 @@
     float err_norm,
     float min_p,
     float max_p,
     int p_grid
 )
 {
     dim3 threads(BLOCKSIZE_X, BLOCKSIZE_Y);
-    dim3 blocks(DIVIDE(columns, BLOCKSIZE_X), DIVIDE(rows, BLOCKSIZE_Y));
+    dim3 blocks(DIVIDE(columns / 4, BLOCKSIZE_X), DIVIDE(rows, BLOCKSIZE_Y));
 
 //     DBGI2(rows, columns);
 //     DBGF2(qzero, maxq);
 
     quantize_err_kernel<<<blocks, threads>>>
     (
         input,
@@ -166,91 +365,54 @@
         err_norm,
         min_p,
         max_p,
         p_grid
     );
 }
 
-__global__ void adjust_error_row_kernel
-(
-    const float* __restrict__ hessian_inv,
-    float* __restrict__ error,
-    const float* __restrict__ weights,
-    const float* __restrict__ quant,
-    int c,
-    int columns,
-    int hcolumns
-)
-{
-    int column = blockIdx.x * blockDim.x + threadIdx.x;
-    if (column >= columns) return;
-
-    float d = hessian_inv[c * hcolumns + c];
-
-    int idx = c * columns + column;
-    float w = weights[idx];
-    float q = quant[idx];
-    error[idx] = (w - q) / d;
-}
-
-void adjust_error_row_cuda
-(
-    const float* hessian_inv,
-    float* error,
-    const float* weights,
-    const float* quant,
-    int c,
-    int columns,
-    int hcolumns
-)
-{
-    dim3 threads(BLOCKSIZE_X, 1);
-    dim3 blocks(DIVIDE(columns, BLOCKSIZE_X), 1);
-
-    adjust_error_row_kernel<<<blocks, threads>>>(hessian_inv, error, weights, quant, c, columns, hcolumns);
-}
-
-
 // Compute z = z - x.T @ y
 
 __global__ void vv_mul_sub_kernel
 (
     const float* __restrict__ x,
     const float* __restrict__ y,
     float* __restrict__ z,
     int x_size,
     int y_size
 )
 {
-    int y_idx = blockIdx.x * blockDim.x + threadIdx.x;
+    int y_idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
     int x_idx = blockIdx.y * blockDim.y + threadIdx.y;
     if (y_idx >= y_size) return;
     if (x_idx >= x_size) return;
-    int z_idx = y_size * x_idx + y_idx;
-
-    float p = x[x_idx] * y[y_idx];
 
-//     curandState state;
-//     int tid = blockIdx.x * blockDim.x + threadIdx.x;
-//     curand_init(1234, tid, clock64(), &state);
-//     float r = curand_uniform(&state);
-//     p *= r;
+    uint64_t z_idx = (uint64_t)y_size * (uint64_t)x_idx + (uint64_t)y_idx;
 
-//    p *- 0.707106478;
-
-    z[z_idx] -= p;
+    float vx = x[x_idx];
+    float4 vy = *((float4*) (y + y_idx));
+    float4 vz = *((float4*) (z + z_idx));
+    vz.x -= vy.x * vx;
+    vz.y -= vy.y * vx;
+    vz.z -= vy.z * vx;
+    vz.w -= vy.w * vx;
+    *((float4*) (z + z_idx)) = vz;
 }
 
 void vv_mul_sub_cuda
 (
     const float* x,
     const float* y,
     float* z,
     int x_size,
     int y_size
 )
 {
-    dim3 threads(BLOCKSIZE_X, BLOCKSIZE_Y);
-    dim3 blocks(DIVIDE(y_size, BLOCKSIZE_X), DIVIDE(x_size, BLOCKSIZE_Y));
+    dim3 blockDim, gridDim;
+    blockDim.x = BLOCKSIZE_X;
+    blockDim.y = BLOCKSIZE_Y;
+    blockDim.z = 1;
+    gridDim.x = DIVIDE(y_size / 4, BLOCKSIZE_X);
+    gridDim.y = DIVIDE(x_size, BLOCKSIZE_Y);
+    gridDim.z = 1;
 
-    vv_mul_sub_kernel<<<blocks, threads>>>(x, y, z, x_size, y_size);
+    vv_mul_sub_kernel<<<gridDim, blockDim>>>(x, y, z, x_size, y_size);
 }
```

## exllamav2/exllamav2_ext/cuda/quantize.cuh

```diff
@@ -14,14 +14,52 @@
     uint16_t* out_q,
     int rows,
     int columns,
     float qzero,
     float maxq
 );
 
+void adjust_error_row_cuda
+(
+    const float* hessian_inv,
+    float* error,
+    const float* weights,
+    const float* quant,
+    int c,
+    int columns,
+    int hcolumns
+);
+
+void fused_quantize_adjust_cuda
+(
+    const float*    weights,
+    float*          quant,
+    const float*    scale,
+    uint16_t*       out_q,
+    const float*    hessian_inv,
+    float*          error,
+    int row,
+    int rows,
+    int columns,
+    float qzero,
+    float maxq
+);
+
+void quantize_rtn_cuda
+(
+    float*          weights,
+    const float*    scale,
+    uint16_t*       out_q,
+    int row,
+    int rows,
+    int columns,
+    float qzero,
+    float maxq
+);
+
 void quantize_err_cuda
 (
     const float* input,
     float* output,
     const float* scale,
     int rows,
     int columns,
@@ -29,25 +67,14 @@
     float maxq,
     float err_norm,
     float min_p,
     float max_p,
     int p_grid
 );
 
-void adjust_error_row_cuda
-(
-    const float* hessian_inv,
-    float* error,
-    const float* weights,
-    const float* quant,
-    int c,
-    int columns,
-    int hcolumns
-);
-
 void vv_mul_sub_cuda
 (
     const float* x,
     const float* y,
     float* z,
     int x_size,
     int y_size
```

## exllamav2/exllamav2_ext/cuda/rms_norm.cu

```diff
@@ -1,18 +1,21 @@
 #include "rms_norm.cuh"
 #include "util.cuh"
+#include "compat.cuh"
 
 #if defined(USE_ROCM)
-#define __shfl_xor_sync(mask, var, laneMask) __shfl_xor(var, laneMask)
+#define NUM_WARPS (1024 / warpSize)
+#define WARP_SIZE (warpSize)
+#else
+#define NUM_WARPS 32
+#define WARP_SIZE 32
 #endif
 
 // y = x * w / sqrt(row_mean(x * x) + epsilon)
 
-#define NUM_WARPS 32
-#define WARP_SIZE 32
 #define BLOCK_SIZE WARP_SIZE
 #define NUM_THREADS (NUM_WARPS * WARP_SIZE)
 
 typedef void (*fp_rms_norm_kernel)
 (
     const half*,
     const half*,
@@ -34,34 +37,38 @@
     const int rows,
     const int dim
 )
 {
     int warp_id = threadIdx.x / WARP_SIZE;
     int lane_id = threadIdx.x % WARP_SIZE;
     int row = blockIdx.x;
-    const half* x_row = x + row * dim;
-    half* y_row = y + row * dim;
-
-    //int blocks_per_warp = DIVIDE(dim, NUM_THREADS);
+    const half2* x_row = (const half2*) (x + row * dim);
+    half2* y_row = (half2*) (y + row * dim);
+    const half2* w2 = (const half2*) w;
 
     // Compute sum of squares for each block
 
     float sum = 0.0f;
-    float itemf[blocks_per_warp];
+    float itemf[blocks_per_warp][2];
 
     #pragma unroll
     for (int i = 0; i < blocks_per_warp; i++)
     {
         int column = warp_id * WARP_SIZE + lane_id + NUM_THREADS * i;
-        if (column >= dim) break;
+        if (column >= dim / 2) break;
 
-        float f = __half2float(x_row[column]);
-        f = fmaxf(-65504.0f, fminf(f, 65504.0f));
-        itemf[i] = f;
-        sum = fma(f, f, sum);
+        half2 x2 = x_row[column];
+        float f0 = __half2float(__low2half(x2));
+        float f1 = __half2float(__high2half(x2));
+        f0 = fmaxf(-65504.0f, fminf(f0, 65504.0f));
+        f1 = fmaxf(-65504.0f, fminf(f1, 65504.0f));
+        itemf[i][0] = f0;
+        itemf[i][1] = f1;
+        sum = fma(f0, f0, sum);
+        sum = fma(f1, f1, sum);
     }
 
     // Shuffle to sum across lanes
 
     __shared__ float sums[NUM_WARPS];
 
     for(int offset = warpSize / 2; offset > 0; offset /= 2) sum += __shfl_xor_sync(0xffffffff, sum, offset);
@@ -75,41 +82,55 @@
 
     // Get norm
 
     float rmf = rsqrtf(sum * r_dim + epsilon);
 
     // Normalize x, scaling by w
 
-    #pragma unroll 4
+    #pragma unroll
     for (int i = 0; i < blocks_per_warp; i++)
     {
         int column = warp_id * WARP_SIZE + lane_id + NUM_THREADS * i;
-        if (column >= dim) return;
+        if (column >= dim / 2) return;
+        half2 w2_ = w2[column];
 
-        float x_itemf = itemf[i];
-        float w_itemf = __half2float(w[column]);
-        float n = x_itemf * w_itemf * rmf;
-        y_row[column] = __float2half_rn(n);
+        float x_itemf0 = itemf[i][0];
+        float x_itemf1 = itemf[i][1];
+        float w_itemf0 = __half2float(__low2half(w2_));
+        float w_itemf1 = __half2float(__high2half(w2_));
+        float n0 = x_itemf0 * w_itemf0 * rmf;
+        float n1 = x_itemf1 * w_itemf1 * rmf;
+        y_row[column] = __halves2half2(__float2half_rn(n0), __float2half_rn(n1));
     }
 }
 
+#define kernel_instance(bpw) \
+    if (blocks_per_warp == bpw) return rms_norm_kernel<bpw>
+
 fp_rms_norm_kernel pick_rms_norm_kernel(const int blocks_per_warp)
 {
-    if (blocks_per_warp == 1) return rms_norm_kernel<1>;
-    if (blocks_per_warp == 2) return rms_norm_kernel<2>;
-    if (blocks_per_warp == 3) return rms_norm_kernel<3>;
-    if (blocks_per_warp == 4) return rms_norm_kernel<4>;
-    if (blocks_per_warp == 5) return rms_norm_kernel<5>;
-    if (blocks_per_warp == 6) return rms_norm_kernel<6>;
-    if (blocks_per_warp == 7) return rms_norm_kernel<7>;
-    if (blocks_per_warp == 8) return rms_norm_kernel<8>;
+    kernel_instance(1);
+    kernel_instance(2);
+    kernel_instance(3);
+    kernel_instance(4);
+    kernel_instance(5);
+    kernel_instance(6);
+    kernel_instance(7);
+    kernel_instance(8);
+    kernel_instance(9);
+    kernel_instance(10);
+    kernel_instance(11);
+    kernel_instance(12);
+    kernel_instance(13);
+    kernel_instance(14);
+    kernel_instance(15);
+    kernel_instance(16);
 	return NULL;
 }
 
-
 void rms_norm_cuda
 (
     const half* x,
     const half* w,
     half* y,
     const float epsilon,
     const int rows,
@@ -120,11 +141,11 @@
     blockDim.x = NUM_THREADS;
     blockDim.y = 1;
     gridDim.x = rows;
     gridDim.y = 1;
 
     float r_dim = 1.0f / (float) dim;
 
-    int blocks_per_warp = DIVIDE(dim, NUM_THREADS);
+    int blocks_per_warp = DIVIDE(dim, NUM_THREADS * 2);
     fp_rms_norm_kernel kernel = pick_rms_norm_kernel(blocks_per_warp);
     kernel<<<gridDim, blockDim>>>(x, w, y, epsilon, r_dim, rows, dim);
 }
```

## exllamav2/exllamav2_ext/cuda/rope.cu

```diff
@@ -3,130 +3,259 @@
 #include "matrix_view.cuh"
 
 const int THREADS_X = 32;
 const int THREADS_Y = 4;
 const int MAX_POS_EMBEDDINGS = 32768;   // Actual number doesn't matter
 const int MAX_ROWS = 32768;             // Actual number doesn't matter
 
-typedef void (*fp_rope_cuda_kernel)
-(
-    half*,
-    const half*,
-    const half*,
-    int,
-    int,
-    int,
-    int,
-    const uint32_t*,
-    int
-);
-
-template<bool use_half2>
-__global__ void rope_cuda_kernel
+__forceinline__ __device__ void rope_cuda_arr_neox
 (
     half* __restrict__ x,
     const half* __restrict__ sin,
     const half* __restrict__ cos,
     int rows_per_batch,
     int head_dim,
     int num_heads,
     int past_len,
-    const uint32_t* __restrict__ past_lens,
+    const int32_t* __restrict__ past_lens,
     int threads_y
 )
 {
     MatrixView_half_rw x_(x, MAX_ROWS, head_dim);
     MatrixView_half sin_(sin, MAX_POS_EMBEDDINGS, head_dim);
     MatrixView_half cos_(cos, MAX_POS_EMBEDDINGS, head_dim);
 
-    int column = (blockIdx.x * THREADS_X + threadIdx.x); if constexpr (use_half2) column *= 2;
+    int column = (blockIdx.x * THREADS_X + threadIdx.x) * 2;
     int half_dim = head_dim / 2;
     if (column >= half_dim) return;
 
     int row = blockIdx.y * threads_y + threadIdx.y;
     if (row >= rows_per_batch) return;
     int batch_offset = blockIdx.z * rows_per_batch;
     int row_offset = batch_offset + row;
 
     // Get sin and cos
 
-    if (past_len == -1) past_len = past_lens[blockIdx.z];
+    if (past_len == -1)
+    {
+        past_len = past_lens[blockIdx.z];
+        past_len = max(past_len, 0);
+    }
+    else if (past_lens)
+    {
+        past_len += past_lens[blockIdx.z];
+    }
+
     int sincos_row = past_len + row / num_heads;
+    sincos_row = max(sincos_row, 0);
+
+    half2 cos2_l = cos_.item_half2(sincos_row, column);
+    half2 cos2_r = cos2_l; //cos_.item_half2(sincos_row, column + half_dim);
+    half2 sin2_l = sin_.item_half2(sincos_row, column);
+    half2 sin2_r = sin2_l; // sin_.item_half2(sincos_row, column + half_dim);
+    sin2_l = __hneg2(sin2_l);
+
+    // Apply embedding to row
+
+    half2 item2_l = x_.item_half2(row_offset, column);
+    half2 item2_r = x_.item_half2(row_offset, column + half_dim);
+    half2 item2_ls = __hmul2(item2_r, sin2_l);
+    half2 item2_rs = __hmul2(item2_l, sin2_r);
+    item2_l = __hfma2(item2_l, cos2_l, item2_ls);
+    item2_r = __hfma2(item2_r, cos2_r, item2_rs);
+    x_.set_half2(row_offset, column, item2_l);
+    x_.set_half2(row_offset, column + half_dim, item2_r);
+}
+
+__forceinline__ __device__ void rope_cuda_arr_gptj
+(
+    half* __restrict__ x,
+    const half* __restrict__ sin,
+    const half* __restrict__ cos,
+    int rows_per_batch,
+    int head_dim,
+    int num_heads,
+    int past_len,
+    const int32_t* __restrict__ past_lens,
+    int threads_y
+)
+{
+    MatrixView_half_rw x_(x, MAX_ROWS, head_dim);
+    MatrixView_half sin_(sin, MAX_POS_EMBEDDINGS, head_dim);
+    MatrixView_half cos_(cos, MAX_POS_EMBEDDINGS, head_dim);
+
+    int column = (blockIdx.x * THREADS_X + threadIdx.x) * 2;
+    if (column >= head_dim) return;
+
+    int row = blockIdx.y * threads_y + threadIdx.y;
+    if (row >= rows_per_batch) return;
+    int batch_offset = blockIdx.z * rows_per_batch;
+    int row_offset = batch_offset + row;
+
+    // Get sin and cos
 
-    if constexpr (use_half2)
+    if (past_len == -1)
     {
-        half2 cos2_l = cos_.item_half2(sincos_row, column);
-        half2 cos2_r = cos_.item_half2(sincos_row, column + half_dim);
-        half2 sin2_l = sin_.item_half2(sincos_row, column);
-        half2 sin2_r = sin_.item_half2(sincos_row, column + half_dim);
-        sin2_l = __hneg2(sin2_l);
-
-        // Apply embedding to row
-
-        half2 item2_l = x_.item_half2(row_offset, column);
-        half2 item2_r = x_.item_half2(row_offset, column + half_dim);
-        half2 item2_ls = __hmul2(item2_r, sin2_l);
-        half2 item2_rs = __hmul2(item2_l, sin2_r);
-        item2_l = __hfma2(item2_l, cos2_l, item2_ls);
-        item2_r = __hfma2(item2_r, cos2_r, item2_rs);
-        x_.set_half2(row_offset, column, item2_l);
-        x_.set_half2(row_offset, column + half_dim, item2_r);
+        past_len = past_lens[blockIdx.z];
+        past_len = max(past_len, 0);
     }
-    else
+    else if (past_lens)
     {
-        half cos_l = cos_.item(sincos_row, column);
-        half cos_r = cos_.item(sincos_row, column + half_dim);
-        half sin_l = sin_.item(sincos_row, column);
-        half sin_r = sin_.item(sincos_row, column + half_dim);
-        sin_l = __hneg(sin_l);
-
-        // Apply embedding to row
-
-        half item_l = x_.item(row_offset, column);
-        half item_r = x_.item(row_offset, column + half_dim);
-        half item_ls = __hmul(item_r, sin_l);
-        half item_rs = __hmul(item_l, sin_r);
-        item_l = __hfma(item_l, cos_l, item_ls);
-        item_r = __hfma(item_r, cos_r, item_rs);
-        x_.set(row_offset, column, item_l);
-        x_.set(row_offset, column + half_dim, item_r);
+        past_len += past_lens[blockIdx.z];
     }
+
+    int sincos_row = past_len + row / num_heads;
+    sincos_row = max(sincos_row, 0);
+
+    half2 cos_01 = cos_.item_half2(sincos_row, column);   // cos[i], cos[i+1]
+    half2_uint32 sin_01_ = half2_uint32(sin_.item_half2(sincos_row, column));
+    sin_01_.as_uint32 ^= (1<<15);
+    half2 sin_01 = sin_01_.as_half2;  // -sin[i], sin[i + 1]
+
+    // Apply embedding to row
+
+    half2 x_01 = x_.item_half2(row_offset, column);  // x[i], x[i+1]
+    half2 x_10 = __lowhigh2highlow(x_01);  // x[i+1], x[i]
+    half2 r = __hmul2(x_01, cos_01);
+    r = __hfma2(x_10, sin_01, r);
+    x_.set_half2(row_offset, column, r);
 }
 
-fp_rope_cuda_kernel pick_rope_cuda_kernel(bool use_half2)
-{
-    if (use_half2) return rope_cuda_kernel<true>;
-    else           return rope_cuda_kernel<false>;
-};
+__global__ void rope_cuda_kernel
+(
+    half* __restrict__ x,
+    const half* __restrict__ sin,
+    const half* __restrict__ cos,
+    int rows_per_batch,
+    int head_dim,
+    int num_heads,
+    int past_len,
+    const int32_t* __restrict__ past_lens,
+    int threads_y,
+    const bool neox_style
+)
+{
+    if (neox_style)
+        rope_cuda_arr_neox(x, sin, cos, rows_per_batch, head_dim, num_heads, past_len, past_lens, threads_y);
+    else
+        rope_cuda_arr_gptj(x, sin, cos, rows_per_batch, head_dim, num_heads, past_len, past_lens, threads_y);
+}
+
+__global__ void rope_cuda_qk_kernel
+(
+    half* __restrict__ x_q,
+    half* __restrict__ x_k,
+    const half* __restrict__ sin,
+    const half* __restrict__ cos,
+    int rows_per_batch_q,
+    int rows_per_batch_k,
+    int head_dim,
+    int num_heads_q,
+    int num_heads_k,
+    int past_len,
+    const int32_t* __restrict__ past_lens,
+    int threads_y,
+    const bool neox_style
+)
+{
+    if (neox_style)
+    {
+        rope_cuda_arr_neox(x_q, sin, cos, rows_per_batch_q, head_dim, num_heads_q, past_len, past_lens, threads_y);
+        rope_cuda_arr_neox(x_k, sin, cos, rows_per_batch_k, head_dim, num_heads_k, past_len, past_lens, threads_y);
+    }
+    else
+    {
+        rope_cuda_arr_gptj(x_q, sin, cos, rows_per_batch_q, head_dim, num_heads_q, past_len, past_lens, threads_y);
+        rope_cuda_arr_gptj(x_k, sin, cos, rows_per_batch_k, head_dim, num_heads_k, past_len, past_lens, threads_y);
+    }
+}
 
 void rope_cuda
 (
     half* x,
     const half* sin,
     const half* cos,
     const int batch_size,
     const int rows_per_batch,
     const int head_dim,
     const int num_heads,
     const int past_len,
-    const uint32_t* past_lens
+    const int32_t* past_lens,
+    const bool neox_style
 )
 {
-    bool use_half2 = true;
-
     // For large batch sizes we risk exceeding grid dimension of 65535, so shift to block dimension instead
 
     int threads_y = THREADS_Y;
     while (DIVIDE(rows_per_batch, threads_y) > 65535) threads_y *= 2;
 
     dim3 blockDim, gridDim;
     blockDim.x = THREADS_X;
     blockDim.y = threads_y;
-    gridDim.x = DIVIDE(head_dim, THREADS_X) / (use_half2 ? 2 : 1);
+    gridDim.x = DIVIDE(head_dim / 2, THREADS_X);
     gridDim.y = DIVIDE(rows_per_batch, threads_y);
     gridDim.z = batch_size;
 
-    fp_rope_cuda_kernel kernel = pick_rope_cuda_kernel(use_half2);
-    kernel<<<gridDim, blockDim>>>(x, sin, cos, rows_per_batch, head_dim, num_heads, past_len, past_lens, threads_y);
+    rope_cuda_kernel<<<gridDim, blockDim>>>
+    (
+        x,
+        sin,
+        cos,
+        rows_per_batch,
+        head_dim,
+        num_heads,
+        past_len,
+        past_lens,
+        threads_y,
+        neox_style
+    );
+}
 
-    cuda_check( cudaPeekAtLastError() );
+void rope_cuda_qk
+(
+    half* x_q,
+    half* x_k,
+    const half* sin,
+    const half* cos,
+    const int batch_size,
+    const int rows_per_batch_q,
+    const int rows_per_batch_k,
+    const int head_dim,
+    const int num_heads_q,
+    const int num_heads_k,
+    const int past_len,
+    const int32_t* past_lens,
+    const bool neox_style
+)
+{
+    // For large batch sizes we risk exceeding grid dimension of 65535, so shift to block dimension instead
+
+    int threads_y = THREADS_Y;
+    int rows_per_batch = max(rows_per_batch_q, rows_per_batch_k);
+    while (DIVIDE(rows_per_batch, threads_y) > 65535) threads_y *= 2;
+
+    dim3 blockDim, gridDim;
+    blockDim.x = THREADS_X;
+    blockDim.y = threads_y;
+    gridDim.x = DIVIDE(head_dim / 2 / (neox_style ? 2 : 1), THREADS_X);
+    gridDim.y = DIVIDE(rows_per_batch, threads_y);
+    gridDim.z = batch_size;
+
+    rope_cuda_qk_kernel<<<gridDim, blockDim>>>
+    (
+        x_q,
+        x_k,
+        sin,
+        cos,
+        rows_per_batch_q,
+        rows_per_batch_k,
+        head_dim,
+        num_heads_q,
+        num_heads_k,
+        past_len,
+        past_lens,
+        threads_y,
+        neox_style
+    );
 }
+
+
```

## exllamav2/exllamav2_ext/cuda/rope.cuh

```diff
@@ -12,11 +12,29 @@
     const half* sin,
     const half* cos,
     const int batch_size,
     const int rows_per_batch,
     const int head_dim,
     const int num_heads,
     const int past_len,
-    const uint32_t* past_lens
+    const int32_t* past_lens,
+    const bool neox_style
+);
+
+void rope_cuda_qk
+(
+    half* x_q,
+    half* x_k,
+    const half* sin,
+    const half* cos,
+    const int batch_size,
+    const int rows_per_batch_q,
+    const int rows_per_batch_k,
+    const int head_dim,
+    const int num_heads_q,
+    const int num_heads_k,
+    const int past_len,
+    const int32_t* past_lens,
+    const bool neox_style
 );
 
 #endif
```

## exllamav2/exllamav2_ext/cuda/util.cuh

```diff
@@ -1,26 +1,37 @@
+#ifndef _util_cuh
+#define _util_cuh
+
+#include <cuda_runtime.h>
+#include <cuda_fp16.h>
+#include <cstdint>
+#include <cstdio>
+#include <ATen/cuda/CUDAContext.h>
 
 #define DIVIDE(x, size) (((x) + (size) - 1) / (size))
 
 #define DBGS(__x) printf("%s\n", __x)
 #define DBGI(__x) printf("%s: %i\n", #__x, __x)
 #define DBGI2(__x, __y) printf("%s, %s: %i, %i\n", #__x, #__y, __x, __y)
 #define DBGI3(__x, __y, __z) printf("%s, %s, %s: %i, %i, %i\n", #__x, #__y, #__z, __x, __y, __z)
+#define DBGI4(__x, __y, __z, __w) printf("%s, %s, %s, %s: %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, __x, __y, __z, __w)
 #define DBGX(__x) printf("%s: %x\n", #__x, __x)
 #define DBGX2(__x, __y) printf("%s, %s: %x, %x\n", #__x, #__y, __x, __y)
 #define DBGX3(__x, __y, __z) printf("%s, %s, %s: %x, %x, %x\n", #__x, #__y, #__z, __x, __y, __z)
 #define DBGF(__x) printf("%s: %f\n", #__x, __x)
 #define DBGF2(__x, __y) printf("%s, %s: %f, %f\n", #__x, #__y, __x, __y)
 #define DBGF3(__x, __y, __z) printf("%s, %s, %s: %f, %f, %f\n", #__x, #__y, #__z, __x, __y, __z)
 #define DBGH(__x) printf("%s: %f\n", #__x, __half2float(__x))
 #define DBGH2(__x, __y) printf("%s, %s: %f, %f\n", #__x, #__y, __half2float(__x), __half2float(__y))
 #define DBGH3(__x, __y, __z) printf("%s, %s, %s: %f, %f, %f\n", #__x, #__y, #__z, __half2float(__x), __half2float(__y), __half2float(__z))
 
 #define DBGIH(__x, __y) printf("%s, %s: %i, %f\n", #__x, #__y, __x, __half2float(__y))
 #define DBGIH2(__x, __y, __z) printf("%s, %s, %s: %i, %f, %f\n", #__x, #__y, #__z, __x, __half2float(__y), __half2float(__z))
+#define DBGI2H2(__x, __y, __z, __w) printf("%s, %s, %s, %s: %i, %i, %f, %f\n", #__x, #__y, #__z, #__w, __x, __y, __half2float(__z), __half2float(__w))
+#define DBGIH3(__x, __y, __z, __w) printf("%s, %s, %s, %s: %i, %f, %f, %f\n", #__x, #__y, #__z, #__w, __x, __half2float(__y), __half2float(__z), __half2float(__w))
 
 __forceinline__ __device__ half dq_scale_(const int qs, const half max_scale)
 {
     half qs_h = __hmul(__int2half_rn(qs + 1), __float2half_rn(1.0f / 16.0f));
     qs_h = __hmul(qs_h, qs_h);
     qs_h = __hmul(qs_h, max_scale);
     return qs_h;
@@ -36,7 +47,42 @@
 {
    if (code != cudaSuccess)
    {
       fprintf(stderr,"CUDA error: %s %s %d\n", cudaGetErrorString(code), file, line);
       if (abort) exit(code);
    }
 }
+
+void print_global_mem(const half* ptr, int rows, int columns, int stride);
+
+__forceinline__ __device__ void print_smem(const uint8_t* ptr, int count)
+{
+    for (int i = 0; i < count; ++i)
+    {
+        printf("%4d ", *ptr++);
+    }
+    printf("\n");
+}
+
+__forceinline__ __device__ void print_smem(const half* ptr, int count)
+{
+    for (int i = 0; i < count; ++i)
+    {
+        printf("%6.3f", __half2float(*ptr++));
+    }
+    printf("\n");
+}
+
+//inline __device__ half2 ___hmax2(half2 a, half2 b)
+//{
+//    half a_low = __low2half(a);
+//    half a_high = __high2half(a);
+//    half b_low = __low2half(b);
+//    half b_high = __high2half(b);
+//
+//    half max_low = __hgt(a_low, b_low) ? a_low : b_low;
+//    half max_high = __hgt(a_high, b_high) ? a_high : b_high;
+//
+//    return __halves2half2(max_low, max_high);
+//}
+
+#endif
```

## exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh

```diff
@@ -2,15 +2,124 @@
 #define _qdq_6_cuh
 
 #include "qdq_util.cuh"
 #include "../../config.h"
 
 #if QMODE_6BIT == 1
 
-  // Not implemented
+// Permutation:
+//
+// dddd3333 33111111  cccc2222 22000000
+// ffff7777 77555555  eeee6666 66444444
+// ffddbbbb bb999999  eeccaaaa aa888888
+
+__forceinline__ __device__ void shuffle_6bit_16
+(
+    uint32_t* q,
+    int stride
+)
+{
+    uint32_t qa = q[0 * stride];
+    uint32_t qb = q[1 * stride];
+    uint32_t qc = q[2 * stride];
+
+    // qa: 55444444 33333322  22221111 11000000
+    // qb: aaaa9999 99888888  77777766 66665555
+    // qc: ffffffee eeeedddd  ddcccccc bbbbbbaa
+
+    uint32_t q00 = (qa      ) & 0b111111;
+    uint32_t q01 = (qa >>  6) & 0b111111;
+    uint32_t q02 = (qa >> 12) & 0b111111;
+    uint32_t q03 = (qa >> 18) & 0b111111;
+    uint32_t q04 = (qa >> 24) & 0b111111;
+    uint32_t q05 = ((qa >> 30) & 0b11) | ((qb & 0b1111) << 2);
+    uint32_t q06 = (qb >>  4) & 0b111111;
+    uint32_t q07 = (qb >> 10) & 0b111111;
+    uint32_t q08 = (qb >> 16) & 0b111111;
+    uint32_t q09 = (qb >> 22) & 0b111111;
+    uint32_t q0a = ((qb >> 28) & 0b1111) | ((qc & 0b11) << 4);
+    uint32_t q0b = (qc >>  2) & 0b111111;
+    uint32_t q0c = (qc >>  8) & 0b111111;
+    uint32_t q0d = (qc >> 14) & 0b111111;
+    uint32_t q0e = (qc >> 20) & 0b111111;
+    uint32_t q0f = (qc >> 26) & 0b111111;
+
+    qa = q00 | (q01 << 16) | (q02 << 6) | (q03 << 22);
+    qb = q04 | (q05 << 16) | (q06 << 6) | (q07 << 22);
+    qc = q08 | (q09 << 16) | (q0a << 6) | (q0b << 22);
+
+    // qa: ....3333 33111111  ....2222 22000000
+    // qb: ....7777 77555555  ....6666 66444444
+    // qc: ....bbbb bb999999  ....aaaa aa888888
+
+    qa |= (q0c & 0b001111) << 12;
+    qc |= (q0c & 0b110000) << 8;
+    qa |= (q0d & 0b001111) << 28;
+    qc |= (q0d & 0b110000) << 24;
+
+    // qa: dddd3333 33111111  cccc2222 22000000
+    // qb: ....7777 77555555  ....6666 66444444
+    // qc: ..ddbbbb bb999999  ..ccaaaa aa888888
+
+    qb |= (q0e & 0b001111) << 12;
+    qc |= (q0e & 0b110000) << 10;
+    qb |= (q0f & 0b001111) << 28;
+    qc |= (q0f & 0b110000) << 26;
+
+    // qa: dddd3333 33111111  cccc2222 22000000
+    // qb: ffff7777 77555555  eeee6666 66444444
+    // qc: ffddbbbb bb999999  eeccaaaa aa888888
+
+    q[0 * stride] = qa;
+    q[1 * stride] = qb;
+    q[2 * stride] = qc;
+}
+
+__forceinline__ __device__ void dequant_6bit_16
+(
+    const uint32_t q_0,
+    const uint32_t q_1,
+    const uint32_t q_2,
+    half2 (&dq)[8],
+    int stride
+)
+{
+    const uint32_t c0 = 0x64006400;
+    const half z1_  = __float2half_rn(-1024.0f - 32.0f);
+    const half2 z1  = __halves2half2(z1_, z1_);
+
+    uint32_t qa = q_0;
+    uint32_t qb = q_1;
+    uint32_t qc = q_2;
+
+    half2_uint32 q0((qa & 0x003f003f) | c0); // half2(q[ 0], q[ 1])      + 1024
+    qa >>= 6;
+    half2_uint32 q1((qa & 0x003f003f) | c0); // half2(q[ 2], q[ 3])      + 1024
+    qa >>= 6;
+    half2_uint32 q2((qb & 0x003f003f) | c0); // half2(q[ 4], q[ 5])      + 1024
+    qb >>= 6;
+    half2_uint32 q3((qb & 0x003f003f) | c0); // half2(q[ 6], q[ 7])      + 1024
+    qb >>= 6;
+    half2_uint32 q4((qc & 0x003f003f) | c0); // half2(q[ 8], q[ 9])      + 1024
+    qc >>= 6;
+    half2_uint32 q5((qc & 0x003f003f) | c0); // half2(q[10], q[11])      + 1024
+    qc >>= 2;
+    half2_uint32 q6((qa & 0x000f000f) | (qc & 0x00300030) | c0); // half2(q[12], q[13])      + 1024
+    qc >>= 2;
+    half2_uint32 q7((qb & 0x000f000f) | (qc & 0x00300030) | c0); // half2(q[14], q[15])      + 1024
+
+    dq[0] = __hadd2(q0.as_half2, z1);
+    dq[1] = __hadd2(q1.as_half2, z1);
+    dq[2] = __hadd2(q2.as_half2, z1);
+    dq[3] = __hadd2(q3.as_half2, z1);
+    dq[4] = __hadd2(q4.as_half2, z1);
+    dq[5] = __hadd2(q5.as_half2, z1);
+    dq[6] = __hadd2(q6.as_half2, z1);
+    dq[7] = __hadd2(q7.as_half2, z1);
+}
 
 #else
 
 __forceinline__ __device__ void shuffle_6bit_16
 (
     uint32_t* q,
     int stride
```

## exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh

```diff
@@ -3,22 +3,24 @@
 
 union half2_uint32
 {
     uint32_t as_uint32;
     half2 as_half2;
     __device__ half2_uint32(uint32_t val) : as_uint32(val) {}
     __device__ half2_uint32(half2 val) : as_half2(val) {}
+    __device__ half2_uint32() : as_uint32(0) {}
 };
 
 union half_uint16
 {
     uint16_t as_uint16;
     half as_half;
     __device__ half_uint16(uint16_t val) : as_uint16(val) {}
     __device__ half_uint16(half val) : as_half(val) {}
+    __device__ half_uint16() : as_uint16(0) {}
 };
 
 // Max_scale premultiplied by 1/256
 
 __forceinline__ __device__ half dq_scale(const int qs, const half max_scale)
 {
     int qs_i = qs + 1;
```

## exllamav2/generator/__init__.py

```diff
@@ -1,10 +1,8 @@
 from exllamav2.version import __version__
 
 from exllamav2.generator.sampler import ExLlamaV2Sampler
 from exllamav2.generator.base import ExLlamaV2BaseGenerator
 from exllamav2.generator.streaming import ExLlamaV2StreamingGenerator
-
-
-
-
-
+from exllamav2.generator.dynamic import ExLlamaV2DynamicGenerator, ExLlamaV2DynamicJob
+from exllamav2.generator.dynamic_async import ExLlamaV2DynamicGeneratorAsync, ExLlamaV2DynamicJobAsync
+from exllamav2.generator.hooks import ExLlamaV2PostSamplingHook, ExLlamaV2PostSamplingResult
```

## exllamav2/generator/base.py

```diff
@@ -1,140 +1,353 @@
+from __future__ import annotations
+
 from exllamav2 import (
     ExLlamaV2,
-    ExLlamaV2Cache,
+    ExLlamaV2CacheBase,
     ExLlamaV2Tokenizer,
     ExLlamaV2Lora,
 )
 from exllamav2.generator import (
     ExLlamaV2Sampler
 )
+from exllamav2.generator.filters import ExLlamaV2Filter
 import torch
 import random
-
-import torch.nn.functional as F
+import threading
+from exllamav2.generator.hooks import ExLlamaV2PostSamplingHook, ExLlamaV2PostSamplingResult
+from exllamav2.embedding import EMBEDDING_INDEX
 
 class ExLlamaV2BaseGenerator:
 
     # Internal state
 
     model: ExLlamaV2
-    cache: ExLlamaV2Cache
+    cache: ExLlamaV2CacheBase
     tokenizer: ExLlamaV2Tokenizer
 
-    sequence_ids: torch.tensor = None
+    sequence_ids: torch.Tensor | None
+
+    abort_event: threading.Event | None
+
 
-    def __init__(self, model, cache, tokenizer):
+    def __init__(self,
+                 model: ExLlamaV2,
+                 cache: ExLlamaV2CacheBase,
+                 tokenizer: ExLlamaV2Tokenizer):
 
         self.model = model
         self.cache = cache
         self.tokenizer = tokenizer
-
+        self.sequence_ids = None
+        self.abort_event = None
 
     # For testing purposes, run a forward pass to make sure CUDA is fully initialized
 
     def warmup(self):
 
         input_ids = torch.zeros((1, 2), dtype = torch.long)
         self.model.forward(input_ids, cache = None, input_mask = None, preprocess_only = True)
+        torch.cuda.synchronize()
 
 
     def full(self):
 
         return self.sequence_ids.shape[-1] >= self.model.config.max_seq_len
 
 
-    # TODO: Argument to allow different random samples over batch dimension
+    def generate_simple(
+        self,
+        prompt: str or list,
+        gen_settings: ExLlamaV2Sampler.Settings,
+        num_tokens: int,
+        seed: int or None = None,
+        token_healing: bool = False,
+        encode_special_tokens: bool = False,
+        decode_special_tokens: bool = False,
+        loras: ExLlamaV2Lora or list[ExLlamaV2Lora] | None = None,
+        stop_token: int or None = -1,
+        add_bos: bool = False,
+        abort_event: threading.Event | None = None,
+        input_embeddings: torch.Tensor | None = None,
+        completion_only: bool = False,
+        filters: list[ExLlamaV2Filter] | None = None,
+        filter_prefer_eos: bool = False,
+    ):
+
+        """
+        Generate one or more completions.
+
+        :param prompt:
+            String or list of strings. If this argument is a list, its length determinse the batch size, and
+            the output will be list of strings as well.
+
+        :param gen_settings:
+            ExLlamaV2Sampler.Settings
+
+        :param num_tokens:
+            Max number of tokens to generate.
+
+        :param seed:
+            Seed for the sampling RNG. Doesn't guarantee perfect determinism from the implementation.
+
+        :param token_healing:
+            Apply token healing by regenerating the last token of the input sequence with prefix
+            constraint.
+
+        :param encode_special_tokens:
+            Encode special tokens (BOS etc.) represented as text in the input. If False, special tokens are
+            interpreted as text by the tokenizer.
+
+        :param decode_special_tokens:
+            Decode special tokens output by the model. If False, tokens marked as special in the tokenizer
+            are decoded as empty strings.
+
+        :param loras:
+            (List of) ExLlamaV2Lora objects to apply during generation
+
+        :param stop_token:
+            ID of the stop token. If this argument is None, no stop token will be considered. The default
+            value is -1, which is interpreted as whatever the EOS token is defined to be in the tokenizer
+            model.
+
+        :param add_bos:
+            Prepend the tokenizer's specified BOS token to the input.
+
+        :param abort_event:
+            Forwarded to the model during generation. Will abort prefill/context ingestion if triggered.
+
+        :param input_embeddings:
+            Tensor of shape (batch_size, n, hidden_size) added to the beginning of the prompt. Batching
+            is not supported when passing input embeddings unless all prompts are the same. Prompt must
+            contain the string `{{EMBED_HERE}}` to indicate where embeddings are to be inserted.
+
+        :param completion_only:
+            Only return completion. If False, returned string will include the input prompt.
+
+        :param filters:
+            List of ExLlamaV2Filters to apply during generation.
+
+        :param filter_prefer_eos:
+            If True, always sample the tokenizer's defined EOS token as soon as it's allowed by the filters
+
+        :return:
+            Completion(s) (str or list[str] depending on the type of the input prompt argument)
+        """
+
+
+        self.abort_event = abort_event
+        if self.abort_event: self.abort_event.clear()
+
+        # Filters
 
-    def generate_simple(self, prompt: str or list,
-                        gen_settings: ExLlamaV2Sampler.Settings,
-                        num_tokens: int,
-                        seed = None,
-                        token_healing = False,
-                        encode_special_tokens = False,
-                        decode_special_tokens = False,
-                        loras = None,
-                        stop_token = -1):
+        if filters is None: filters = []
 
         # Default stop token
 
         if stop_token == -1: stop_token = self.tokenizer.eos_token_id
 
         # Accept LoRA or list of LoRAs
 
         if loras is not None and isinstance(loras, ExLlamaV2Lora): loras = [loras]
 
         # Apply seed
 
         if seed is not None: random.seed(seed)
 
-        # Tokenize input and produce padding mask if needed
+        # Tokenize input and produce padding mask if needed, inserting embeddings if provided
 
         batch_size = 1 if isinstance(prompt, str) else len(prompt)
-        ids = self.tokenizer.encode(prompt, encode_special_tokens = encode_special_tokens)
+        prompts_identical = batch_size == 1 or all(s == prompt[0] for s in prompt)
+
+        if input_embeddings is not None:
+
+            embed_marker = "{{EMBED_HERE}}"
+            prompt_split = prompt.split(embed_marker)
+            assert len(prompt_split) == 2, \
+                f"Prompt must contain one instance of {embed_marker} when embeddings are provided"
+
+            if batch_size > 1: assert prompts_identical, \
+                "Batched generation with input embeddings requires all prompts to be identical."
+
+            assert input_embeddings.shape[0] == batch_size, \
+                "Input embeddings tensor does not match batch size of prompt."
+
+            pre_ids, _ = self.tokenizer.encode(prompt_split[0].rstrip(" \t"),
+                                               encode_special_tokens = encode_special_tokens,
+                                               return_offsets = True,
+                                               add_bos = add_bos)
+            post_ids, _ = self.tokenizer.encode(prompt_split[1].lstrip(" \t"),
+                                               encode_special_tokens = encode_special_tokens,
+                                               return_offsets = True,
+                                               add_bos = False)
+
+            num_emb_tokens = input_embeddings.shape[1]
+            image_ids = torch.arange(EMBEDDING_INDEX, EMBEDDING_INDEX + num_emb_tokens, dtype = torch.long).unsqueeze(0)
+            ids = torch.cat((pre_ids, image_ids, post_ids), dim = -1)
+
+            position_offsets = None
+
+        else:
+            ids, position_offsets = self.tokenizer.encode(prompt,
+                                                          encode_special_tokens = encode_special_tokens,
+                                                          return_offsets = True,
+                                                          add_bos = add_bos)
+
+            if prompts_identical:
+                position_offsets = None
+
+        # Truncate prompt if generation would cause cache overflow
 
         overflow = ids.shape[-1] + num_tokens - self.model.config.max_seq_len
         if overflow > 0: ids = ids[:, overflow:]
+        else: overflow = 0
 
         mask = self.tokenizer.padding_mask(ids) if batch_size > 1 else None
 
+        first_token = max(-overflow, 0)
+
+        # Completion only
+
+        if completion_only:
+            first_token = ids.shape[-1]
+
         # Prepare for healing
 
         unhealed_token = None
         if ids.shape[-1] < 2: token_healing = False
         if token_healing:
             unhealed_token = ids[:, -1:]
             ids = ids[:, :-1]
 
         # Process prompt and begin gen
 
-        self._gen_begin_base(ids, mask, loras)
+        self._gen_begin_base(ids,
+                             mask,
+                             loras,
+                             position_offsets = position_offsets,
+                             input_embeddings = input_embeddings)
+
+        if self.abort_event and self.abort_event.is_set():
+            if isinstance(prompt, str): return ""
+            else: return [""] * len(prompt)
+
+        # Remove indexed embeddings from generator's sequence
+
+        if input_embeddings is not None:
+            self.sequence_ids[self.sequence_ids >= EMBEDDING_INDEX] = self.tokenizer.pad_token_id
 
         # Begin filters
 
+        healed_token = []
         id_to_piece = self.tokenizer.get_id_to_piece_list()
         if unhealed_token is not None:
             unhealed_token_list = unhealed_token.flatten().tolist()
             heal = [id_to_piece[x] for x in unhealed_token_list]
         else:
             heal = None
-        gen_settings.begin_filters(heal)
+
+        for f in filters: f.begin(heal)
 
         # Generate tokens
 
         batch_eos = [False] * batch_size
 
         for i in range(num_tokens):
 
-            logits = self.model.forward(self.sequence_ids[:, -1:], self.cache, input_mask = mask, loras = loras).float().cpu()
-            token, _, _ = ExLlamaV2Sampler.sample(logits, gen_settings, self.sequence_ids, random.random(), self.tokenizer, prefix_token = unhealed_token)
+            if self.abort_event and self.abort_event.is_set():
+                break
+
+            logits = self.model.forward(self.sequence_ids[:, -1:],
+                                        self.cache,
+                                        input_mask = mask,
+                                        loras = loras,
+                                        position_offsets = position_offsets,
+                                        indexed_embeddings = input_embeddings).float().cpu()
+
+            token, ptokens, pprobs, prob, eos = \
+            ExLlamaV2Sampler.sample(
+                logits,
+                gen_settings,
+                self.sequence_ids,
+                random.random(),
+                self.tokenizer,
+                prefix_token = unhealed_token,
+                filters = filters,
+                filter_prefer_eos = filter_prefer_eos
+            )
+
+            if unhealed_token is not None:
+                unhealed_token_copy = unhealed_token
+                healed_token = token
 
-            eos = False
             if stop_token is not None:
                 for b in range(batch_size):
                     if token[b, 0].item() == stop_token:
                         batch_eos[b] = True
                         if all(batch_eos): eos = True
                     if batch_eos[b]:
                         token[b, 0] = self.tokenizer.pad_token_id
 
+            # Post sampling hook
+
+            if gen_settings.post_sampling_hooks:
+                p = ExLlamaV2PostSamplingResult(
+                    sampled_token = token,
+                    sampled_prob = prob,
+                    logits = logits,
+                    candidate_tokens = None if ptokens.is_meta else ptokens,
+                    candidate_probs = None if pprobs.is_meta else pprobs
+                )
+                for h in gen_settings.post_sampling_hooks:
+                    h(p)
+                token = p.sampled_token
+                if p.feed_filters:
+                    for f in filters: f.feed(token)
+            else:
+                for f in filters: f.feed(token)
+
             self.sequence_ids = torch.cat([self.sequence_ids, token], dim = 1)
-            gen_settings.feed_filters(token)
 
             unhealed_token = None
             if eos: break
 
         # Decode
 
-        text = self.tokenizer.decode(self.sequence_ids, decode_special_tokens = decode_special_tokens)
+        decode_ids = self.sequence_ids[:, first_token:]
+        if input_embeddings is not None:
+            decode_ids = torch.stack([decode_ids[i][decode_ids[i] != self.tokenizer.pad_token_id] for i in range(batch_size)])
 
-        if isinstance(prompt, str): return text[0]
-        return text
+        if len(healed_token) and completion_only:
+            decode_ids = torch.cat([healed_token, decode_ids], dim = -1)
 
+        text = self.tokenizer.decode(decode_ids, decode_special_tokens = decode_special_tokens)
 
-    def _gen_begin_base(self, input_ids, mask = None, loras = None):
+        if len(healed_token) and completion_only:
+            pre_text = self.tokenizer.decode(unhealed_token_copy, decode_special_tokens = decode_special_tokens)
+            text = [t[len(p):] for t, p in zip(text, pre_text)]
 
-        self.cache.current_seq_len = 0
-        self.model.forward(input_ids[:, :-1], self.cache, input_mask = mask, preprocess_only = True, loras = loras)
+        if isinstance(prompt, str):
+            return text[0]
+        else:
+            return text
 
-        self.sequence_ids = input_ids.clone()
+
+    def _gen_begin_base(self,
+                        input_ids: torch.Tensor,
+                        mask: torch.Tensor | None = None,
+                        loras: ExLlamaV2Lora or list[ExLlamaV2Lora] or None = None,
+                        position_offsets: torch.Tensor | None = None,
+                        input_embeddings: torch.Tensor | None = None):
+
+        self.cache.current_seq_len = 0
         self.sequence_ids = input_ids
 
+        self.model.forward(input_ids[:, :-1],
+                           self.cache,
+                           input_mask = mask,
+                           preprocess_only = True,
+                           loras = loras,
+                           position_offsets = position_offsets,
+                           abort_event = self.abort_event,
+                           indexed_embeddings = input_embeddings)
+        if self.abort_event and self.abort_event.is_set():
+            self.sequence_ids = self.sequence_ids[:, :self.cache.current_seq_len + 1]
```

## exllamav2/generator/sampler.py

```diff
@@ -1,185 +1,293 @@
+from __future__ import annotations
+from dataclasses import dataclass, field
 import torch
+import torch.nn.functional as F
 from exllamav2 import ExLlamaV2Tokenizer
+from exllamav2.generator.filters import ExLlamaV2Filter
+from exllamav2.generator.hooks import ExLlamaV2PostSamplingHook
 from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
-
+from copy import copy
 
 class ExLlamaV2Sampler:
 
+    @dataclass
     class Settings:
 
-        token_repetition_penalty = 1.15
-        token_repetition_range = -1
-        token_repetition_decay = 0
-
-        temperature = 0.9
-        top_k = 40
-        top_p = 0.9
-        min_p = 0
-        tfs = 0
-        typical = 0
-
-        temperature_last = False
-
-        mirostat = False
-        mirostat_tau = 1.5
-        mirostat_eta = 0.1
-        mirostat_mu = None  # (re)initialized from mirostat_tau on first sample
-
-        token_bias = None
-
-        filters = []
+        token_repetition_penalty: float = 1.025
+        token_repetition_range: int = -1
+        token_repetition_decay: int  = 0
+
+        token_frequency_penalty: float = 0.0
+        token_presence_penalty: float = 0.0
+
+        temperature: float = 0.8
+        smoothing_factor: float = 0.0
+        min_temp: float = 0
+        max_temp: float = 0.0
+        temp_exponent: float = 1.0
+        top_k: int = 50
+        top_p: float = 0.8
+        top_a: float = 0.0
+        min_p: float = 0
+        tfs: float = 0
+        typical: float = 0
+        skew: float = 0
+
+        temperature_last: bool = False
+
+        mirostat: bool = False
+        mirostat_tau: float = 1.5
+        mirostat_eta: float = 0.1
+        mirostat_mu: float | None = None  # (re)initialized from mirostat_tau on first sample
+
+        token_bias: torch.Tensor | None = None
+        cfg_scale: float | None = None
+
+        post_sampling_hooks: list[ExLlamaV2PostSamplingHook] = field(default_factory = list)
+
+        @staticmethod
+        def greedy(**kwargs) -> ExLlamaV2Sampler.Settings:
+            defaults = {
+                "temperature": 1.0,
+                "token_repetition_penalty": 1.0,
+                "top_p": 0.0,
+                "top_k": 1,
+            }
+            defaults.update(kwargs)
+            return ExLlamaV2Sampler.Settings(**defaults)
 
 
         def clone(self):
-
-            c = ExLlamaV2Sampler.Settings()
-
-            c.token_repetition_penalty = self.token_repetition_penalty
-            c.token_repetition_range = self.token_repetition_range
-            c.token_repetition_decay = self.token_repetition_decay
-
-            c.temperature = self.temperature
-            c.top_k = self.top_k
-            c.top_p = self.top_p
-            c.min_p = self.min_p
-            c.tfs = self.tfs
-            c.typical = self.typical
-
-            c.mirostat = self.mirostat
-            c.mirostat_tau = self.mirostat_tau
-            c.mirostat_eta = self.mirostat_eta
-            c.mirostat_mu = None if self.mirostat_mu is None else self.mirostat_mu.copy()
-
-            c.token_bias = self.token_bias
-            c.filters = [f.clone() for f in self.filters]
-
+            c = copy(self)
             return c
 
 
         def greedy_clone(self):
-
             c = ExLlamaV2Sampler.Settings()
             c.top_k = 1
             c.top_p = 0
             c.token_repetition_penalty = self.token_repetition_penalty
             c.token_repetition_range = self.token_repetition_range
             c.token_repetition_decay = self.token_repetition_decay
-            c.token_bias = self.token_bias
+            c.token_frequency_penalty = self.token_frequency_penalty
+            c.token_presence_penalty = self.token_presence_penalty
+            c.token_bias = None
             c.filters = []
             return c
 
 
-        def disallow_tokens(self, tokenizer, tokens):
+        def disallow_tokens(
+            self,
+            tokenizer: ExLlamaV2Tokenizer,
+            tokens: list[int]
+        ):
+            """Utility function to set/update the logit bias, disallowing specific tokens in the supplied list"""
 
             if self.token_bias is None:
                 padding = -tokenizer.config.vocab_size % 32
                 self.token_bias = torch.zeros((tokenizer.config.vocab_size + padding,), dtype = torch.float)
 
             self.token_bias[tokens] = float("-inf")
 
 
-        def begin_filters(self, prefix_str = ""):
-
-            for f in self.filters: f.begin(prefix_str)
-
-
-        def feed_filters(self, feed_token):
-
-            for f in self.filters: f.feed(feed_token)
-
-
     @staticmethod
-    def sample(logits: torch.tensor, settings: Settings, sequence_ids: torch.tensor, random: float, tokenizer: ExLlamaV2Tokenizer, prefix_token = None):
+    def sample(
+        logits: torch.tensor,
+        settings: Settings,
+        sequence_ids: torch.tensor,
+        random: float,
+        tokenizer: ExLlamaV2Tokenizer,
+        prefix_token: torch.Tensor | None = None,
+        return_top_tokens: int = 0,
+        blocked_tokens: list[int] | None = None,
+        filters: list[ExLlamaV2Filter] | None = None,
+        filter_prefer_eos: bool = False
+    ):
+
+        """
+        Sample tokens from (batched) logits tensor
+
+        :param logits:
+            Input logits, float tensor of shape (batch_size, 1, vocab_size)
+
+        :param settings:
+            ExLlamaV2Sampler.Settings
+
+        :param sequence_ids:
+            Past token IDs to consider for repetition penalty etc., shape (batch_size, seq_len)
+
+        :param random:
+            Float between 0 and 1, determining sampling point in the final normalized distribution.
+
+        :param tokenizer:
+            ExLlamaV2Tokenizer
+
+        :param prefix_token:
+            Tensor of shape (batch_size, 1). If provided, sampling will be restricted to token pieces that begin with
+            this token. Used for token healing.
+
+        :param return_top_tokens:
+            Number of top tokens to return
+
+        :param blocked_tokens:
+            List of tokens to ban temporarily
+
+        :param filters:
+            List of ExLlamaV2Filters. Sampling will be constrained to the intersection of allowed tokens for all
+            filters.
+
+        :param filter_prefer_eos:
+            If True, always sample the tokenizer's defined EOS token as soon as it's allowed by the filters
+
+        :return:
+            Tuple of:
+            - Sampled tokens, tensor of shape (batch_size, 1)
+            - Top candidates per token (batch_size, 1, return_top_tokens), or meta tensor if return_top_tokens = 0
+            - Top probs per token (batch_size, 1, return_top_tokens), or meta tensor if return_top_tokens = 0
+            - Probabilities per token, shape (batch_size, 1)
+            - True if the current filter has reached a stop condition
+        """
 
         batch_size, _, vocab_size = logits.shape
+        if filters is None: filters = []
+
+        assert logits.shape[1] == 1, \
+            "Logits tensor is incorrect shape, must be (bsz, 1, vocab_size)"
+        assert prefix_token is None or prefix_token.shape == (batch_size, 1), \
+            "Prefix token list doesn't match batch shape"
+        if settings.cfg_scale is not None:
+            assert batch_size == 2, "CFG requires logits to be bsz 2"
+        else:
+            assert batch_size == 1 or len(filters) == 0, "Filters not implemented for batch size > 1"
+
+        logits = logits.squeeze(1)
+
+        # CFG
+
+        if settings.cfg_scale is not None:
+            logits = F.log_softmax(logits, dim = -1)
+            logits = settings.cfg_scale * logits[0] + (1 - settings.cfg_scale) * logits[1]
+            logits = logits.unsqueeze(0)
+            batch_size = 1
 
-        assert logits.shape[1] == 1, "Logits tensor is incorrect shape, must be (bsz, 1, vocab_size)"
-        assert prefix_token is None or prefix_token.shape == (batch_size, 1), "Prefix token list doesn't match batch shape"
-        assert batch_size == 1 or len(settings.filters) == 0, "Filters not implemented for batch size > 1"
+        # Prepare filter
 
-        logits = logits.clone().squeeze(1)
-        logit_filter = torch.ones((batch_size, vocab_size), dtype = torch.bool)
+        logit_filter = torch.empty((batch_size, vocab_size), dtype = torch.bool)
+        ext_c.fast_fill_cpu_ones_bool(logit_filter)
 
         # Repetition penalty
 
-        if settings.token_repetition_penalty != 1.0:
+        if settings.token_repetition_penalty != 1.0 or \
+            settings.token_frequency_penalty != 0.0 or \
+            settings.token_presence_penalty != 0.0:
 
-            ext_c.apply_rep_penalty(sequence_ids,
+            ext_c.apply_rep_penalty(sequence_ids[:, :],
                                     settings.token_repetition_penalty,
                                     settings.token_repetition_range,
                                     settings.token_repetition_decay,
+                                    settings.token_frequency_penalty,
+                                    settings.token_presence_penalty,
                                     logits)
 
+        # Temporarily ban individual tokens
+
+        if blocked_tokens:
+            logits[:, blocked_tokens] = -1e30
+
         # Token bias
 
-        if settings.token_bias is not None: logits += settings.token_bias
+        if settings.token_bias is not None:
+            # logits = logits + settings.token_bias
+            ext_c.fast_fadd_cpu(logits, settings.token_bias)
 
         # Evaluate filters
 
-        if len(settings.filters) > 0:
+        if len(filters) > 0:
 
             pass_tokens = None
             end_tokens = None
-            for f in settings.filters:
+            for f in filters:
 
                 pt, et = f.next()
-                pass_tokens = pt if pass_tokens is None else pass_tokens & pt
-                end_tokens = et if end_tokens is None else end_tokens | et
+                if pt is not None: pass_tokens = pt if pass_tokens is None else pass_tokens & pt
+                if et is not None: end_tokens = et if end_tokens is None else end_tokens | et
 
-            assert pass_tokens, "Filter excluded all tokens"
-            ext_c.logit_filter_exclusive(logit_filter, [sorted(list(pass_tokens))])
+            if pass_tokens is not None:
+                assert pass_tokens, "Filter excluded all tokens"
+                if filter_prefer_eos and tokenizer.eos_token_id in pass_tokens:
+                    pass_tokens = { tokenizer.eos_token_id }
+                ext_c.logit_filter_exclusive(logit_filter, [sorted(list(pass_tokens))])
 
         # Healing
 
         if prefix_token is not None:
 
             prefix_id_to_ids = tokenizer.get_prefix_id_to_ids_dict()
 
             valid_token_lists = []
             for i in range(batch_size):
                 valid_token_lists.append(prefix_id_to_ids[prefix_token[i, 0].item()])
 
             ext_c.logit_filter_exclusive(logit_filter, valid_token_lists)
 
-        # for i in range(logit_filter.shape[-1]):
-        #     if logit_filter[0, i].item():
-        #         print(i)
-
         # Begin Mirostat
 
         if settings.mirostat:
             if settings.mirostat_mu is None:
                 settings.mirostat_mu = [0.0] * batch_size
 
+        # Mask off logits if tokenizer's vocabulary is smaller than head layer
+
+        vs = tokenizer.get_vocab_size()
+        if vs < logits.shape[-1]:
+            logits[:, vs:] = float("-inf")
+
         # Sampling
 
         batch_size = logits.shape[0]
 
         output_tokens = torch.empty((batch_size, 1), device = "cpu", dtype = torch.long)
         output_probs = torch.empty((batch_size, 1), device = "cpu", dtype = torch.float)
-
-        m = ext_c.sample_basic(logits,
-                               1.0 if settings.temperature_last else settings.temperature,
-                               settings.top_k,
-                               settings.top_p,
-                               settings.min_p,
-                               settings.tfs,
-                               settings.typical,
-                               random,
-                               output_tokens,
-                               output_probs,
-                               logit_filter,
-                               settings.mirostat,
-                               settings.mirostat_mu if settings.mirostat else [],
-                               settings.mirostat_tau,
-                               settings.mirostat_eta,
-                               settings.temperature if settings.temperature_last else 1.0)
+        if return_top_tokens == 0:
+            output_ktokens = none_tensor
+            output_kprobs = none_tensor
+        else:
+            output_ktokens = torch.empty((batch_size, 1, return_top_tokens), device = "cpu", dtype = torch.long)
+            output_kprobs = torch.empty((batch_size, 1, return_top_tokens), device = "cpu", dtype = torch.float)
+
+        m = ext_c.sample_basic(
+            logits,
+            1.0 if settings.temperature_last else settings.temperature,
+            settings.top_k,
+            settings.top_p,
+            settings.top_a,
+            settings.min_p,
+            settings.tfs,
+            settings.typical,
+            random,
+            output_tokens,
+            output_probs,
+            output_kprobs,
+            output_ktokens,
+            logit_filter,
+            settings.mirostat,
+            settings.mirostat_mu if settings.mirostat else [],
+            settings.mirostat_tau,
+            settings.mirostat_eta,
+            settings.temperature if settings.temperature_last else 1.0,
+            settings.min_temp,
+            settings.max_temp,
+            settings.temp_exponent,
+            settings.smoothing_factor,
+            settings.skew
+        )
 
         if settings.mirostat: settings.mirostat_mu = m
 
         # Stop condition from filters
 
         end_filter = False
-        if len(settings.filters) > 0 and output_tokens[0].item() in end_tokens: end_filter = True
+        if len(filters) > 0 and output_tokens[0].item() in end_tokens:
+            end_filter = True
 
-        return output_tokens, output_probs, end_filter
+        return output_tokens, output_ktokens, output_kprobs, output_probs, end_filter
```

## exllamav2/generator/streaming.py

```diff
@@ -1,267 +1,734 @@
+from __future__ import annotations
+
 from ast import Tuple
+from typing import Union, Tuple
 from exllamav2 import (
     ExLlamaV2,
     ExLlamaV2Cache,
+    ExLlamaV2CacheBase,
     ExLlamaV2Tokenizer,
-    ExLlamaV2Lora
+    ExLlamaV2Lora,
 )
 from exllamav2.generator import (
     ExLlamaV2Sampler,
     ExLlamaV2BaseGenerator
 )
-
+from exllamav2.generator.filters import ExLlamaV2Filter
+from exllamav2.generator.ngram import NgramCache
 import torch
 import random
+import threading
+from exllamav2.generator.hooks import ExLlamaV2PostSamplingHook, ExLlamaV2PostSamplingResult
+from exllamav2.embedding import EMBEDDING_INDEX
+from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
+import numpy as np
 
 class ExLlamaV2StreamingGenerator(ExLlamaV2BaseGenerator):
 
+    # Constants
+
     tail_decode_tokens: int = 2
-    
-    remaining_tokens: int = 0
-    held_text: str = ""
-    held_utf8_tokens: torch.tensor = None
-    expect_utf8: int = 0
-    held_tokens: torch.Tensor or None = None
-    settings: ExLlamaV2Sampler.Settings = None
-    stop_strings: list = []
-    stop_tokens: list = []
-    no_tokens: torch.Tensor = None
+    max_fallback_tokens: int = 4
+
+    no_tokens: torch.Tensor
+    no_ptokens: torch.Tensor
+    no_probs: torch.Tensor
+    no_pprobs: torch.Tensor
+    no_logits: torch.Tensor
+
+    # Generation settings
+
+    settings: ExLlamaV2Sampler.Settings
+
+    return_probabilities: bool
+    return_top_tokens: int
+    return_logits: bool
+
+    position_offsets: torch.Tensor | None
+    input_mask: torch.Tensor | None
+    indexed_embeddings: torch.Tensor | None
+
+    # Stop conditions
+
+    stop_strings: set
+    stop_strings_utf32_buffer: np.array or None
+    stop_strings_utf32_offsets: np.array or None
+    stop_tokens: set
+    remaining_tokens: int
+
+    # Speculative decoding
 
-    first_token = False
-    heal_next_token = False
+    future_logits: torch.Tensor | None
+    future_tokens: torch.Tensor | None
 
-    draft_model: ExLlamaV2 or None = None
-    draft_cache: ExLlamaV2Cache or None = None
+    total_draft_tokens: int
+    total_tokens: int
+    accepted_draft_tokens: int
 
-    future_logits: torch.tensor or None = None
-    future_tokens: torch.tensor or None = None
+    # Draft model
+
+    draft_model: ExLlamaV2 | None
+    draft_cache: ExLlamaV2CacheBase | None
     num_speculative_tokens: int
-    speculative_prob_threshold: float = 0.25
-    total_draft_tokens: int = 0
-    total_tokens: int = 0
-    accepted_draft_tokens: int = 0
+    speculative_prob_threshold: float
+
+    # N-gram decoding
+
+    speculative_ngram: bool
+    speculative_ngram_min: int
+    speculative_ngram_max: int
+    speculative_ngram_max_inf: int
+    speculative_ngram_threshold: int
+    ngram: NgramCache | None
+    ngram_preloaded: NgramCache | None
+
+    # Output buffers
+
+    held_text: str
+    held_tokens: torch.Tensor | None
+    held_ptokens: torch.Tensor | None
+    held_probs: torch.Tensor | None
+    held_pprobs: torch.Tensor | None
+    held_logits: torch.Tensor | None
+
+    # Token healing
+
+    first_token: bool
+    heal_next_token: bool
+
+    # LoRAs
+
+    active_loras: list[ExLlamaV2Lora]
+
+    # Extra decoding options
+
+    decode_special_tokens: bool
+
+    # Banned strings
+
+    banned_strings: list[str]
+    banned_strings_utf32_buffer: np.array or None
+    banned_strings_utf32_offsets: np.array or None
+    ban_checkpoint: dict | None
+    blocked_tokens: list[int]
+    blocked_position: int
+    current_blocked_tokens: list[int]
+    reuse_logits: torch.Tensor | None
+
+    # Filters
+
+    filters: list[ExLlamaV2Filter] | None
+    filter_prefer_eos: bool
 
-    active_loras = []
 
     def __init__(self, model, cache, tokenizer, draft_model = None, draft_cache = None, num_speculative_tokens = 5):
         super().__init__(model, cache, tokenizer)
 
-        self.stop_strings = []
-        self.stop_tokens = [tokenizer.eos_token_id]
+        # Stop conditions
 
-        self.no_tokens = torch.empty((1, 0), dtype = torch.long)
+        self.stop_strings = set()
+        self.stop_strings_utf32_buffer = None
+        self.stop_strings_utf32_offsets = None
+        self.stop_tokens = {tokenizer.eos_token_id,}
+        self.remaining_tokens = 0
+
+        # Generation settings
+
+        self.return_probabilities = False
+        self.return_top_tokens = 0
+        self.return_logits = False
+        self.indexed_embeddings = None
+
+        # Speculative decoding
+
+        self.future_logits = None
+        self.future_tokens = None
+
+        self.total_draft_tokens = 0
+        self.total_tokens = 0
+        self.accepted_draft_tokens = 0
+
+        # Draft model
 
         if draft_model:
             self.draft_model = draft_model
             self.num_speculative_tokens = num_speculative_tokens
             if draft_cache:
                 self.draft_cache = draft_cache
             else:
                 self.draft_cache = ExLlamaV2Cache(draft_model,
                                                   batch_size = cache.batch_size,
                                                   max_seq_len = cache.max_seq_len)
+        else:
+            self.draft_model = None
+            self.draft_cache = None
+
+        self.speculative_prob_threshold = 0.25
 
+        # N-gram decoding
 
-    def set_stop_conditions(self, stop_conditions):
+        self.speculative_ngram = False
+        self.speculative_ngram_min = 1
+        self.speculative_ngram_max = 5
+        self.speculative_ngram_max_inf = 3
+        self.speculative_ngram_threshold = 1
+        self.ngram = None
+        self.ngram_preloaded = None
 
-        assert isinstance(stop_conditions, list)
+        # Token healing
+
+        self.active_loras = []
+
+        # Banned strings
 
-        self.stop_strings = []
-        self.stop_tokens = []
+        self.banned_strings = []
+        self.banned_strings_utf32_buffer = None
+        self.banned_strings_utf32_offsets = None
+        self.ban_checkpoint = None
+        self.blocked_tokens = []
+        self.blocked_position = 0
+        self.current_blocked_tokens = []
+        self.reuse_logits = None
+
+
+    def set_stop_conditions(self,
+                            stop_conditions: list | tuple | set):
+        """
+        :param stop_conditions:
+            List of either token IDs or strings that will cause stream_ex to emit the EOS signal. String values do not
+            have to match whole tokens and can span multiple tokens.
+
+        Example:
+            generator.set_stop_conditions(tokenizer.eos_token_id, "\nUser:", "###")
+        """
+
+        self.stop_strings = set()
+        self.stop_tokens = set()
         for t in stop_conditions:
-            if isinstance(t, int): self.stop_tokens += [t]
-            elif isinstance(t, str): self.stop_strings += [t]
+            if isinstance(t, int): self.stop_tokens.add(t)
+            elif isinstance(t, str): self.stop_strings.add(t)
             else: raise ValueError("Unsupported type in stop_conditions")
-    
-    
-    def begin_stream(self, input_ids: torch.Tensor, gen_settings: ExLlamaV2Sampler.Settings, token_healing = False, loras = None):
+        self.stop_strings_utf32_buffer, self.stop_strings_utf32_offsets = \
+            self.strings_to_utf32(list(self.stop_strings))
+
+
+    # Legacy function
+
+    def begin_stream(self,
+                     input_ids: torch.Tensor,
+                     gen_settings: ExLlamaV2Sampler.Settings,
+                     token_healing: bool = False,
+                     loras: ExLlamaV2Lora | list[ExLlamaV2Lora] = None,
+                     input_mask: torch.Tensor | None = None,
+                     position_offsets: torch.Tensor | None = None,
+                     abort_event: threading.Event = None,
+                     **kwargs):
+        """
+        See ExLlamaV2StreamingGenerator.begin_stream_ex
+        """
+
+        self.begin_stream_ex(input_ids,
+                             gen_settings,
+                             token_healing,
+                             loras,
+                             input_mask,
+                             position_offsets,
+                             abort_event = abort_event)
+
+
+    # Begin stream
+
+    def begin_stream_ex(
+        self,
+        input_ids: torch.Tensor,
+        gen_settings: ExLlamaV2Sampler.Settings,
+        token_healing: bool = False,
+        loras: ExLlamaV2Lora | list[ExLlamaV2Lora] = None,
+        input_mask: torch.Tensor | None = None,
+        position_offsets: torch.Tensor | None = None,
+        return_probabilities: bool = False,
+        return_top_tokens: int = 0,
+        return_logits: bool = False,
+        abort_event: threading.Event = None,
+        input_embeddings: torch.Tensor | None = None,
+        decode_special_tokens: bool = False,
+        banned_strings: list[str] | None = None,
+        filters: list[ExLlamaV2Filter] | None = None,
+        filter_prefer_eos: bool = False,
+        **kwargs
+    ):
+        """
+        Resets the generator and starts a new completion of the supplied input_ids. Reuses the existing
+        cache for any token IDs matching the previous sequence.
+
+        :param input_ids:
+            Input token ID sequence, as produced by ExLlamaV2Tokenizer. Streaming generator does not
+            currently support batch size > 1 except when performing CFG, in which case batch size
+            must be 2.
+
+        :param gen_settings:
+            Sampling settings
+
+        :param token_healing:
+            Apply token healing by regenerating the last token of the input sequence with prefix
+            constraint.
+
+        :param loras:
+            (List of) ExLlamaV2Lora objects to apply during generation
+
+        :param input_mask:
+            Optional attention mask for the input
+
+        :param position_offsets:
+            Optional position offsets
+
+        :param return_probabilities:
+            Return tensor of post-sampler probabilities for each selected token
+
+        :param return_top_tokens:
+            Number of top candidate tokens to return for each selected token, along with their final
+            probabilities.
+
+        :param return_logits:
+            Return the raw logits output by the model for each streamed token
+
+        :param abort_event:
+            Forwarded to the model during generation. Will abort prefill/context ingestion if triggered.
+
+        :param input_embeddings:
+            Tensor of shape (batch_size, n, hidden_size) added to the beginning of the prompt. Batching
+            is not supported when passing input embeddings unless all prompts are the same. Prompt must
+            contain the string `{{EMBED_HERE}}` to indicate where embeddings are to be inserted.
+
+        :param decode_special_tokens:
+            Also decode special tokens into output text stream
+
+        :param filters:
+            List of ExLlamaV2Filters to apply during generation.
+
+        :param filter_prefer_eos:
+            If True, always sample the tokenizer's defined EOS token as soon as it's allowed by the filters
+
+        :param banned_strings:
+            List of strings that the generator will refuse to output. As soon as a partial match happens,
+            a checkpoint is saved that the generator can rewind to if need be. Subsequent tokens are then
+            held until the full string is resolved (match or no match) and either emitted or discarded,
+            accordingly. Strings are case-insensitive.
+        """
+
+        self.return_probabilities = return_probabilities
+        self.return_top_tokens = return_top_tokens
+        self.return_logits = return_logits
+
+        self.abort_event = abort_event
+
+        self.decode_special_tokens = decode_special_tokens
+
+        assert input_ids.shape[0] <= 2, "Streaming generator does not support batch size > 1"
+        if input_ids.shape[0] == 2:
+            assert gen_settings.cfg_scale is not None, "No CFG scale set"
+
+        self.position_offsets = position_offsets
+        self.input_mask = input_mask
 
-        # Accept LoRA or list of LoRAs
         if loras is not None and isinstance(loras, ExLlamaV2Lora): loras = [loras]
         self.active_loras = loras
 
+        self.indexed_embeddings = input_embeddings
+
+        # Decluttering
+
+        self.no_logits = torch.empty((0, ((self.model.config.vocab_size + 31) // 32) * 32), dtype = torch.float)
+        self.no_tokens = torch.empty((1, 0), dtype = torch.long)
+        self.no_probs = torch.empty((1, 0), dtype = torch.float)
+        self.no_ptokens = torch.empty((1, 0, self.return_top_tokens), dtype = torch.long)
+        self.no_pprobs = torch.empty((1, 0, self.return_top_tokens), dtype = torch.float)
+
+        # Initialize state
+
         self.held_text = ""
-        self.held_utf8_tokens = self.no_tokens
-        self.expect_utf8 = 0
+
         self.held_tokens = self.no_tokens
+        self.held_ptokens = self.no_ptokens
+        self.held_probs = self.no_probs
+        self.held_pprobs = self.no_pprobs
+        self.held_logits = self.no_logits
         self.settings = gen_settings
-        self._gen_begin_reuse(input_ids, gen_settings)
 
+        # Ingest prompt
+
+        assert input_embeddings is None or self.draft_model is None, \
+            "Can not use input embeddings with draft model"
+
+        self._gen_begin_reuse(input_ids, gen_settings)
         self.heal_next_token = (token_healing and self.sequence_ids.shape[-1] >= 2)
 
+        # Remove indexed embeddings from generator's sequence
+
+        if input_embeddings is not None:
+            self.sequence_ids[self.sequence_ids >= EMBEDDING_INDEX] = self.tokenizer.pad_token_id
+
+        # Initialize n-gram cache
+
+        if self.speculative_ngram:
+            self.ngram = NgramCache(self.speculative_ngram_min,
+                                    self.speculative_ngram_max,
+                                    self.ngram_preloaded)
+            self.ngram.update(self.sequence_ids[0].tolist())
+
+        # Banned strings
+
+        if banned_strings is None: banned_strings = []
+        self.banned_strings = [s.lower() for s in banned_strings]
+        self.banned_strings_utf32_buffer, self.banned_strings_utf32_offsets = \
+            self.strings_to_utf32(self.banned_strings)
+
+        self.ban_checkpoint = None
+        self.blocked_tokens = []
+        self.blocked_position = -1
+        self.current_blocked_tokens = []
+        self.reuse_logits = None
+
+        # Filters
+
+        self.filters = filters if filters is not None else []
+        self.filter_prefer_eos = filter_prefer_eos
+
+
+    # Convert list of strings to UTF32 format needed, to pass by reference to partial matching function
+
+    def strings_to_utf32(self, strings: list[str]) -> (np.array, list[int]):
+
+        if not strings: return bytearray(), None
+
+        encoded_strings = [s.encode("utf-32-le") for s in strings]
+        encoded_lengths = [len(s) for s in encoded_strings]
+        offsets = [0] + encoded_lengths
+        for i in range(1, len(offsets)):
+            offsets[i] += offsets[i - 1]
+        total_length = offsets[-1]
+        concat_strings = bytearray(total_length)
+        for s, offset in zip(encoded_strings, offsets[:-1]):
+            concat_strings[offset:offset + len(s)] = s
+
+        concat_strings = np.frombuffer(concat_strings, dtype = np.uint8)
+        offsets = np.frombuffer(np.array(offsets, dtype = np.int32), dtype = np.uint8)
+        return concat_strings, offsets
+
+
+    # Get the next chunk of text in the stream
+
+    def stream_ex(self, ban_tokens: list[int] | None = None, **kwargs):
+        """
+        Perform one streaming iteration, returning one chunk of text.
+
+        :param ban_tokens:
+            List of tokens to disallow for this iteration only.
+
+        Returns a dict with the following entries:
+
+            chunk:
+                Decoded output text. May be an empty string if the generator holds text to resolve a
+                stop condition.
+
+            eos:
+                Boolean EOS signal. True if one of the specified stop conditions has been met
+
+            chunk_token_ids:
+                Tensor of tokens corresponding to the output text, of shape (1, n). n may be zero if
+                tokens are being held due to a partially met stop condition. Held tokens will be emitted
+                on subsequent calls to stream_ex() when the stop condition is resolved.
+
+                In the case of token healing, this does not include the last token of the input even
+                though its value may have changed.
+
+                If a string stop condition matches a partial token, the ID for the full token is included.
+
+            probs: (if return_probabilities == True)
+                Tensor of probabilities (1, n) corresponding to the chunk_token_ids
+
+            top_tokens, top_probs: (if return_top_tokens > 0)
+                Top-K token IDs with corresponding probabilities, shape (1, n, k). Sorted in descending
+                order. Probabilities may sum to less than 1 if more than k tokens were candidates at the
+                final sampling stage. If less than k tokens were candidates, the list will be padded with
+                zero-probability tokens whose order is undefined.
+
+            return_logits: (if return_logits == True)
+                Raw output logits for the model, shape (1, n, vocab_size)
+        """
+
+        chunk, eos, chunk_token_ids, probs, ptokens, pprobs, logits, extra = self._stream(
+            ban_tokens = ban_tokens
+        )
+
+        ret = { "chunk": chunk,
+                "eos": eos,
+                "chunk_token_ids": chunk_token_ids }
+
+        if self.return_probabilities:
+            ret["probs"] = probs
 
-    # Get the next chunk of text in the stream. Returns eos if stop condition has been met but does not count tokens
+        if self.return_top_tokens > 0:
+            ret["top_probs"] = pprobs
+            ret["top_tokens"] = ptokens
 
-    def stream(self) -> (str, bool, torch.Tensor):
+        if self.return_logits:
+            ret["logits"] = logits.unsqueeze(0)
+
+        if extra:
+            ret.update(extra)
+
+        return ret
+
+
+    # Legacy function
+
+    def stream(self, **kwargs) -> Union[Tuple[str, bool, torch.Tensor],
+                                  Tuple[str, bool, torch.Tensor, torch.Tensor],
+                                  Tuple[str, bool, torch.Tensor, torch.Tensor, torch.Tensor]]:
+        """
+        Legacy functions that returns a tuple rather than a dict. See ExLlamaV2StreamingGenerator.stream_ex
+        """
+
+        assert self.return_top_tokens == 0, "Use stream_ex() to return top K probs"
+
+        chunk, eos, chunk_token_ids, probs, _, _, logits, _ = self._stream()
+        ret = [chunk, eos, chunk_token_ids]
+
+        if self.return_probabilities:
+            ret.append(probs)
+
+        if self.return_logits:
+            ret.append(logits)
+        
+        return tuple(ret)
+
+
+    def _stream(self, ban_tokens: list[str] | None = None) -> (str, bool, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, dict | None):
+
+        # Blocked/banned tokens
+
+        self.current_blocked_tokens = [] if ban_tokens is None else ban_tokens
+        if self.cache.current_seq_len == self.blocked_position:
+            self.current_blocked_tokens += self.blocked_tokens
 
         # Token healing
 
         if self.heal_next_token:
 
-            # Pop the last toke
+            # Pop the last token
 
-            old_tail = self.tokenizer.decode(self.sequence_ids[:, -self.tail_decode_tokens:])[0]
+            old_tail = self.tokenizer.decode(self.sequence_ids[:, -self.tail_decode_tokens:],
+                                             decode_special_tokens = self.decode_special_tokens)[0]
             last_token = self.sequence_ids[:, -1:]
             self.sequence_ids = self.sequence_ids[:, :-1]
             self.cache.current_seq_len -= 1
 
             # Start filters
 
             if self.first_token:
 
-                self.settings.begin_filters(self.tokenizer.get_id_to_piece_list()[last_token])
+                for f in self.filters:
+                    f.begin(self.tokenizer.get_id_to_piece_list(self.decode_special_tokens)[last_token])
                 self.first_token = False
 
             # Regenerate the last token again, with prefix
 
-            healed_token, eos = self._gen_single_token(self.settings, prefix_token = last_token)
-            new_tail = self.tokenizer.decode(self.sequence_ids[:, -self.tail_decode_tokens:])[0]
+            healed_token, _, _, _, eos, logits, dev_logits = self._gen_single_token(self.settings, prefix_token = last_token)
+            new_tail = self.tokenizer.decode(self.sequence_ids[:, -self.tail_decode_tokens:],
+                                             decode_special_tokens = self.decode_special_tokens)[0]
             self.held_text += new_tail[len(old_tail):]
 
             self.heal_next_token = False
 
             # In case we only needed the healed token
 
-            if eos: return self.held_text, True, self.no_tokens
+            if eos: return self.held_text, True, self.no_tokens, self.no_probs, self.no_ptokens, self.no_pprobs, self.no_logits, None
 
         # Start filters when not healing
 
         else:
 
             if self.first_token:
 
-                self.settings.begin_filters()
+                for f in self.filters: f.begin("")
                 self.first_token = False
 
-
-        # Decode the current tail end of the sequence
-
-        old_tail = self.tokenizer.decode(self.sequence_ids[:, -self.tail_decode_tokens:])[0]
-
         # Generate a single token and append to the sequence
 
-        next_token, eos = self._gen_single_token(self.settings)
+        next_token, next_ptokens, next_pprobs, next_prob, eos, next_logits, dev_logits = self._gen_single_token(self.settings)
 
         # End immediately if it was a stop token
 
-        if next_token in self.stop_tokens:
-            return self.held_text, True, self.no_tokens
-
-        # Decode the tail end of the sequence with the added token to get (actual) characters added
+        if next_token.item() in self.stop_tokens:
+            return self.held_text, True, self.no_tokens, self.no_probs, self.no_ptokens, self.no_pprobs, self.no_logits, None
 
-        new_tail = self.tokenizer.decode(self.sequence_ids[:, -(self.tail_decode_tokens + 1):])[0]
-        new_text = new_tail[len(old_tail):]
-
-        next_token, new_text = self._catch_utf8(next_token, new_text)
+        id_to_piece = self.tokenizer.get_id_to_piece_list(self.decode_special_tokens)
+        new_text = id_to_piece[next_token]
 
         self.held_text += new_text
         self.held_tokens = torch.cat([self.held_tokens, next_token], dim = -1)
+        if self.return_probabilities:
+            self.held_probs = torch.cat([self.held_probs, next_prob], dim = 1)
+        if self.return_top_tokens > 0:
+            self.held_ptokens = torch.cat([self.held_ptokens, next_ptokens], dim = 1)
+            self.held_pprobs = torch.cat([self.held_pprobs, next_pprobs], dim = 1)
+        if self.return_logits:
+            self.held_logits = torch.cat([self.held_logits, next_logits], dim = 0)
 
         # Return now if newly added token ends a filter
 
-        if eos: return self.held_text, True, self.held_tokens
-
-        # Hold text as long as it contains part of a stop string
-
-        partial_ss = False
-        for ss in self.stop_strings:
-
-            # Check if held_text fully contains stop string
-
-            position = self.held_text.find(ss)
-            if position != -1:
-                return self.held_text[:position], True, self.no_tokens
+        if eos: return self.held_text, True, self.held_tokens, self.held_probs, self.held_ptokens, self.held_pprobs, self.held_logits, None
 
-            # Check for overlap between end of held_text and start of stop string
+        # Hold text if it contains an incomplete character
 
-            overlap = 0
-            for j in range(1, min(len(self.held_text), len(ss)) + 1):
-                if self.held_text[-j:] == ss[:j]: overlap = j
-            if overlap > 0: partial_ss = True
-
-        # If holding text because of a partial stop condition, return nothing but also EOS = False
-
-        if partial_ss:
-            return "", False, self.no_tokens
+        if self.held_text.endswith("�") and not self.held_text.endswith("�����"):
+            test_decode = self.tokenizer.decode(
+                self.held_tokens,
+                decode_special_tokens=self.decode_special_tokens
+            )[0]
+            if not test_decode.endswith("�"):
+                self.held_text = test_decode
+            else:
+                return "", False, self.no_tokens, self.no_probs, self.no_ptokens, self.no_pprobs, self.no_logits, None
 
-        # No stop condition, so return whatever is being held
+        # Hold text as long as it contains part of a banned string
 
+        def set_checkpoint():
+            self.ban_checkpoint = {
+                "position": self.cache.current_seq_len - 1,
+                "held_text": self.held_text[:-len(new_text)],
+                "held_tokens": self.held_tokens[:, :-1],
+                "held_probs": self.held_probs[:, :-1],
+                "held_ptokens": self.held_ptokens[:, :-1, :],
+                "held_pprobs": self.held_pprobs[:, :-1, :],
+                "held_logits": self.held_logits[:-1, :],
+                "offending_token": next_token,
+                "next_logits": dev_logits
+            }
+            self.blocked_position = self.cache.current_seq_len - 1
+
+        def rewind_checkpoint():
+            cp = self.ban_checkpoint
+            self.sequence_ids = self.sequence_ids[:, :cp["position"]+1]
+            self.cache.current_seq_len = cp["position"]
+            off_text = self.held_text[len(cp["held_text"]):]
+            self.held_text = cp["held_text"]
+            self.held_tokens = cp["held_tokens"]
+            self.held_probs = cp["held_probs"]
+            self.held_ptokens = cp["held_ptokens"]
+            self.held_pprobs = cp["held_pprobs"]
+            self.held_logits = cp["held_logits"]
+            self.future_logits = None
+            self.future_tokens = None
+            self.ban_checkpoint = None
+            self.reuse_logits = cp["next_logits"]
+            return cp["offending_token"], off_text
+
+        if self.banned_strings_utf32_offsets is not None:
+            match = ext_c.partial_strings_match(
+                np.frombuffer(self.held_text.lower().encode("utf-32-le"), dtype = np.uint8),
+                self.banned_strings_utf32_offsets,
+                self.banned_strings_utf32_buffer
+            )
+            if match >= 0:
+                if self.ban_checkpoint is None: set_checkpoint()
+                offending_token, offending_text = rewind_checkpoint()
+                self.blocked_tokens.append(offending_token.item())
+                extra_ret = { "suppressed": offending_text }
+                return "", False, self.no_tokens, self.no_probs, self.no_ptokens, self.no_pprobs, self.no_logits, extra_ret
+            if match == -2:
+                if self.ban_checkpoint is None: set_checkpoint()
+                return "", False, self.no_tokens, self.no_probs, self.no_ptokens, self.no_pprobs, self.no_logits, None
+
+        # Check for stop strings and hold text as long as it contains part of a stop string
+
+        if self.stop_strings_utf32_offsets is not None:
+            match = ext_c.partial_strings_match(
+                np.frombuffer(self.held_text.encode("utf-32-le"), dtype = np.uint8),
+                self.stop_strings_utf32_offsets,
+                self.stop_strings_utf32_buffer
+            )
+            if match >= 0:
+                return self.held_text[:match], True, self.no_tokens, self.no_probs, self.no_ptokens, self.no_pprobs, self.no_logits, None
+            if match == -2:
+                return "", False, self.no_tokens, self.no_probs, self.no_ptokens, self.no_pprobs, self.no_logits, None
+
+        # No stop condition or banned string, so clear checkpoint and return whatever is being held
+
+        self.ban_checkpoint = None
+        self.blocked_tokens = []
+        self.blocked_position = -1
         stream_text = self.held_text
         stream_tokens = self.held_tokens
+        stream_probs = self.held_probs
+        stream_ptokens = self.held_ptokens
+        stream_pprobs = self.held_pprobs
+        stream_logits = self.held_logits
         self.held_text = ""
         self.held_tokens = self.no_tokens
-        return stream_text, False, stream_tokens
-    
-
-    def _decode_utf8(self):
-
-        if self.held_utf8_tokens.shape[-1] == 0: return self.no_tokens, ""
-
-        try:
-            id_to_ord = self.tokenizer.get_id_to_ord_list()
-            b = [id_to_ord[x] for x in self.held_utf8_tokens[0].tolist()]
-            c = bytes(b).decode('utf-8')
-        except UnicodeDecodeError:
-            c = "�"
-
-        pre_t = self.held_utf8_tokens
-        self.held_utf_tokens = self.no_tokens
-        return pre_t, c
-
-
-    def _catch_utf8(self, next_token, new_text):
+        self.held_probs = self.no_probs
+        self.held_ptokens = self.no_ptokens
+        self.held_pprobs = self.no_pprobs
+        self.held_logits = self.no_logits
+        return stream_text, False, stream_tokens, stream_probs, stream_ptokens, stream_pprobs, stream_logits, None
 
-        if self.expect_utf8 == 0:
 
-            if new_text != "�": return next_token, new_text
+    # Helper for limiting the sequence length to the cache length. Necessary in case the cache prefill was
+    # aborted since the incomplete cache might otherwise be reused
 
-            id_to_ord = self.tokenizer.get_id_to_ord_list()
-            t = next_token[0, 0].item()
-            b = id_to_ord[t]
+    def _truncate_seq_to_cache(self):
+        cachelen = self.cache.current_seq_len
+        if self.draft_cache: cachelen = min(cachelen, self.draft_cache.current_seq_len)
+        self.sequence_ids = self.sequence_ids[:, :cachelen + 1]
 
-            if 0 < b < 256:
-                if b & 0b1100000 == 0b1000000: self.expect_utf8 = 2
-                if b & 0b1110000 == 0b1100000: self.expect_utf8 = 3
-                if b & 0b1111000 == 0b1110000: self.expect_utf8 = 4
-                if b & 0b1111100 == 0b1111000: self.expect_utf8 = 5
-            self.held_utf8_tokens = self.no_tokens
-            if self.expect_utf8 == 0: return next_token, new_text
-            new_text = ""
-
-        if self.expect_utf8:
-
-            if len(new_text) > 1:
-
-                pre_t, pre_c = self._decode_utf8()
-                next_token = torch.cat((pre_t, next_token), dim = -1)
-                new_text = pre_c + new_text
-                return next_token, new_text
-
-            self.held_utf8_tokens = torch.cat((self.held_utf8_tokens, next_token), dim = -1)
-            self.expect_utf8 -= 1
-            if self.expect_utf8 == 0: return self._decode_utf8()
-            return self.no_tokens, ""
 
+    # Begin a generation (prefill/ingest)
 
     def _gen_begin(self, in_tokens, gen_settings):
 
         self.sequence_ids = in_tokens.clone()
         self.cache.current_seq_len = 0
-        self.model.forward(self.sequence_ids[:, :-1], self.cache, preprocess_only = True, loras = self.active_loras)
+        self.model.forward(self.sequence_ids[:, :-1],
+                           self.cache,
+                           preprocess_only = True,
+                           loras = self.active_loras,
+                           input_mask = self.input_mask,
+                           position_offsets = self.position_offsets,
+                           abort_event = self.abort_event,
+                           indexed_embeddings = self.indexed_embeddings)
+        if self.abort_event and self.abort_event.is_set():
+            self._truncate_seq_to_cache()
+            return
 
         if self.draft_model is not None:
             self.draft_cache.current_seq_len = 0
-            self.draft_model.forward(self.sequence_ids[:, :-1], self.draft_cache, preprocess_only = True)
+            self.draft_model.forward(self.sequence_ids[:1, :-1],
+                                     self.draft_cache,
+                                     input_mask = self.input_mask,
+                                     position_offsets = self.position_offsets,
+                                     preprocess_only = True,
+                                     abort_event = self.abort_event)
+            if self.abort_event and self.abort_event.is_set():
+                self._truncate_seq_to_cache()
+                return
+            self.future_logits = None
+            self.future_tokens = None
+
+        if self.speculative_ngram:
             self.future_logits = None
             self.future_tokens = None
 
         self.first_token = True
 
 
+    # Begin a generation (prefill/ingest) while reusing the K/V cache for any starting tokens that are
+    # unchanged since the last generation
+
     def _gen_begin_reuse(self, in_tokens, gen_settings):
 
         if self.sequence_ids is None or self.cache.current_seq_len == 0:
             self._gen_begin(in_tokens, gen_settings)
             return
 
         reuse = 0
@@ -275,66 +742,150 @@
         self.cache.current_seq_len = reuse - 1
         if self.draft_model is not None:
             self.draft_cache.current_seq_len = reuse - 1
         self.sequence_ids = in_tokens[:, :reuse]
 
         if reuse < in_tokens.shape[-1]: self._gen_feed_tokens(in_tokens[:, reuse:], gen_settings)
 
-        if self.draft_model is not None:
+        if self.speculative_ngram or self.draft_model is not None:
             self.future_logits = None
             self.future_tokens = None
 
+        self.first_token = True
+
+
+    # Ingest a number of tokens, appending to the current sequence
 
     def _gen_feed_tokens(self, in_tokens, gen_settings):
 
         if self.sequence_ids is None:
             self._gen_begin(in_tokens, gen_settings)
             return
 
         start = self.cache.current_seq_len
         self.sequence_ids = torch.cat((self.sequence_ids, in_tokens), dim = 1)
 
-        self.model.forward(self.sequence_ids[:, start : -1], self.cache, preprocess_only = True, loras = self.active_loras)
+        self.model.forward(self.sequence_ids[:, start : -1],
+                           self.cache,
+                           preprocess_only = True,
+                           loras = self.active_loras,
+                           input_mask = self.input_mask,
+                           position_offsets = self.position_offsets,
+                           abort_event = self.abort_event,
+                           indexed_embeddings = self.indexed_embeddings)
+        if self.abort_event and self.abort_event.is_set():
+            self._truncate_seq_to_cache()
+            return
 
         if self.draft_model is not None:
-            self.draft_model.forward(self.sequence_ids[:, start: -1], self.draft_cache, preprocess_only = True)
+            self.draft_model.forward(self.sequence_ids[:, start: -1],
+                                     self.draft_cache,
+                                     preprocess_only = True,
+                                     input_mask = self.input_mask,
+                                     position_offsets = self.position_offsets,
+                                     abort_event = self.abort_event)
+            if self.abort_event and self.abort_event.is_set():
+                self._truncate_seq_to_cache()
+                return
             self.future_logits = None
             self.future_tokens = None
 
 
+    # Generate a single token and append to sequence
+
     def _gen_single_token(self, gen_settings, prefix_token = None):
 
-        if self.draft_model is None:
+        if self.speculative_ngram:
+
+            token, ptokens, pprobs, prob, eos, logits = self._gen_single_token_ngram(gen_settings, prefix_token)
+            dev_logits = None
 
-            logits = self.model.forward(self.sequence_ids[:, -1:], self.cache, loras = self.active_loras).float().cpu()
-            token, _, eos = ExLlamaV2Sampler.sample(logits, gen_settings, self.sequence_ids, random.random(), self.tokenizer, prefix_token)
+        elif self.draft_model is None:
+
+            if self.reuse_logits is not None:
+                dev_logits = self.reuse_logits
+                self.reuse_logits = None
+                self.cache.current_seq_len += 1
+                logits = dev_logits.float().cpu()
+            else:
+                dev_logits = self.model.forward(
+                    self.sequence_ids[:, -1:],
+                    self.cache,
+                    loras = self.active_loras,
+                    input_mask = self.input_mask,
+                    position_offsets = self.position_offsets
+                )
+                logits = dev_logits.float().cpu()
+
+            token, ptokens, pprobs, prob, eos = ExLlamaV2Sampler.sample(
+                logits,
+                gen_settings,
+                self.sequence_ids[:1, :],
+                random.random(),
+                self.tokenizer,
+                prefix_token,
+                self.return_top_tokens,
+                blocked_tokens = self.current_blocked_tokens,
+                filters = self.filters,
+                filter_prefer_eos = self.filter_prefer_eos
+            )
 
         else:
 
-            token, eos = self._gen_single_token_speculative(gen_settings, prefix_token)
+            token, ptokens, pprobs, prob, eos, logits = \
+                self._gen_single_token_speculative(gen_settings, prefix_token)
+            dev_logits = None
+
+        # Post sampling hook
+
+        if gen_settings.post_sampling_hooks:
+            p = ExLlamaV2PostSamplingResult(
+                sampled_token = token,
+                sampled_prob = prob,
+                logits = logits,
+                candidate_tokens = None if ptokens.is_meta else ptokens,
+                candidate_probs = None if pprobs.is_meta else pprobs
+            )
+            for h in gen_settings.post_sampling_hooks:
+                h(p)
+            token = p.sampled_token
+            if p.feed_filters:
+                for f in self.filters: f.feed(token)
+        else:
+            for f in self.filters: f.feed(token)
+
+        # Accept token
+        
+        if self.sequence_ids.shape[0] > 1 and token.shape[0] == 1:
+            self.sequence_ids = torch.cat([self.sequence_ids, token.repeat(self.sequence_ids.shape[0], 1)], dim = 1)
+        else:
+            self.sequence_ids = torch.cat([self.sequence_ids, token], dim = 1)
+        
+        return token, ptokens, pprobs, prob, eos, logits.flatten(1), dev_logits
 
-        self.sequence_ids = torch.cat([self.sequence_ids, token], dim = 1)
-        gen_settings.feed_filters(token)
-        return token, eos
 
+    # Speculative decoding with draft model
 
     def _gen_single_token_speculative(self, gen_settings, prefix_token = None):
 
         if self.future_tokens is None:
 
             # Generate draft
 
             draft_gen_settings = gen_settings.greedy_clone()
-            draft_sequence_ids = self.sequence_ids.clone()
+            draft_sequence_ids = self.sequence_ids[:1, :]
             num_drafted_tokens = 0
 
             for k in range(self.num_speculative_tokens):
 
-                logits = self.draft_model.forward(draft_sequence_ids[:, -1:], self.draft_cache).float().cpu()
-                token, prob, _ = ExLlamaV2Sampler.sample(logits, draft_gen_settings, draft_sequence_ids, random.random(), self.tokenizer, prefix_token if k == 0 else None)
+                logits = self.draft_model.forward(draft_sequence_ids[:, -1:],
+                                                  self.draft_cache,
+                                                  input_mask = self.input_mask,
+                                                  position_offsets = self.position_offsets).float().cpu()
+                token, _, _, prob, _ = ExLlamaV2Sampler.sample(logits, draft_gen_settings, draft_sequence_ids, random.random(), self.tokenizer, prefix_token if k == 0 else None)
 
                 if prob < self.speculative_prob_threshold:
                     self.draft_cache.current_seq_len -= 1
                     break
 
                 draft_sequence_ids = torch.cat((draft_sequence_ids, token), dim = 1)
                 num_drafted_tokens += 1
@@ -343,46 +894,149 @@
 
             # Rewind draft cache
 
             self.draft_cache.current_seq_len -= num_drafted_tokens
 
             # Forward last sampled token plus draft through model
 
-            self.future_tokens = draft_sequence_ids[:, -1 - num_drafted_tokens:]
-            self.future_logits = self.model.forward(self.future_tokens, self.cache, loras = self.active_loras).float().cpu()
+            if self.sequence_ids.shape[0] > 1:
+                self.future_tokens = draft_sequence_ids[:, -1 - num_drafted_tokens:].repeat(self.sequence_ids.shape[0], 1)
+            else:
+                self.future_tokens = draft_sequence_ids[:, -1 - num_drafted_tokens:]
+            self.future_logits = self.model.forward(self.future_tokens,
+                                                    self.cache,
+                                                    loras = self.active_loras,
+                                                    input_mask = self.input_mask,
+                                                    position_offsets = self.position_offsets).float().cpu()
 
             # Rewind model cache
 
             self.cache.current_seq_len -= num_drafted_tokens + 1
 
         # Sample the first future logits
 
-        token, _, eos = ExLlamaV2Sampler.sample(self.future_logits[:, :1, :], gen_settings, self.sequence_ids, random.random(), self.tokenizer, prefix_token)
+        logits = self.future_logits[:, :1, :]
+        token, ptokens, pprobs, prob, eos = ExLlamaV2Sampler.sample(
+            logits,
+            gen_settings,
+            self.sequence_ids[:1, :], random.random(),
+            self.tokenizer,
+            prefix_token,
+            self.return_top_tokens,
+            blocked_tokens = self.current_blocked_tokens,
+            filters = self.filters,
+            filter_prefer_eos = self.filter_prefer_eos
+        )
         self.future_logits = self.future_logits[:, 1:, :]
         self.future_tokens = self.future_tokens[:, 1:]
         self.cache.current_seq_len += 1
         self.draft_cache.current_seq_len += 1
 
         # If sampled token doesn't match future token or no more future tokens
 
         if self.future_tokens.shape[-1] == 0 or self.future_tokens[0, 0] != token[0, 0]:
             self.future_tokens = None
             self.future_logits = None
         else:
             self.accepted_draft_tokens += 1
         self.total_tokens += 1
 
-        return token, eos
+        return token, ptokens, pprobs, prob, eos, logits
+
+
+    # Speculative decoding with n-gram predictions
+
+    def _gen_single_token_ngram(self, gen_settings, prefix_token = None):
+
+        if self.future_tokens is None:
 
+            inf_ids = self.sequence_ids[0, -1:].tolist()
+            pred_ids = self.sequence_ids[0, -self.speculative_ngram_max:].tolist()
+
+            threshold = self.speculative_ngram_threshold
+            while len(inf_ids) < self.speculative_ngram_max_inf:
+                t = self.ngram.predict_next(pred_ids, threshold, self.ngram_preloaded)
+                if t is None: break
+                pred_ids = pred_ids[1:] + [t]
+                inf_ids += [t]
+                threshold += 1
+
+            if len(inf_ids) + self.cache.current_seq_len > self.cache.max_seq_len:
+                inf_ids = inf_ids[:-(len(inf_ids) + self.cache.current_seq_len - self.cache.max_seq_len)]
+
+            self.future_tokens = torch.tensor([inf_ids], dtype = torch.long)
+            self.future_logits = self.model.forward(self.future_tokens,
+                                                    self.cache,
+                                                    loras = self.active_loras,
+                                                    input_mask = self.input_mask,
+                                                    position_offsets = self.position_offsets)
+            self.future_logits = self.future_logits.float().cpu()
+
+            self.cache.current_seq_len -= len(inf_ids)
+            self.total_draft_tokens += len(inf_ids) - 1
+
+        # Sample the first future logits
+
+        logits = self.future_logits[:, :1, :]
+        token, ptokens, pprobs, prob, eos = ExLlamaV2Sampler.sample(
+            logits,
+            gen_settings,
+            self.sequence_ids[:1, :],
+            random.random(),
+            self.tokenizer,
+            prefix_token,
+            self.return_top_tokens,
+            blocked_tokens = self.current_blocked_tokens,
+            filters=self.filters,
+            filter_prefer_eos=self.filter_prefer_eos
+        )
+        self.future_logits = self.future_logits[:, 1:, :]
+        self.future_tokens = self.future_tokens[:, 1:]
+        self.cache.current_seq_len += 1
+
+        # Update predictor
+
+        tail_ids = self.sequence_ids[0, -(self.speculative_ngram_max - 1):].tolist() + [token.item()]
+        self.ngram.update_single(tail_ids)
+
+        # If sampled token doesn't match future token or no more future tokens
+
+        if self.future_tokens.shape[-1] == 0 or self.future_tokens[0, 0] != token[0, 0]:
+            self.future_tokens = None
+            self.future_logits = None
+        else:
+            self.accepted_draft_tokens += 1
+        self.total_tokens += 1
+
+        return token, ptokens, pprobs, prob, eos, logits
+
+
+    # Some metrics
 
     def reset_sd_stats(self):
 
         self.total_tokens = 0
         self.total_draft_tokens = 0
         self.accepted_draft_tokens = 0
 
 
     def get_sd_stats(self):
 
-        efficiency = self.accepted_draft_tokens / self.total_tokens
-        accuracy = self.accepted_draft_tokens / self.total_draft_tokens
-        return efficiency, accuracy, self.total_tokens, self.total_draft_tokens, self.accepted_draft_tokens
+        efficiency = self.accepted_draft_tokens / self.total_tokens if self.total_tokens else 0
+        accuracy = self.accepted_draft_tokens / self.total_draft_tokens if self.total_draft_tokens else 0
+        return efficiency, accuracy, self.total_tokens, self.total_draft_tokens, self.accepted_draft_tokens
+
+
+    def ngram_preload(self,
+                      input_ids: torch.Tensor):
+        """
+        Preload the n-gram cache with some tokenized example text. (Every call to begin_stream_ex creates
+        a new dynamic cache based on the provided context and generated tokens.)
+        """
+
+        if isinstance(input_ids, torch.Tensor):
+            input_ids = input_ids[0].tolist()
+
+        self.ngram_preloaded = NgramCache(self.speculative_ngram_min, self.speculative_ngram_max, None)
+        self.ngram_preloaded.update(input_ids)
+
+
```

## exllamav2/generator/filters/__init__.py

```diff
@@ -1,4 +1,5 @@
 from exllamav2.version import __version__
 
 from exllamav2.generator.filters.base import ExLlamaV2Filter
 from exllamav2.generator.filters.select import ExLlamaV2SelectFilter
+from exllamav2.generator.filters.prefix import ExLlamaV2PrefixFilter
```

## exllamav2/generator/filters/base.py

```diff
@@ -1,31 +1,35 @@
 from exllamav2 import (
     ExLlamaV2,
-    ExLlamaV2Tokenizer
+    ExLlamaV2Tokenizer,
 )
 
 class ExLlamaV2Filter:
 
     # Internal state
 
     model: ExLlamaV2
     tokenizer: ExLlamaV2Tokenizer
     sequence_str: str
 
 
-    def __init__(self, model, tokenizer):
+    def __init__(self,
+                 model: ExLlamaV2,
+                 tokenizer: ExLlamaV2Tokenizer):
 
         self.model = model
         self.tokenizer = tokenizer
         self.sequence_str = ""
 
 
-    def clone(self):
-
-        c = ExLlamaV2Filter(self.model, self.tokenizer)
+    def clone(self, c = None):
+        if c is None:
+            c = ExLlamaV2Filter.__new__(ExLlamaV2Filter)
+        c.model = self.model
+        c.tokenizer = self.tokenizer
         c.sequence_str = self.sequence_str
         return c
 
 
     def begin(self, prefix_str):
         pass
```

## exllamav2/generator/filters/select.py

```diff
@@ -3,55 +3,77 @@
     ExLlamaV2Tokenizer
 )
 
 from exllamav2.generator.filters.base import ExLlamaV2Filter
 
 class ExLlamaV2SelectFilter(ExLlamaV2Filter):
 
-    options: list
+    options: list[str]
     offset: int
     prefix: str
     case_insensitive: bool
     sequence_str_cmp: str
 
-    def __init__(self, model, tokenizer, options, case_insensitive = False):
+    def __init__(self,
+                 model: ExLlamaV2,
+                 tokenizer: ExLlamaV2Tokenizer,
+                 options: list[str],
+                 case_insensitive: bool = False):
+        """
+        :param options:
+            List of possible strings that may be generated.
+
+        :param case_insensitive:
+            Ignore case.
+        """
+        
         super().__init__(model, tokenizer)
 
         self.options = options if not case_insensitive else [o.lower() for o in options]
         self.case_insensitive = case_insensitive
         self.offset = 0
         self.prefix = ""
         self.sequence_str_cmp = ""
 
 
-    def begin(self, prefix_str = ""):
+    def clone(self, c = None):
+        if c is None:
+            c = ExLlamaV2SelectFilter.__new__(ExLlamaV2SelectFilter)
+        super().clone(c)
+        c.options = self.options
+        c.offset = self.offset
+        c.prefix = self.prefix
+        c.case_insensitive = self.case_insensitive
+        c.sequence_str_cmp = self.sequence_str_cmp
+        return c
+
+
+    def begin(self, prefix_str: str = ""):
 
         self.sequence_str = ""
         self.sequence_str_cmp = ""
-        self.prefix = prefix_str
+        self.prefix = prefix_str if prefix_str is not None else ""
         self.offset = 0
 
 
-    def feed(self, token):
+    def feed(self, token: int):
 
         id_to_piece = self.tokenizer.get_id_to_piece_list()
         piece = id_to_piece[token]
         self.sequence_str += piece
         if self.case_insensitive:
             split = max(len(self.prefix) - self.offset, 0)
             piece_l = piece[:split]
             piece_r = piece[split:].lower()
             self.sequence_str_cmp += piece_l + piece_r
         else:
             self.sequence_str_cmp += piece
         self.offset += len(piece)
 
 
-    # TODO: Evaluate overhead and maybe move to extension
-
     def next(self):
 
         # prefix_to_ids = self.tokenizer.get_prefix_to_ids_dict()
         id_to_piece = self.tokenizer.get_id_to_piece_list()
         # pass_str = set()
         # end_str = set()
 
@@ -101,8 +123,8 @@
                             if option_cased.startswith(s):
                                 pass_tokens.add(l)
                                 # pass_str.add(s)
                                 if option == "":
                                     end_tokens.add(l)
                                     # end_str.add(s)
 
-        return pass_tokens, end_tokens
+        return pass_tokens, end_tokens
```

## exllamav2/server/websocket.py

```diff
@@ -11,36 +11,44 @@
     ExLlamaV2Sampler
 )
 
 import exllamav2.server.websocket_actions as actions
 
 import websockets, asyncio
 import json
+import threading, asyncio
 
 class ExLlamaV2WebSocketServer:
 
     ip: str
     port: int
 
     model: ExLlamaV2
     tokenizer: ExLlamaV2Tokenizer
     cache: ExLlamaV2Cache
     generator = ExLlamaV2StreamingGenerator
 
+    stop_signal = threading.Event()
+    model_lock = asyncio.Lock()
+    active_requests: list
+
 
     def __init__(self, ip: str, port: int, model: ExLlamaV2, tokenizer: ExLlamaV2Tokenizer, cache: ExLlamaV2Cache):
 
         self.ip = ip
         self.port = port
         self.model = model
         self.tokenizer = tokenizer
         self.cache = cache
 
         self.generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)
 
+        self.stop_signal.clear()
+        self.active_requests = []
+
 
     def serve(self):
 
         print(f" -- Starting WebSocket server on {self.ip} port {self.port}")
 
         start_server = websockets.serve(self.main, self.ip, self.port)
         asyncio.get_event_loop().run_until_complete(start_server)
@@ -48,8 +56,10 @@
 
 
     async def main(self, websocket, path):
 
         async for message in websocket:
 
             request = json.loads(message)
-            await actions.dispatch(request, websocket, self)
+            r = asyncio.create_task(actions.dispatch(request, websocket, self))
+            self.active_requests.append(r)
+            self.active_requests = [r for r in self.active_requests if not r.done()]
```

## exllamav2/server/websocket_actions.py

```diff
@@ -3,14 +3,15 @@
 #     ExLlamaV2,
 #     ExLlamaV2Config,
 #     ExLlamaV2Cache,
 #     ExLlamaV2Tokenizer
 # )
 
 import json
+import asyncio
 
 from exllamav2.generator import (
     ExLlamaV2StreamingGenerator,
     ExLlamaV2Sampler
 )
 
 async def dispatch(request, ws, server):
@@ -21,14 +22,15 @@
     if "request_id" in request: response["request_id"] = request["request_id"]
     if "response_id" in request: response["response_id"] = request["response_id"]
 
     if action_ == "echo": echo(request, ws, server, response)
     elif action_ == "estimate_token": estimate_token(request, ws, server, response)
     elif action_ == "lefttrim_token": lefttrim_token(request, ws, server, response)
     elif action_ == "infer": await infer(request, ws, server, response)
+    elif action_ == "stop": stop(request, ws, server, response)
 
     else:
         print(f" ## Unknown request from client: {request}")
         return
 
     await ws.send(json.dumps(response))
 
@@ -100,17 +102,23 @@
                 response_id: str,                   # (optional) response ID to echo in response packet
                 text: str,                          # input prompt
                 max_new_tokens: int,                # max num new tokens
                 stream: bool,                       # stream response
                 stream_full: bool,                  # return full response-so-far with each streamed chunk
                 top_p: float,                       # (optional) top-P threshold (0 to disable)
                 top_k: int,                         # (optional) top-K count (0 to disable)
+                top_a: float,                       # (optional) top-A threshold (0 to disable)
+                min_p: float,                       # (optional) min-P threshold (0 to disable)
                 typical: float,                     # (optional) typical threshold (0 to disable)
                 temperature: float,                 # (optional) sampling temperature (1.0 = no temp adjust)
                 rep_pen: float,                     # (optional) repetition penalty (1.0 = no penalty)
+                freq_pen: float,                    # (optional) frequency penalty (0.0 = no penalty)
+                pres_pen: float,                    # (optional) presence penalty (0.0 = no penalty)
+                skew: float,                        # (optional) skew factor (0.0 = disabled)
+                customBos: str,                     # (optional) custom BOS token
                 stop_conditions: [str|int],         # (optional) list of stop conditions
                 token_healing: bool,                # (optionsl) enable token healing
                 tag: str }                          # (optional) tag to echo in response packet
 
     streams:  { action: str = "infer",
                 request_id: str,                    # (optional)
                 response_id: str,                   # (optional)
@@ -120,75 +128,132 @@
 
     response: { action: str = "infer",
                 request_id: str,                    # (optional)
                 response_id: str,                   # (optional)
                 response_type: str = "full",
                 util_text: str,                     # input context (pruned if max_seq_len exceeded)
                 response: str,                      # full response excluding input prompt
-                tag: str }                          # (optional)
+                tag: str,                           # (optional)
+                stop_reason: str }                  # "eos", "num_tokens" or "interrupted"
     """
 
-    # Mode
+    async with server.model_lock:
 
-    stream = request["stream"]
-    if "tag" in request:
-        response["tag"] = request["tag"]
+        server.stop_signal.clear()
 
-    # Stop conditions
+        # Mode
 
-    sc = [server.tokenizer.eos_token_id]
-    if "stop_conditions" in request:
-        ss = request["stop_conditions"]
-        if not isinstance(ss, list): ss = [ss]
-        sc += ss
+        stream = request["stream"]
+        if "tag" in request:
+            response["tag"] = request["tag"]
 
-    # Full response
+        # Stop conditions
 
-    full_response = request.get("full_response", False)
+        sc = [server.tokenizer.eos_token_id]
+        if "stop_conditions" in request:
+            ss = request["stop_conditions"]
+            if not isinstance(ss, list): ss = [ss]
+            sc += ss
 
-    # Tokenize and trim prompt
+        if "bann_bann" in request:
+            bb = request["bann_bann"]
+            if not isinstance(bb, list): bb = [bb]
+        else:
+            bb = None
 
-    full_ctx = request["text"]
-    num_tokens = request["max_new_tokens"]
+        # Full response
 
-    ids = server.tokenizer.cached_encode_str(full_ctx)
-    overflow = ids.shape[-1] + num_tokens - server.model.config.max_seq_len
-    if overflow > 0:
-        ids = ids[:, overflow:]
-        util_ctx = server.tokenizer.decode(ids)
-    else:
-        util_ctx = full_ctx
+        full_response = request['stream_full'] if 'stream_full' in request else False
+        # Tokenize and trim prompt
+
+        full_ctx = request["text"]
+        num_tokens = request["max_new_tokens"]
+
+        cb=''
+        if 'customBos' in request:
+            cb = request['customBos']
+        ids = server.tokenizer.cached_encode_str(cb+full_ctx)
+        overflow = ids.shape[-1] + num_tokens - server.model.config.max_seq_len
+
+
+        if overflow < 0:
+            util_ctx = cb+full_ctx
+
+        elif 'customBos' in request:
+            ids = ids[:,:1]+ids[:, overflow+1:]
+            util_ctx = server.tokenizer.decode(ids)
+
+        else:
+            ids = ids[:, overflow:]
+            util_ctx = server.tokenizer.decode(ids)
+            
+
+        # Sampler
+
+        gs = ExLlamaV2Sampler.Settings()
+        gs.top_k = int(request["top_k"]) if "top_k" in request else 100
+        gs.top_p = float(request["top_p"]) if "top_p" in request else 0.8
+        gs.top_a = float(request["top_a"]) if "top_a" in request else 0
+        gs.min_p = float(request["min_p"]) if "min_p" in request else 0
+        gs.typical = float(request["typical"]) if "typical" in request else 0
+        gs.temperature = float(request["temperature"]) if "temperature" in request else 0.9
+        gs.skew = float(request["skew"]) if "skew" in request else 0.0
+        gs.token_repetition_penalty = float(request["rep_pen"]) if "rep_pen" in request else 1.05
+        gs.token_frequency_penalty = float(request["freq_pen"]) if "freq_pen" in request else 0.0
+        gs.token_presence_penalty = float(request["pres_pen"]) if "pres_pen" in request else 0.0
+        if bb is not None:
+            gs.disallow_tokens(server.tokenizer, bb)
+
+        # Generate
+
+        server.generator.set_stop_conditions(sc)
+        server.generator.begin_stream(ids, gs, token_healing = request["token_healing"] if "token_healing" in request else False)
+
+        completion = ""
+        gen_tokens = 0
+        response["util_text"] = util_ctx
+        while True:
+            chunk, eos, _ = server.generator.stream()
+            completion += chunk
+            gen_tokens += 1
+
+            if stream and chunk != "":
+                response["response_type"] = "chunk"
+                response["chunk"] = chunk
+                if full_response: response["response"] = completion
+                await ws.send(json.dumps(response))
+            response["chunk"] = ''
+
+            if eos:
+                response["stop_reason"] = "eos"
+                break
 
-    # Sampler
+            if gen_tokens >= num_tokens:
+                response["stop_reason"] = "num_tokens"
+                break
+
+            await asyncio.sleep(0)
+
+            if server.stop_signal.is_set():
+                server.stop_signal.clear()
+                response["stop_reason"] = "interrupted"
+                break
+
+        #if stream: del response["chunk"]
+        response["response_type"] = "full"
+
+        response["response"] = completion
+
+
+def stop(request, ws, server, response):
+
+    """
+    request:  { action: str = "stop",
+                request_id: str,                    # (optional) request ID to echo in response packet
+                response_id: str }                  # (optional) response ID to echo in response packet
+
+    response: { action: str = "stop",
+                request_id: str,                    # (optional)
+                response_id: str }                  # (optional)
+    """
 
-    gs = ExLlamaV2Sampler.Settings()
-    gs.top_k = int(request["top_k"]) if "top_k" in request else 100
-    gs.top_p = float(request["top_p"]) if "top_p" in request else 0.8
-    gs.typical = float(request["typical"]) if "typical" in request else 0
-    gs.temperature = float(request["temperature"]) if "temperature" in request else 0.95
-    gs.token_repetition_penalty = float(request["rep_pen"]) if "rep_pen" in request else 1.15
-
-    # Generate
-
-    server.generator.set_stop_conditions(sc)
-    server.generator.begin_stream(ids, gs, token_healing = request["token_healing"] if "token_healing" in request else False)
-
-    completion = ""
-    gen_tokens = 0
-
-    while True:
-        chunk, eos, _ = server.generator.stream()
-        completion += chunk
-        gen_tokens += 1
-
-        if stream and chunk != "":
-            response["response_type"] = "chunk"
-            response["chunk"] = chunk
-            if full_response: response["response"] = completion
-            await ws.send(json.dumps(response))
-
-        if eos or gen_tokens >= num_tokens: break
-
-    if stream: del response["chunk"]
-    response["response_type"] = "full"
-    response["util_text"] = util_ctx
-    response["response"] = completion
+    server.stop_signal.set()
```

## Comparing `exllamav2-0.0.9.dist-info/LICENSE` & `exllamav2-0.1.0.dist-info/LICENSE`

 * *Files identical despite different names*

