# Comparing `tmp/ALLM-1.0.6-py3-none-any.whl.zip` & `tmp/ALLM-1.0.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,25 @@
-Zip file size: 22300 bytes, number of entries: 21
--rw-rw-rw-  2.0 fat       39 b- defN 24-May-12 00:08 ALLM/__init__.py
--rw-rw-rw-  2.0 fat     3706 b- defN 24-May-12 00:08 ALLM/agentapi.py
--rw-rw-rw-  2.0 fat     3440 b- defN 24-May-12 00:08 ALLM/agentchat.py
--rw-rw-rw-  2.0 fat     4986 b- defN 24-May-12 00:08 ALLM/azureagentapi.py
--rw-rw-rw-  2.0 fat     3298 b- defN 24-May-12 00:08 ALLM/azureagentchat.py
--rw-rw-rw-  2.0 fat     2435 b- defN 24-May-12 00:08 ALLM/azurecli.py
--rw-rw-rw-  2.0 fat      757 b- defN 24-May-12 00:08 ALLM/cli.py
--rw-rw-rw-  2.0 fat    11849 b- defN 24-May-12 00:08 ALLM/instruct.py
--rw-rw-rw-  2.0 fat     3096 b- defN 24-May-12 00:08 ALLM/newagent.py
--rw-rw-rw-  2.0 fat     2795 b- defN 24-May-12 00:08 ALLM/serve.py
--rw-rw-rw-  2.0 fat     1927 b- defN 24-May-12 00:08 ALLM/studio.py
--rw-rw-rw-  2.0 fat      683 b- defN 24-May-12 00:08 ALLM/test.py
--rw-rw-rw-  2.0 fat     3096 b- defN 24-May-12 00:08 ALLM/updateagent.py
--rw-rw-rw-  2.0 fat     4530 b- defN 24-May-12 00:08 ALLM/vertexagentapi.py
--rw-rw-rw-  2.0 fat     2824 b- defN 24-May-12 00:08 ALLM/vertexagentchat.py
--rw-rw-rw-  2.0 fat     2124 b- defN 24-May-12 00:08 ALLM/vertexcli.py
--rw-rw-rw-  2.0 fat     7642 b- defN 24-May-12 05:45 ALLM-1.0.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-May-12 05:45 ALLM-1.0.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat      517 b- defN 24-May-12 05:45 ALLM-1.0.6.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        5 b- defN 24-May-12 05:45 ALLM-1.0.6.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     1558 b- defN 24-May-12 05:45 ALLM-1.0.6.dist-info/RECORD
-21 files, 61399 bytes uncompressed, 19834 bytes compressed:  67.7%
+Zip file size: 30236 bytes, number of entries: 23
+-rw-rw-rw-  2.0 fat       39 b- defN 24-May-26 05:29 ALLM/__init__.py
+-rw-rw-rw-  2.0 fat     3706 b- defN 24-May-26 05:29 ALLM/agentapi.py
+-rw-rw-rw-  2.0 fat     3440 b- defN 24-May-26 05:29 ALLM/agentchat.py
+-rw-rw-rw-  2.0 fat     4986 b- defN 24-May-26 05:29 ALLM/azureagentapi.py
+-rw-rw-rw-  2.0 fat     3298 b- defN 24-May-26 05:29 ALLM/azureagentchat.py
+-rw-rw-rw-  2.0 fat     2435 b- defN 24-May-26 05:29 ALLM/azurecli.py
+-rw-rw-rw-  2.0 fat    25481 b- defN 24-May-26 05:29 ALLM/backend.py
+-rw-rw-rw-  2.0 fat     4952 b- defN 24-May-26 05:29 ALLM/chat_history.py
+-rw-rw-rw-  2.0 fat     3221 b- defN 24-May-26 05:29 ALLM/cli.py
+-rw-rw-rw-  2.0 fat    12685 b- defN 24-May-26 05:29 ALLM/instruct.py
+-rw-rw-rw-  2.0 fat     3096 b- defN 24-May-26 05:29 ALLM/newagent.py
+-rw-rw-rw-  2.0 fat     2231 b- defN 24-May-26 05:29 ALLM/serve.py
+-rw-rw-rw-  2.0 fat     1932 b- defN 24-May-26 05:29 ALLM/studio.py
+-rw-rw-rw-  2.0 fat      683 b- defN 24-May-26 05:29 ALLM/test.py
+-rw-rw-rw-  2.0 fat     3096 b- defN 24-May-26 05:29 ALLM/updateagent.py
+-rw-rw-rw-  2.0 fat     4530 b- defN 24-May-26 05:29 ALLM/vertexagentapi.py
+-rw-rw-rw-  2.0 fat     2826 b- defN 24-May-26 05:29 ALLM/vertexagentchat.py
+-rw-rw-rw-  2.0 fat     2126 b- defN 24-May-26 05:29 ALLM/vertexcli.py
+-rw-rw-rw-  2.0 fat     7777 b- defN 24-May-26 17:37 ALLM-1.0.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-26 17:37 ALLM-1.0.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat      549 b- defN 24-May-26 17:37 ALLM-1.0.7.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        5 b- defN 24-May-26 17:37 ALLM-1.0.7.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     1709 b- defN 24-May-26 17:37 ALLM-1.0.7.dist-info/RECORD
+23 files, 94895 bytes uncompressed, 27548 bytes compressed:  71.0%
```

## zipnote {}

```diff
@@ -12,14 +12,20 @@
 
 Filename: ALLM/azureagentchat.py
 Comment: 
 
 Filename: ALLM/azurecli.py
 Comment: 
 
+Filename: ALLM/backend.py
+Comment: 
+
+Filename: ALLM/chat_history.py
+Comment: 
+
 Filename: ALLM/cli.py
 Comment: 
 
 Filename: ALLM/instruct.py
 Comment: 
 
 Filename: ALLM/newagent.py
@@ -42,23 +48,23 @@
 
 Filename: ALLM/vertexagentchat.py
 Comment: 
 
 Filename: ALLM/vertexcli.py
 Comment: 
 
-Filename: ALLM-1.0.6.dist-info/METADATA
+Filename: ALLM-1.0.7.dist-info/METADATA
 Comment: 
 
-Filename: ALLM-1.0.6.dist-info/WHEEL
+Filename: ALLM-1.0.7.dist-info/WHEEL
 Comment: 
 
-Filename: ALLM-1.0.6.dist-info/entry_points.txt
+Filename: ALLM-1.0.7.dist-info/entry_points.txt
 Comment: 
 
-Filename: ALLM-1.0.6.dist-info/top_level.txt
+Filename: ALLM-1.0.7.dist-info/top_level.txt
 Comment: 
 
-Filename: ALLM-1.0.6.dist-info/RECORD
+Filename: ALLM-1.0.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ALLM/cli.py

```diff
@@ -1,16 +1,86 @@
 import argparse
 from .instruct import load_model, infer
+from flask import Flask, render_template, request
+from flask_socketio import SocketIO
+import time
+from flask_cors import CORS
+from concurrent.futures import ThreadPoolExecutor
+import os
+from llama_index.llms.llama_cpp import LlamaCPP
 
+
+def cli_inference(args):
+    model_path = load_model(args.name)
+    print("Starting CLI-based inference...\n")
+    infer(model_path, args.temperature, args.max_new_tokens, args.model_kwargs)
+
+def launch_ui():
+        app = Flask(__name__)
+        CORS(app, resources={r"/*": {"origins": "*"}})
+        socketio = SocketIO(app)
+
+        prompt_template = "<s>[INST] {prompt} [/INST]"
+
+        executor = ThreadPoolExecutor(max_workers=1)
+
+        @app.route('/')
+        def index():
+            return render_template('index.html')
+
+        @app.route('/send_message', methods=['POST'])
+        def send_message():
+            data = request.json
+            prompt = data['userInput']
+            modelPath = data['filePath']
+            filepath = os.path.normpath(modelPath)
+            path=filepath.replace("\\", "\\\\")
+            temperature = data['temperature']
+            useGPU = data['useGPU']
+            print('Received userInput:', prompt)
+            print('Received filePath:', path)
+            print('Received temperature:', temperature)
+            print('Received useGPU:', useGPU)
+
+            formatted_prompt = prompt_template.format(prompt=prompt)
+
+            # Initialize LLAM object with the selected model path, temperature, and GPU settings
+            llm = LlamaCPP(
+                model_path=path,
+                temperature=float(temperature),
+                max_new_tokens=512,
+                context_window=3900,
+                model_kwargs={"n_gpu_layers": 1 if useGPU else 0},
+                verbose=False,
+            )
+
+            # Perform inference and stream the generated tokens
+            response_iter = llm.stream_complete(formatted_prompt)
+            prev_token = ''
+            for response in response_iter:
+                token = response.delta
+                # new_token = token.replace(prev_token, token)
+                # if new_token:
+                print(token, end='')
+                socketio.emit('response', token)
+                # prev_token = token
+            time.sleep(0.01)
+
+
+            return None
+        socketio.run(app)
 def main():
     parser = argparse.ArgumentParser()
     parser.add_argument("--name", type=str, default="Mistral", help="Name of the model or path to the model file")
     parser.add_argument("--temperature", type=float, default=0.5, help="Temperature for sampling")
     parser.add_argument("--max_new_tokens", type=int, default=512, help="Maximum number of new tokens to generate")
     parser.add_argument("--model_kwargs", type=dict, default={"n_gpu_layers":0}, help="Arguments for the model")
+    parser.add_argument('--launch', action='store_true', help='Launch the UI for inference')
     args = parser.parse_args()
 
-    model_path = load_model(args.name)
-    infer(model_path, args.temperature, args.max_new_tokens, args.model_kwargs)
+    if args.launch:
+        launch_ui()
+    else:
+        cli_inference(args)
 
 if __name__ == "__main__":
     main()
```

## ALLM/instruct.py

```diff
@@ -1,11 +1,14 @@
 from llama_index.llms.llama_cpp import LlamaCPP
 from huggingface_hub import hf_hub_download
 import os
 import shutil
+from flask import Flask, request, jsonify
+
+
 
 def load_model(model):
             model_dir = 'model'
             if not os.path.exists(model_dir):
                 os.makedirs(model_dir)
 
             model_files = [f for f in os.listdir('model') if f.endswith('.gguf')]
@@ -146,14 +149,28 @@
                     # except:
                     #     print("Kindly check input parameter to the model_load() function. The parameter should be one of the following: \"Mistral\", \"mistral\", \"Mistral_instruct\", \"mistral_instruct\", \"Llama2\", \"Llama2_chat\", or a local gguf file path on your system")
 
             return model_path
 
 
 
+model_files = [f for f in os.listdir('model') if f.endswith('.gguf')]
+model_path = load_model(model_files[0]) if model_files else None
+
+llm = LlamaCPP(
+    model_path=model_path,
+    temperature=0.1,
+    max_new_tokens=512,
+    context_window=3900,
+    model_kwargs={"n_gpu_layers": 0},
+    verbose=False,
+)
+
+
+
 def infer(model_path, temperature=0.5, max_new_tokens=512, model_kwargs={"n_gpu_layers":0}):
 
 
         # Define a prompt template
         prompt_template = "<s>[INST] {prompt} [/INST]"
         # print(model_path)
 
@@ -189,9 +206,20 @@
                 for response in response_iter:
                     print(response.delta, end="", flush=True)
 
             except KeyboardInterrupt:
                 print("\nExiting...")
                 break
 
+def api_serve():
+        user_input = request.json.get("input")
+
+        if user_input.lower() == "exit":
+            return jsonify({"response": "Exiting chat."})
+
+        prompt_template = "<s>[INST] {prompt} [/INST]"
+        prompt = prompt_template.format(prompt=user_input)
+
+        response_iter = llm.stream_complete(prompt)
+        response_text = ''.join(response.delta for response in response_iter)
 
-        
+        return jsonify({"response": response_text})
```

## ALLM/serve.py

```diff
@@ -1,48 +1,29 @@
 from flask import Flask, request, jsonify
-from llama_index.llms.llama_cpp import LlamaCPP
-from .instruct import load_model
+from .instruct import load_model, api_serve
 import os
+from llama_index.llms.llama_cpp import LlamaCPP
+import json
 
 app = Flask(__name__)
 
-# Check if apiconfig.txt exists in the model folder
-config_file_path = os.path.join('model', 'apiconfig.txt')
-if not os.path.exists(config_file_path):
-    # Create apiconfig.txt with default values
-    with open(config_file_path, 'w') as file:
-       file.write('Host=127.0.0.1\n')
-       file.write('Port=5000\n')
-       file.write('CertFile=""\n')
-       file.write('CertKey=""\n')
-
-# Read host and port from apiconfig.txt
-if os.path.exists(config_file_path):
-    with open(config_file_path, 'r') as file:
-        for line in file:
-            key, value = line.strip().split('=')
-            if key == 'Host':
-                host = value
-            elif key == 'Port':
-                port = int(value)
-            elif key == 'CertFile':
-                cert_file = value.strip('"')
-            elif key == 'CertKey':
-                cert_key = value.strip('"')
-else:
-    print("File doesn't exist.")
-    host = '127.0.0.1'
-    port = 5000
-    cert_file = ""
-    cert_key = ""
-
-print("Host:", host)
-print("Port:", port)
-print("CertFile:", cert_file)
-print("CertKey:", cert_key)
+model_dir = "model"
+
+config = {
+    "host": "127.0.0.1",
+    "port": "5173",
+    "cert_file": "",
+    "cert_key": ""
+}
+
+file_path=os.path.join(model_dir, "apiconfig.json")
+if not os.path.exists(file_path):
+    with open(file_path, "w") as json_file:
+        json.dump(config, json_file)
+
 
 model_files = [f for f in os.listdir('model') if f.endswith('.gguf')]
 model_path = load_model(model_files[0]) if model_files else None
 
 llm = LlamaCPP(
     model_path=model_path,
     temperature=0.1,
@@ -64,16 +45,22 @@
 
     response_iter = llm.stream_complete(prompt)
     response_text = ''.join(response.delta for response in response_iter)
 
     return jsonify({"response": response_text})
 
 def main():
+        with open(file_path, "r") as json_file:
+            config = json.load(json_file)
+            host=config["host"]
+            port=config["port"]
+            cert_file=config["cert_file"]
+            cert_key=config["cert_key"]
         if cert_file is not "" and cert_key is not "":
-            print(f"Inference is working on https://{host}:{port}/v1/chat/completions. You can configure custom host IP and port, and ssl certificate via the apiconfig.txt file available at {config_file_path}")
+            print(f"Inference is working on https://{host}:{port}/v1/chat/completions. You can configure custom host IP and port, and ssl certificate via the apiconfig.txt file available at {file_path}")
             app.run(host=host, port=port, debug=False, ssl_context=(cert_file,cert_key))
         else:
-            print(f"Inference is working on http://{host}:{port}/v1/chat/completions. You can configure custom host IP and port, and ssl certificate via the apiconfig.txt file available at {config_file_path}")
+            print(f"Inference is working on http://{host}:{port}/v1/chat/completions. You can configure custom host IP and port, and ssl certificate via the apiconfig.txt file available at {file_path}")
             app.run(host=host, port=port, debug=False)
 
 if __name__ == "__main__":
     main()
```

## ALLM/studio.py

```diff
@@ -15,15 +15,14 @@
 executor = ThreadPoolExecutor(max_workers=1)
 
 @app.route('/')
 def index():
     return render_template('index.html')
 
 @app.route('/send_message', methods=['POST'])
-@app.route('/send_message', methods=['POST'])
 def send_message():
     data = request.json
     prompt = data['userInput']
     modelPath = data['filePath']
     filepath = os.path.normpath(modelPath)
     path=filepath.replace("\\", "\\\\")
     temperature = data['temperature']
@@ -56,11 +55,12 @@
         socketio.emit('response', token)
         # prev_token = token
     time.sleep(0.01)
 
 
     return None
 
-
+def main():
+    socketio.run(app, host='localhost', port=8080)
 
 if __name__ == '__main__':
-    socketio.run(app)
+    main()
```

## ALLM/vertexagentchat.py

```diff
@@ -50,15 +50,15 @@
     parser.add_argument("--agent", type=str, default=None, help="Name of the agent to query.")
     args = parser.parse_args()
     with open(file_path, "r") as json_file:
         config = json.load(json_file)
         projectid=config["project_id"]
         region=config["region"]
         model=config["model"] 
-        print(projectid, region, model)
+        # print(projectid, region, model)
     if projectid != "" and region != "" and model != "":
     # Initialize Vertex AI
         vertexai.init(project=projectid, location=region)
         # Load the model
         multimodal_model = GenerativeModel(model_name=model)
     else:
         vertexai.init(project=args.projectid, location=args.region)
```

## ALLM/vertexcli.py

```diff
@@ -37,15 +37,15 @@
 
     # if os.path.exists(file_path):
     with open(file_path, "r") as json_file:
         config = json.load(json_file)
         projectid=config["project_id"]
         region=config["region"]
         model=config["model"] 
-        print(projectid, region, model)
+        # print(projectid, region, model)
     if projectid != "" and region != "" and model != "":
     # Initialize Vertex AI
         vertexai.init(project=projectid, location=region)
         # Load the model
         multimodal_model = GenerativeModel(model_name=model)
     else:
         vertexai.init(project=args.projectid, location=args.region)
```

## Comparing `ALLM-1.0.6.dist-info/METADATA` & `ALLM-1.0.7.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 Metadata-Version: 2.1
 Name: ALLM
-Version: 1.0.6
-Summary: A simple and efficient python library for fast inference of LLMs.
+Version: 1.0.7
+Summary: A simple and efficient python library for fast inference of LLMs
 Author: All Advance AI
 Author-email: allmdev@allaai.com
 Maintainer: All Advance AI
 Maintainer-email: allmdev@allaai.com
 Keywords: GGUF,GGUF Large Language Model,GGUF Large Language Models,GGUF Large Language Modeling,GGUF Large Language Modeling Library
 Description-Content-Type: text/markdown
 Requires-Dist: Flask
@@ -19,14 +19,15 @@
 Requires-Dist: chromadb ==0.3.26
 Requires-Dist: pdfminer.six
 Requires-Dist: pydantic ==1.10.13
 Requires-Dist: sentence-transformers
 Requires-Dist: vertexai
 Requires-Dist: google-cloud-aiplatform
 Requires-Dist: openai
+Requires-Dist: python-multipart
 
 
 # ALLM
 
 
 ALLM is a Python library designed for fast inference of GGUF (Generic Global Unsupervised Features) Large Language Models (LLMs) on both CPU and GPU. It provides a convenient interface for loading pre-trained GGUF models and performing inference using them. This library is ideal for applications where quick response times are crucial, such as chatbots, text generation, and more.
 
@@ -193,18 +194,23 @@
 
 ### 2.6 AzureOpenAI AgentChat API 
 
 ```bash
 allm-agentapi-vertex --agent agent_name
 ```
 
+
+### 2.7 ALLM-Enterprise
+You can launch the UI with the following command:
+```bash
+allm-launch
+```
+
 ## Supported Model names.
 
 - Llama3
 - Llama2
 - Llama
 - Llama2_chat
 - Llama_chat
 - Mistral
 - Mistral_instruct
-
-
```

## Comparing `ALLM-1.0.6.dist-info/entry_points.txt` & `ALLM-1.0.7.dist-info/entry_points.txt`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 [console_scripts]
 allm-agentapi = ALLM.agentapi:main
 allm-agentapi-azure = ALLM.azureagentapi:main
 allm-agentapi-vertex = ALLM.vertexagentapi:main
 allm-agentchat = ALLM.agentchat:main
 allm-agentchat-azure = ALLM.azureagentchat:main
 allm-agentchat-vertex = ALLM.vertexagentchat:main
+allm-launch = ALLM.backend:main
 allm-newagent = ALLM.newagent:main
 allm-run = ALLM.cli:main
 allm-run-azure = ALLM.azurecli:main
 allm-run-vertex = ALLM.vertexcli:main
 allm-serve = ALLM.serve:main
 allm-studio = ALLM.studio:main
 allm-updateagent = ALLM.updateagent:main
```

## Comparing `ALLM-1.0.6.dist-info/RECORD` & `ALLM-1.0.7.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 ALLM/__init__.py,sha256=8XSO2SVH9cZ_Iut8rNwcDxtEaBp4LIdXP6hAKGRgeaI,39
 ALLM/agentapi.py,sha256=vnkpCaviW0w5EQTE99els4Nb8QABv0TgNoP2FdFsWVQ,3706
 ALLM/agentchat.py,sha256=UHLn4mVEBz6C77G8aaf3DWrzuVPfA-PCEp6uFZpcGyM,3440
 ALLM/azureagentapi.py,sha256=dzMwkJHfRBqRxYAwpe_aQRgASqnsNBaEzT9jsc791QQ,4986
 ALLM/azureagentchat.py,sha256=6L9m2B6dfe9LUWdd_Vj6_TT1n1GBXxVvqOeA-yTuV_Q,3298
 ALLM/azurecli.py,sha256=nLoSVPpxWwIJWhiddB0_qzNw53hrAE6ijK0diDlaGYs,2435
-ALLM/cli.py,sha256=wWOt7Uv_DmQKT821rEIaEVh9MFJVz0n19BXgwTHsdDY,757
-ALLM/instruct.py,sha256=qFO727_ehoI5Luj07D1ptomdwtRg0MByR1hr1bcA8as,11849
+ALLM/backend.py,sha256=owOM8c2NBz2Z4sxzqBCd0hbNJq79nQ1aZrlETzco0Jc,25481
+ALLM/chat_history.py,sha256=Et8kDK4RKdIr-v4CoYdvd5yb37GxTHpu4U1gHw5C2GY,4952
+ALLM/cli.py,sha256=13o2i7g_DtOyP8N8x7QdMr8iUGr4raJX26NJ9oqpgNw,3221
+ALLM/instruct.py,sha256=-3ybhrmAwgLy_MptpJHhFnbQq1NcT4DHeMV2-46Tzl8,12685
 ALLM/newagent.py,sha256=BzdKTA6gutbZbJS3nL1aWZqmTxe7QABkrLz9zpxlQZc,3096
-ALLM/serve.py,sha256=SWs9Afph4glJjCcX46rYNA0N0xQZiXmBv0H_DXNCxnU,2795
-ALLM/studio.py,sha256=bI4_O3Ed6T3ngPBTVqhJS3U6i9DvSuSDgh8h88hjAOc,1927
+ALLM/serve.py,sha256=RUKwji_795izII7XlYku1KUK6bvgR_SYBB9N34GwqUA,2231
+ALLM/studio.py,sha256=LlrIEWcs-o8cIjnprJVijR8cNxrOsnFvl9AgTrk43iw,1932
 ALLM/test.py,sha256=CxP_WW9ZXEiLCJh_9MLdieFU-yfNMwWlXrcrs7gtXx8,683
 ALLM/updateagent.py,sha256=BzdKTA6gutbZbJS3nL1aWZqmTxe7QABkrLz9zpxlQZc,3096
 ALLM/vertexagentapi.py,sha256=fPbLJoy8Wirh7wPc31EzlgLSqfF2e4PkEwYCO9h4yqk,4530
-ALLM/vertexagentchat.py,sha256=k15s16rOtQYNBQnGWI4cXYd4ywOcT2lSaHx_34Qb6DQ,2824
-ALLM/vertexcli.py,sha256=48piq7FW15eRJoQCKR8x3ke_gvsBhJsKVfMLHJX8auw,2124
-ALLM-1.0.6.dist-info/METADATA,sha256=EzY6QEfGm5OOu1dEAkn04xpUVl4RazxuPIyB6XYaR7o,7642
-ALLM-1.0.6.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-ALLM-1.0.6.dist-info/entry_points.txt,sha256=78f3Rj0eXUq9pIusKsqKN1CeG1LmR6CXcvgGWxmT4z8,517
-ALLM-1.0.6.dist-info/top_level.txt,sha256=7I9ZpiXpdc4mz6ZgmIvmsbw5M2OC1m99v28mTvj1qEM,5
-ALLM-1.0.6.dist-info/RECORD,,
+ALLM/vertexagentchat.py,sha256=dYvJ2_teBGeEN3TTrXq4pSulYJW5kmNPKFa9xg9ec9s,2826
+ALLM/vertexcli.py,sha256=-5BgOKO7oMsjcsJRluAM0ivWllzpBpWmVQIzHbq_Xrk,2126
+ALLM-1.0.7.dist-info/METADATA,sha256=_tQnW5QBeUdK_6efPT0rHFNKh0_mu9ltZhnSfSnKJgc,7777
+ALLM-1.0.7.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+ALLM-1.0.7.dist-info/entry_points.txt,sha256=vCZAJD4b7UpXLvqotmz5BRa2CTGSKaDp549bW4X0isw,549
+ALLM-1.0.7.dist-info/top_level.txt,sha256=7I9ZpiXpdc4mz6ZgmIvmsbw5M2OC1m99v28mTvj1qEM,5
+ALLM-1.0.7.dist-info/RECORD,,
```

