# Comparing `tmp/tabgan-2.0.5-py2.py3-none-any.whl.zip` & `tmp/tabgan-2.2.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,27 +1,27 @@
-Zip file size: 37415 bytes, number of entries: 25
--rw-rw-rw-  2.0 fat      128 b- defN 23-Sep-28 19:58 _ForestDiffusion/README.MD
--rw-rw-rw-  2.0 fat      124 b- defN 23-Oct-03 19:39 _ForestDiffusion/__init__.py
--rw-rw-rw-  2.0 fat    18289 b- defN 23-Oct-03 20:45 _ForestDiffusion/diffusion_with_trees_class.py
--rw-rw-rw-  2.0 fat      213 b- defN 23-Oct-03 19:39 _ForestDiffusion/utils/__init__.py
--rw-rw-rw-  2.0 fat     6390 b- defN 23-Sep-28 19:53 _ForestDiffusion/utils/diffusion.py
--rw-rw-rw-  2.0 fat     1215 b- defN 23-Oct-03 19:39 _ForestDiffusion/utils/utils_diffusion.py
+Zip file size: 39337 bytes, number of entries: 25
+-rw-rw-rw-  2.0 fat      128 b- defN 24-May-25 12:23 _ForestDiffusion/README.MD
+-rw-rw-rw-  2.0 fat      124 b- defN 24-May-25 12:23 _ForestDiffusion/__init__.py
+-rw-rw-rw-  2.0 fat    18289 b- defN 24-May-25 12:23 _ForestDiffusion/diffusion_with_trees_class.py
+-rw-rw-rw-  2.0 fat      213 b- defN 24-May-25 12:23 _ForestDiffusion/utils/__init__.py
+-rw-rw-rw-  2.0 fat     6390 b- defN 24-May-25 12:23 _ForestDiffusion/utils/diffusion.py
+-rw-rw-rw-  2.0 fat     1215 b- defN 24-May-25 12:23 _ForestDiffusion/utils/utils_diffusion.py
 -rw-rw-rw-  2.0 fat       60 b- defN 21-Dec-25 19:24 _ctgan/README.MD
 -rw-rw-rw-  2.0 fat      202 b- defN 21-Dec-25 19:24 _ctgan/__init__.py
 -rw-rw-rw-  2.0 fat     4408 b- defN 23-May-04 17:50 _ctgan/conditional.py
 -rw-rw-rw-  2.0 fat     2466 b- defN 23-May-04 18:44 _ctgan/models.py
 -rw-rw-rw-  2.0 fat     1241 b- defN 23-Mar-29 20:14 _ctgan/sampler.py
 -rw-rw-rw-  2.0 fat    11533 b- defN 23-May-04 18:27 _ctgan/synthesizer.py
 -rw-rw-rw-  2.0 fat     5959 b- defN 23-Oct-03 19:53 _ctgan/transformer.py
--rw-rw-rw-  2.0 fat      374 b- defN 21-Dec-25 19:24 tabgan/__init__.py
--rw-rw-rw-  2.0 fat     4630 b- defN 23-Oct-03 20:49 tabgan/abc_sampler.py
--rw-rw-rw-  2.0 fat     8374 b- defN 23-Oct-03 20:45 tabgan/adversarial_model.py
+-rw-rw-rw-  2.0 fat      477 b- defN 24-May-26 12:32 tabgan/__init__.py
+-rw-rw-rw-  2.0 fat     4630 b- defN 24-May-25 12:23 tabgan/abc_sampler.py
+-rw-rw-rw-  2.0 fat     8458 b- defN 24-May-25 12:34 tabgan/adversarial_model.py
 -rw-rw-rw-  2.0 fat    10998 b- defN 23-Mar-29 20:14 tabgan/encoders.py
--rw-rw-rw-  2.0 fat    17722 b- defN 23-Oct-03 21:02 tabgan/sampler.py
--rw-rw-rw-  2.0 fat     1766 b- defN 23-May-04 17:50 tabgan/utils.py
--rw-rw-rw-  2.0 fat       84 b- defN 23-Oct-03 21:06 tabgan-2.0.5.dist-info/AUTHORS.rst
--rw-rw-rw-  2.0 fat    11474 b- defN 23-Oct-03 21:06 tabgan-2.0.5.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    10741 b- defN 23-Oct-03 21:06 tabgan-2.0.5.dist-info/METADATA
--rw-rw-rw-  2.0 fat      110 b- defN 23-Oct-03 21:06 tabgan-2.0.5.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       31 b- defN 23-Oct-03 21:06 tabgan-2.0.5.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2009 b- defN 23-Oct-03 21:06 tabgan-2.0.5.dist-info/RECORD
-25 files, 120541 bytes uncompressed, 34205 bytes compressed:  71.6%
+-rw-rw-rw-  2.0 fat    21364 b- defN 24-May-26 14:19 tabgan/sampler.py
+-rw-rw-rw-  2.0 fat     5961 b- defN 24-May-26 14:58 tabgan/utils.py
+-rw-rw-rw-  2.0 fat       84 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/AUTHORS.rst
+-rw-rw-rw-  2.0 fat    11474 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    11332 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      110 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       31 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2009 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/RECORD
+25 files, 129156 bytes uncompressed, 36127 bytes compressed:  72.0%
```

## zipnote {}

```diff
@@ -51,26 +51,26 @@
 
 Filename: tabgan/sampler.py
 Comment: 
 
 Filename: tabgan/utils.py
 Comment: 
 
-Filename: tabgan-2.0.5.dist-info/AUTHORS.rst
+Filename: tabgan-2.2.0.dist-info/AUTHORS.rst
 Comment: 
 
-Filename: tabgan-2.0.5.dist-info/LICENSE
+Filename: tabgan-2.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: tabgan-2.0.5.dist-info/METADATA
+Filename: tabgan-2.2.0.dist-info/METADATA
 Comment: 
 
-Filename: tabgan-2.0.5.dist-info/WHEEL
+Filename: tabgan-2.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: tabgan-2.0.5.dist-info/top_level.txt
+Filename: tabgan-2.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: tabgan-2.0.5.dist-info/RECORD
+Filename: tabgan-2.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tabgan/__init__.py

```diff
@@ -1,9 +1,10 @@
 # -*- coding: utf-8 -*-
 from pkg_resources import DistributionNotFound, get_distribution
+from .sampler import OriginalGenerator, Sampler, GANGenerator, ForestDiffusionGenerator, LLMGenerator
 
 try:
     # Change here if project is renamed and does not equal the package name
     dist_name = __name__
     __version__ = get_distribution(dist_name).version
 except DistributionNotFound:
     __version__ = "unknown"
```

## tabgan/adversarial_model.py

```diff
@@ -1,13 +1,14 @@
 import numpy as np
 import pandas as pd
 from lightgbm import LGBMClassifier
 from scipy.stats import rankdata
 from sklearn.metrics import roc_auc_score
 from sklearn.model_selection import StratifiedKFold
+import warnings
 
 from tabgan.encoders import MultipleEncoder, DoubleValidationEncoderNumerical
 
 
 class AdversarialModel:
     def __init__(
             self,
@@ -118,14 +119,16 @@
             X: Input training dataframe
             y: Target for X
 
         Returns:
             mean_score_train, mean_score_val, avg_num_trees
         """
         # process cat cols
+        warnings.filterwarnings("ignore", category=UserWarning)
+
         if self.cat_validation == "None":
             encoder = MultipleEncoder(
                 cols=self.cat_cols, encoders_names_tuple=self.encoders_names
             )
             X = encoder.fit_transform(X, y)
 
         for n_fold, (train_idx, val_idx) in enumerate(
```

## tabgan/sampler.py

```diff
@@ -1,30 +1,32 @@
 # -*- coding: utf-8 -*-
 
-import gc
 import logging
 import warnings
 from typing import Tuple
 
 import numpy as np
 import pandas as pd
-from _ForestDiffusion import ForestDiffusionModel
+import torch
+from be_great import GReaT
 
+from _ForestDiffusion import ForestDiffusionModel
 from _ctgan.synthesizer import _CTGANSynthesizer as CTGAN
 from tabgan.abc_sampler import Sampler, SampleData
 from tabgan.adversarial_model import AdversarialModel
-from tabgan.utils import setup_logging, get_year_mnth_dt_from_date, collect_dates
+from tabgan.utils import setup_logging, _drop_col_if_exist, \
+    get_columns_if_exists, _sampler, get_year_mnth_dt_from_date, collect_dates
 
 warnings.filterwarnings("ignore")
 
 __author__ = "Insaf Ashrapov"
 __copyright__ = "Insaf Ashrapov"
 __license__ = "Apache 2.0"
 
-__all__ = ["OriginalGenerator", "GANGenerator", "ForestDiffusionGenerator"]
+__all__ = ["OriginalGenerator", "GANGenerator", "ForestDiffusionGenerator", "LLMGenerator"]
 
 
 class OriginalGenerator(SampleData):
     def __init__(self, *args, **kwargs):
         self.args = args
         self.kwargs = kwargs
 
@@ -46,14 +48,23 @@
         self.args = args
         self.kwargs = kwargs
 
     def get_object_generator(self) -> Sampler:
         return SamplerDiffusion(*self.args, **self.kwargs)
 
 
+class LLMGenerator(SampleData):
+    def __init__(self, *args, **kwargs):
+        self.args = args
+        self.kwargs = kwargs
+
+    def get_object_generator(self) -> Sampler:
+        return SamplerLLM(*self.args, **self.kwargs)
+
+
 class SamplerOriginal(Sampler):
     def __init__(
             self,
             gen_x_times: float = 1.1,
             cat_cols: list = None,
             bot_filter_quantile: float = 0.001,
             top_filter_quantile: float = 0.999,
@@ -64,31 +75,32 @@
                 "max_bin": 100,
                 "n_estimators": 150,
                 "learning_rate": 0.02,
                 "random_state": 42,
             },
             pregeneration_frac: float = 2,
             only_generated_data: bool = False,
-            gen_params: dict = {'batch_size': 500, 'patience': 25, "epochs": 500, }
+            gen_params: dict = {"batch_size": 45, 'patience': 25, "epochs": 50, "llm": "distilgpt2"},
     ):
         """
 
         @param gen_x_times: float = 1.1 - how much data to generate, output might be less because of postprocessing and
         adversarial filtering
         @param cat_cols: list = None - categorical columns
         @param bot_filter_quantile: float = 0.001 - bottom quantile for postprocess filtering
         @param top_filter_quantile: float = 0.999 - top quantile for postprocess filtering
         @param is_post_process: bool = True - perform or not postfiltering, if false bot_filter_quantile
          and top_filter_quantile ignored
         @param adversarial_model_params: dict params for adversarial filtering model, default values for binary task
         @param pregeneration_frac: float = 2 - for generation step gen_x_times * pregeneration_frac amount of data
-        will generated. However in postprocessing (1 + gen_x_times) % of original data will be returned
+        will be generated. However, in postprocessing (1 + gen_x_times) % of original data will be returned
         @param only_generated_data: bool = False If True after generation get only newly generated, without
-        concating input train dataframe.
-        @param gen_params: dict params for GAN training. Only works for SamplerGAN or ForestDiffusionGenerator.
+        concatenating input train dataframe.
+        @param gen_params: dict params for GAN training. Only works for SamplerGAN, ForestDiffusionGenerator,
+        LLMGenerator.
         """
         self.gen_x_times = gen_x_times
         self.cat_cols = cat_cols
         self.is_post_process = is_post_process
         self.bot_filter_quantile = bot_filter_quantile
         self.top_filter_quantile = top_filter_quantile
         self.adversarial_model_params = adversarial_model_params
@@ -136,15 +148,15 @@
             )
         self._validate_data(train_df, target, test_df)
         train_df[self.TEMP_TARGET] = target
         generated_df = train_df.sample(
             frac=(1 + self.pregeneration_frac), replace=True, random_state=42
         )
         generated_df = generated_df.reset_index(drop=True)
-        gc.collect()
+
         logging.info(
             "Generated shape: {} and {}".format(
                 generated_df.drop(self.TEMP_TARGET, axis=1).shape,
                 generated_df[self.TEMP_TARGET].shape,
             )
         )
         return (
@@ -183,15 +195,14 @@
                 ]
                 if filtered_df.shape[0] < 10:
                     raise ValueError(
                         "After post-processing generated data's shape less than 10. For columns {} test "
                         "might be highly skewed.".format(num_col)
                     )
                 train_df = filtered_df
-        gc.collect()
         logging.info(
             "Generated shapes after postprocessing: {} plus target".format(
                 train_df.drop(self.TEMP_TARGET, axis=1).shape
             )
         )
         return (
             train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True),
@@ -211,15 +222,15 @@
 
         train_df["test_similarity"] = ad_model.trained_model.predict(
             train_df.drop(self.TEMP_TARGET, axis=1)
         )
         train_df.sort_values("test_similarity", ascending=False, inplace=True)
         train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
         del ad_model
-        gc.collect()
+
         return (
             train_df.drop(["test_similarity", self.TEMP_TARGET], axis=1).reset_index(
                 drop=True
             ),
             train_df[self.TEMP_TARGET].reset_index(drop=True),
         )
 
@@ -239,17 +250,32 @@
                     "Something gone wrong: shape of train_df = {} is not equal to target = {} shape".format(
                         train_df.shape[0], target.shape[0]
                     )
                 )
 
 
 class SamplerGAN(SamplerOriginal):
+    def check_params(self):
+        if self.gen_params["batch_size"] % 2 != 0:
+            logging.warning(
+                "batch_size should even, but {} is provided. Increasing by 1".format(self.gen_params["batch_size"]))
+            self.gen_params["batch_size"] = self.gen_params["batch_size"] + 1
+
+        if "patience" not in self.gen_params:
+            logging.warning("patience param is not set for GAN params, so setting it to default ""25""")
+            self.gen_params["patience"] = 25
+
+        if "epochs" not in self.gen_params:
+            logging.warning("patience param is not set for GAN params, so setting it to default ""50""")
+            self.gen_params["epochs"] = 50
+
     def generate_data(
             self, train_df, target, test_df, only_generated_data: bool
     ) -> Tuple[pd.DataFrame, pd.DataFrame]:
+        self.check_params()
         self._validate_data(train_df, target, test_df)
         if target is not None:
             train_df[self.TEMP_TARGET] = target
         ctgan = CTGAN(batch_size=self.gen_params["batch_size"], patience=self.gen_params["patience"])
         logging.info("training GAN")
         if self.cat_cols is None:
             ctgan.fit(train_df, [], epochs=self.gen_params["epochs"])
@@ -261,15 +287,15 @@
         )
         data_dtype = train_df.dtypes.values
 
         for i in range(len(generated_df.columns)):
             generated_df[generated_df.columns[i]] = generated_df[
                 generated_df.columns[i]
             ].astype(data_dtype[i])
-        gc.collect()
+
         if not only_generated_data:
             train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
             logging.info(
                 "Generated shapes: {} plus target".format(
                     _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
                 )
             )
@@ -283,15 +309,14 @@
                     _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
                 )
             )
             return (
                 _drop_col_if_exist(generated_df, self.TEMP_TARGET),
                 get_columns_if_exists(generated_df, self.TEMP_TARGET),
             )
-        gc.collect()
 
         return (
             _drop_col_if_exist(train_df, self.TEMP_TARGET),
             get_columns_if_exists(train_df, self.TEMP_TARGET),
         )
 
 
@@ -307,26 +332,26 @@
             forest_model = ForestDiffusionModel(train_df.to_numpy(), label_y=None, n_t=50,
                                                 duplicate_K=100,
                                                 diffusion_type='flow', n_jobs=-1)
         else:
             forest_model = ForestDiffusionModel(train_df.to_numpy(), label_y=None, n_t=50,
                                                 duplicate_K=100,
                                                 # todo fix bug with cat cols
-                                                #cat_indexes=self.get_column_indexes(train_df, self.cat_cols),
+                                                # cat_indexes=self.get_column_indexes(train_df, self.cat_cols),
                                                 diffusion_type='flow', n_jobs=-1)
         logging.info("Finished training ForestDiffusionModel")
-        generated_df = forest_model.generate(batch_size=int(self.gen_x_times*train_df.to_numpy().shape[0]))
+        generated_df = forest_model.generate(batch_size=int(self.gen_x_times * train_df.to_numpy().shape[0]))
         data_dtype = train_df.dtypes.values
         generated_df = pd.DataFrame(generated_df)
         generated_df.columns = train_df.columns
         for i in range(len(generated_df.columns)):
             generated_df[generated_df.columns[i]] = generated_df[
                 generated_df.columns[i]
             ].astype(data_dtype[i])
-        gc.collect()
+
         if not only_generated_data:
             train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
             logging.info(
                 "Generated shapes: {} plus target".format(
                     _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
                 )
             )
@@ -340,48 +365,92 @@
                     _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
                 )
             )
             return (
                 _drop_col_if_exist(generated_df, self.TEMP_TARGET),
                 get_columns_if_exists(generated_df, self.TEMP_TARGET),
             )
-        gc.collect()
 
         return (
             _drop_col_if_exist(train_df, self.TEMP_TARGET),
             get_columns_if_exists(train_df, self.TEMP_TARGET),
         )
 
     @staticmethod
     def get_column_indexes(df, column_names):
         return [df.columns.get_loc(col) for col in column_names]
 
 
-def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:
-    _logger = logging.getLogger(__name__)
-    _logger.info("Starting generating data")
-    train, test = creator.generate_data_pipe(in_train, in_target, in_test)
-    _logger.info(train, test)
-    _logger.info("Finished generation\n")
-    return train, test
+class SamplerLLM(SamplerOriginal):
+    def check_params(self):
+        if "llm" not in self.gen_params:
+            logging.warning("llm param is not set for LLM params, so setting it to default ""distilgpt2""")
+            self.gen_params["llm"] = "distilgpt2"
+        if "max_length" not in self.gen_params:
+            logging.warning("max_length param is not set for LLM params, so setting it to default ""500""")
+            self.gen_params["max_length"] = "500"
+
+        if self.gen_params["epochs"] < 3:
+            logging.warning(
+                "Current set epoch = {} for llm training is too low, setting to 3!""".format(
+                    self.gen_params["epochs"]))
+            self.gen_params["epochs"] = 3
 
+    def generate_data(
+            self, train_df, target, test_df, only_generated_data: bool
+    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
+        self._validate_data(train_df, target, test_df)
+        self.check_params()
+        if target is not None:
+            train_df[self.TEMP_TARGET] = target
+        logging.info("Fitting LLM model")
+        is_fp16 = torch.cuda.is_available()
+        model = GReaT(llm=self.gen_params["llm"], batch_size=self.gen_params["batch_size"],
+                      epochs=self.gen_params["epochs"], fp16=is_fp16)
+        model.fit(train_df)
 
-def _drop_col_if_exist(df, col_to_drop) -> pd.DataFrame:
-    """Drops col_to_drop from input dataframe df if such column exists"""
-    if col_to_drop in df.columns:
-        return df.drop(col_to_drop, axis=1)
-    else:
-        return df
+        logging.info("Finished training ForestDiffusionModel")
+        device = "cuda" if torch.cuda.is_available() else "cpu"
+
+        generated_df = model.sample(int(self.gen_x_times * train_df.shape[0]), device=device,
+                                    max_length=self.gen_params["max_length"])
+        data_dtype = train_df.dtypes.values
+        generated_df = pd.DataFrame(generated_df)
+        generated_df.columns = train_df.columns
+        for i in range(len(generated_df.columns)):
+            generated_df[generated_df.columns[i]] = generated_df[
+                generated_df.columns[i]
+            ].astype(data_dtype[i])
 
+        if not only_generated_data:
+            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
+            logging.info(
+                "Generated shapes: {} plus target".format(
+                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
+                )
+            )
+            return (
+                _drop_col_if_exist(train_df, self.TEMP_TARGET),
+                get_columns_if_exists(train_df, self.TEMP_TARGET),
+            )
+        else:
+            logging.info(
+                "Generated shapes: {} plus target".format(
+                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
+                )
+            )
+            return (
+                _drop_col_if_exist(generated_df, self.TEMP_TARGET),
+                get_columns_if_exists(generated_df, self.TEMP_TARGET),
+            )
 
-def get_columns_if_exists(df, col) -> pd.DataFrame:
-    if col in df.columns:
-        return df[col]
-    else:
-        return None
+        return (
+            _drop_col_if_exist(train_df, self.TEMP_TARGET),
+            get_columns_if_exists(train_df, self.TEMP_TARGET),
+        )
 
 
 if __name__ == "__main__":
     setup_logging(logging.DEBUG)
     train_size = 75
     train = pd.DataFrame(
         np.random.randint(-10, 150, size=(train_size, 4)), columns=list("ABCD")
@@ -390,14 +459,20 @@
     target = pd.DataFrame(np.random.randint(0, 2, size=(train_size, 1)), columns=list("Y"))
     test = pd.DataFrame(np.random.randint(0, 100, size=(train_size, 4)), columns=list("ABCD"))
     _sampler(OriginalGenerator(gen_x_times=15), train, target, test)
     _sampler(
         GANGenerator(gen_x_times=10, only_generated_data=False,
                      gen_params={"batch_size": 500, "patience": 25, "epochs": 500, }), train, target, test
     )
+    _sampler(
+        LLMGenerator(gen_params={"batch_size": 32, "epochs": 4, "llm": "distilgpt2",
+                                 "max_length": 500}).generate_data_pipe(train,
+                                                                        target,
+                                                                        test, )
+    )
 
     _sampler(OriginalGenerator(gen_x_times=15), train, None, train)
     _sampler(
         GANGenerator(cat_cols=["A"], gen_x_times=20, only_generated_data=True),
         train,
         None,
         train,
@@ -406,15 +481,16 @@
         ForestDiffusionGenerator(cat_cols=["A"], gen_x_times=1, only_generated_data=True),
         train,
         None,
         train,
     )
     _sampler(
         ForestDiffusionGenerator(gen_x_times=10, only_generated_data=False,
-                     gen_params={"batch_size": 500, "patience": 25, "epochs": 500, }), train, target, test
+                                 gen_params={"batch_size": 500, "patience": 25, "epochs": 500, }),
+        train, target, test
     )
 
     min_date = pd.to_datetime('2019-01-01')
     max_date = pd.to_datetime('2021-12-31')
 
     d = (max_date - min_date).days + 1
```

## tabgan/utils.py

```diff
@@ -1,15 +1,18 @@
 import logging
-import sys
 import os
 import random
+import sys
 
-import pandas as pd
 import numpy as np
+import pandas as pd
 import torch
+from scipy.stats import entropy
+
+__all__ = ["compare_dataframes"]
 
 
 def setup_logging(loglevel):
     """Setup basic logging
 
     Args:
       loglevel (int): minimum loglevel for emitting messages
@@ -58,8 +61,100 @@
     os.environ['PYTHONHASHSEED'] = str(seed)
     np.random.seed(seed)
     torch.manual_seed(seed)
     torch.cuda.manual_seed(seed)
     torch.backends.cudnn.deterministic = True
 
 
+def _sampler(creator, in_train, in_target, in_test) -> None:
+    _logger = logging.getLogger(__name__)
+    _logger.info("Starting generating data")
+    train, test = creator.generate_data_pipe(in_train, in_target, in_test)
+    _logger.info(train, test)
+    _logger.info("Finished generation\n")
+    return train, test
+
+
+def _drop_col_if_exist(df, col_to_drop) -> pd.DataFrame:
+    """Drops col_to_drop from input dataframe df if such column exists"""
+    if col_to_drop in df.columns:
+        return df.drop(col_to_drop, axis=1)
+    else:
+        return df
+
+
+def get_columns_if_exists(df, col) -> pd.DataFrame:
+    if col in df.columns:
+        return df[col]
+    else:
+        return None
+
+
+def compare_dataframes(df_original, df_generated):
+    """
+    Compares two DataFrames for similarity in terms of uniqueness, data quality, and PSI.
+
+    Args:
+      df_original: The original DataFrame.
+      df_generated: The DataFrame with generated numbers.
+
+    Returns:
+        float: A score between 0 (no similarity) and 1 (high similarity) representing the similarity of the two
+        DataFrames.
+
+    # Example usage
+    df1 = pd.DataFrame({"col1": [1, 2, 3, 4], "col2": ["a", "b", "a", "c"]})
+    df2 = pd.DataFrame({"col1": [1, 2, 5, 6], "col2": ["a", "b", "x", "y"]})
+
+    similarity_score = compare_dataframes(df1.copy(), df2.copy())
+    print(similarity_score)
+    """
+
+    # Handle potential differences in row count
+    n_original = len(df_original)
+    n_generated = len(df_generated)
+    min_rows = min(n_original, n_generated)
+
+    # Uniqueness: Ratio of non-null unique values in generated vs original (weighted by min rows)
+    uniq_original = df_original.nunique().sum() / (len(df_original.columns) + 1e-6)
+    uniq_generated = df_generated.nunique().sum() / (len(df_generated.columns) + 1e-6)
+    uniqueness_score = (uniq_generated / uniq_original) * (min_rows / n_generated)
+
+    # Data Quality: Distribution similarity using Kolmogorov-Smirnov test (average across columns)
+    data_quality_scores = []
+    for col in df_original.columns:
+        if col in df_generated.columns:
+            # Ensure both columns have numeric data types before applying K-S test
+            if pd.api.types.is_numeric_dtype(df_original[col]) and pd.api.types.is_numeric_dtype(df_generated[col]):
+                _, p_value = df_original[col].value_counts().sort_index(ascending=False).diff().dropna().abs().sum() / (
+                        n_original + 1e-6), \
+                             df_generated[col].value_counts().sort_index(
+                                 ascending=False).diff().dropna().abs().sum() / (n_generated + 1e-6)
+                # Avoid zero division and set minimum p-value to a small positive value
+                p_value = max(p_value, 1e-6)
+                data_quality_scores.append(p_value)
+    data_quality_score = sum(data_quality_scores) / len(data_quality_scores) if data_quality_scores else 1
+
+    # PSI Similarity: Average PSI across all column pairs (capped at theoretical maximum)
+    psi_scores = []
+    for col_orig in df_original.columns:
+        if col_orig in df_generated.columns:
+            p_orig = df_original[col_orig].value_counts(normalize=True)
+            p_gen = df_generated[col_orig].value_counts(normalize=True)
+            # Handle potential division by zero with entropy function (uses log2 internally)
+            h_orig = entropy(p_orig, base=2)
+            h_gen = entropy(p_gen, base=2)
+            h_joint = entropy(pd.concat([p_orig, p_gen], ignore_index=True), base=2)
+            psi = max(0, min(h_orig + h_gen - h_joint, 1))  # Ensure non-negative and cap at 1
+            psi_scores.append(psi)
+    psi_similarity = sum(psi_scores) / len(psi_scores) if psi_scores else 1
+
+    # Combine uniqueness, data quality, and PSI scores (weighted)
+    similarity_score = 0.5 * uniqueness_score + 0.3 * data_quality_score + 0.2 * psi_similarity
+
+    # Ensure score is between 0 and 1
+    similarity_score = min(max(similarity_score, 0), 1)
+
+    return similarity_score
+
+
 TEMP_TARGET = "_temp_target"
```

## Comparing `tabgan-2.0.5.dist-info/LICENSE` & `tabgan-2.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tabgan-2.0.5.dist-info/METADATA` & `tabgan-2.2.0.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tabgan
-Version: 2.0.5
+Version: 2.2.0
 Summary: Applying GAN in tabular data generation for uneven distribution
 Home-page: https://github.com/Diyago/GAN-for-tabular-data
 Author: Insaf Ashrapov
 Author-email: iashrapov@gmail.com
 License: Apache License 2.0
 Project-URL: Documentation, https://github.com/Diyago/GAN-for-tabular-data
 Platform: any
@@ -29,81 +29,85 @@
 Requires-Dist: pytest ; extra == 'testing'
 Requires-Dist: pytest-cov ; extra == 'testing'
 
 [![CodeFactor](https://www.codefactor.io/repository/github/diyago/gan-for-tabular-data/badge)](https://www.codefactor.io/repository/github/diyago/gan-for-tabular-data)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
 [![Downloads](https://pepy.tech/badge/tabgan)](https://pepy.tech/project/tabgan)
 
-# GANs and Diffusions for tabular  data
+# GANs and TimeGANs, Diffusions, LLM for tabular  data
 
 <img src="./images/tabular_gan.png" height="15%" width="15%">
-Generative Adversarial Networks (GANs) are well-known for their success in realistic image generation. However, they can also be applied to generate tabular data. Here will give opportunity to try some of them.
+Generative  Networks are well-known for their success in realistic image generation. However, they can also be applied to generate tabular data. Here will give opportunity to try some of them.
 
 * Arxiv article: ["Tabular GANs for uneven distribution"](https://arxiv.org/abs/2010.00638)
 * Medium post: [GANs for tabular data](https://towardsdatascience.com/review-of-gans-for-tabular-data-a30a2199342)
 
 ## How to use library
 
 * Installation: `pip install tabgan`
 * To generate new data to train by sampling and then filtering by adversarial training
   call `GANGenerator().generate_data_pipe`:
 
 ``` python
-from tabgan.sampler import OriginalGenerator, GANGenerator, ForestDiffusionGenerator
+from tabgan.sampler import OriginalGenerator, GANGenerator, ForestDiffusionGenerator, LLMGenerator
 import pandas as pd
 import numpy as np
 
+
 # random input data
 train = pd.DataFrame(np.random.randint(-10, 150, size=(150, 4)), columns=list("ABCD"))
 target = pd.DataFrame(np.random.randint(0, 2, size=(150, 1)), columns=list("Y"))
 test = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list("ABCD"))
 
 # generate data
 new_train1, new_target1 = OriginalGenerator().generate_data_pipe(train, target, test, )
-new_train2, new_target2 = GANGenerator().generate_data_pipe(train, target, test, )
+new_train2, new_target2 = GANGenerator(gen_params={"batch_size": 500, "epochs": 10, "patience": 5 }).generate_data_pipe(train, target, test, )
 new_train3, new_target3 = ForestDiffusionGenerator().generate_data_pipe(train, target, test, )
+new_train4, new_target4 = LLMGenerator(gen_params={"batch_size": 32, 
+                                                          "epochs": 4, "llm": "distilgpt2", "max_length": 500}).generate_data_pipe(train, target, test, )
 
 # example with all params defined
 new_train4, new_target4 = GANGenerator(gen_x_times=1.1, cat_cols=None,
            bot_filter_quantile=0.001, top_filter_quantile=0.999, is_post_process=True,
            adversarial_model_params={
                "metrics": "AUC", "max_depth": 2, "max_bin": 100, 
                "learning_rate": 0.02, "random_state": 42, "n_estimators": 100,
            }, pregeneration_frac=2, only_generated_data=False,
            gen_params = {"batch_size": 500, "patience": 25, "epochs" : 500,}).generate_data_pipe(train, target,
                                           test, deep_copy=True, only_adversarial=False, use_adversarial=True)
 ```
 
-All samplers `OriginalGenerator`, `ForestDiffusionGenerator` and `GANGenerator` have same input parameters.
+All samplers `OriginalGenerator`, `ForestDiffusionGenerator`, `LLMGenerator` and `GANGenerator` have same input parameters.
 
 1. **GANGenerator** based on **CTGAN**
-2. **ForestDiffusionGenerator** based on **Forest Diffusion**
+2. **ForestDiffusionGenerator** based on **Forest Diffusion (Tabular Diffusion and Flow-Matching)**
+2. **LLMGenerator** based on **Language Models are Realistic Tabular Data Generators (GReaT framework)**
 
 * **gen_x_times**: float = 1.1 - how much data to generate, output might be less because of postprocessing and
   adversarial filtering
 * **cat_cols**: list = None - categorical columns
 * **bot_filter_quantile**: float = 0.001 - bottom quantile for postprocess filtering
 * **top_filter_quantile**: float = 0.999 - top quantile for postprocess filtering
 * **is_post_process**: bool = True - perform or not post-filtering, if false bot_filter_quantile and top_filter_quantile
   ignored
 * **adversarial_model_params**: dict params for adversarial filtering model, default values for binary task
-* **pregeneration_frac**: float = 2 - for generataion step gen_x_times * pregeneration_frac amount of data will
+* **pregeneration_frac**: float = 2 - for generation step gen_x_times * pregeneration_frac amount of data will
   generated. However in postprocessing (1 + gen_x_times) % of original data will be returned
 * **gen_params**: dict params for GAN training
 
 For `generate_data_pipe` methods params:
 
 * **train_df**: pd.DataFrame Train dataframe which has separate target
 * **target**: pd.DataFrame Input target for the train dataset
 * **test_df**: pd.DataFrame Test dataframe - newly generated train dataframe should be close to it
 * **deep_copy**: bool = True - make copy of input files or not. If not input dataframes will be overridden
-* **only_adversarial**: bool = False - only adversarial fitering to train dataframe will be performed
+* **only_adversarial**: bool = False - only adversarial filtering to train dataframe will be performed
 * **use_adversarial**: bool = True - perform or not adversarial filtering
 * **only_generated_data**: bool = False  - After generation get only newly generated, without 
-  concating input train dataframe.  
+  concatenating input train dataframe.  
 * **@return**: -> Tuple[pd.DataFrame, pd.DataFrame] - Newly generated train dataframe and test data
 
 Thus, you may use this library to improve your dataset quality:
 
 ``` python
 def fit_predict(clf, X_train, y_train, X_test, y_test):
     clf.fit(X_train, y_train)
@@ -153,14 +157,19 @@
                                                                     )
 new_train = collect_dates(new_train)
 ```
 
 ## Experiments
 ### Datasets and experiment design
 
+**Check for data generation quality**
+Just use built-in function
+```
+compare_dataframes(original_df, generated_df) # return between 0 and 1
+```
 **Running experiment**
 
 To run experiment follow these steps:
 
 1. Clone the repository. All required dataset are stored in `./Research/data` folder
 2. Install requirements `pip install -r requirements.txt`
 4. Run all experiments  `python ./Research/run_experiment.py`. Run all experiments  `python run_experiment.py`. You may
@@ -185,19 +194,14 @@
 | credit                 |           0.997 |          **0.998** |                      0.997 |
 | employee               |           **0.986** |          0.966 |                      0.972 |
 | mortgages              |           0.984 |          0.964 |                      **0.988** |
 | poverty_A              |           0.937 |          **0.950** |                      0.933 |
 | taxi                   |           0.966 |          0.938 |                      **0.987** |
 | adult                  |           0.995 |          0.967 |                      **0.998** |
 
-## Acknowledgments
-
-The author would like to thank Open Data Science community [7] for many valuable discussions and educational help in the
-growing field of machine and deep learning.
-
 ## Citation
 
 If you use **GAN-for-tabular-data** in a scientific publication, we would appreciate references to the following BibTex entry:
 arxiv publication:
 ```bibtex
 @misc{ashrapov2020tabular,
       title={Tabular GANs for uneven distribution}, 
@@ -213,7 +217,9 @@
 
 [1] Lei Xu LIDS, Kalyan Veeramachaneni. Synthesizing Tabular Data using Generative Adversarial Networks (2018). arXiv:
 1811.11264v1 [cs.LG]
 
 [2] Alexia Jolicoeur-Martineau and Kilian Fatras and Tal Kachman. Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees ((2023) https://github.com/SamsungSAILMontreal/ForestDiffusion [cs.LG]
 
 [3] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni. Modeling Tabular data using Conditional GAN. NeurIPS, (2019)
+
+[4] Vadim Borisov and Kathrin Sessler and Tobias Leemann and Martin Pawelczyk and Gjergji Kasneci. Language Models are Realistic Tabular Data Generators. ICLR, (2023)
```

## Comparing `tabgan-2.0.5.dist-info/RECORD` & `tabgan-2.2.0.dist-info/RECORD`

 * *Files 19% similar despite different names*

```diff
@@ -7,19 +7,19 @@
 _ctgan/README.MD,sha256=zSNDsgIXevr-bUjEgOeDKlxV8RYoESShnRd3oqbIcpg,60
 _ctgan/__init__.py,sha256=InABC-CaliFy48om-ZZXYpkaAc6HYl-UqGUsJikMOgo,202
 _ctgan/conditional.py,sha256=YIrLbLkFoo1wQV85IkzXQ64S6hdyTxgVMBEQu3GIZ_8,4408
 _ctgan/models.py,sha256=p12SKtk93PfSufGYCII2hXlxWS-ehjCjVoX3CGLzOYs,2466
 _ctgan/sampler.py,sha256=CV6qS6j3ak7b61blVXgg8U03h32uAJ-qmaz6aY5Ip7I,1241
 _ctgan/synthesizer.py,sha256=D-DXtEf1KWbzx_ZudRhrhJKeIxZmt9DVFDkr-pwJuFA,11533
 _ctgan/transformer.py,sha256=Q4SJ1RzhUxci9v03-0OzIoaTqh_qTahlVECSqIt3SbA,5959
-tabgan/__init__.py,sha256=3N-yothmI3_WSN5CSOwE4MPz0ozpnq98-xlyI2mwcdU,374
+tabgan/__init__.py,sha256=whMF4h6WW0WfMDA4-i28t4vJ_W-wJDOwa0fxivqkKio,477
 tabgan/abc_sampler.py,sha256=o2NI8I5FjZzw5UGVpNmLpJ4SquGHeZKUIoBRF6djCwY,4630
-tabgan/adversarial_model.py,sha256=df9J58F-7gJUPsUy8yETMm7tZdhbrFvGqQdVg4pPiM0,8374
+tabgan/adversarial_model.py,sha256=HvK2T4utz2kY4gYRYajcMYkREUkSwZtpkD3Bs3hlTrA,8458
 tabgan/encoders.py,sha256=h2_XF_0y0wzavWPirvrwGQsUEK_ip6OUCN5a6DH4MUE,10998
-tabgan/sampler.py,sha256=AJhUUCR1rKH_K6Ncox42XI3xcjnk7OthCsi3gH084SU,17722
-tabgan/utils.py,sha256=4uj-_t_OW2RhHmGHEss7nqRhTGo3g9enTIjzysmyZLc,1766
-tabgan-2.0.5.dist-info/AUTHORS.rst,sha256=W9n6ZnEnD4-CCJnQYfYD2TK2bAO50DlyZ26Iq0cLrys,84
-tabgan-2.0.5.dist-info/LICENSE,sha256=tyJzqT3CJebQ4J1RdlGJoIJobfuZyYz6LL-7XmWWtcI,11474
-tabgan-2.0.5.dist-info/METADATA,sha256=iksrg4RekU3JkP4NY9HWa6YulCzcK9gmomxNtI09x_o,10741
-tabgan-2.0.5.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-tabgan-2.0.5.dist-info/top_level.txt,sha256=jGz7lHjOpBcJOj1d2rrHpwuLyqmw32PmLfzpCEg2Eck,31
-tabgan-2.0.5.dist-info/RECORD,,
+tabgan/sampler.py,sha256=-mMDJJZ3FMfivdArgagL36dWMVvOy3jUv8CE4rb_9uk,21364
+tabgan/utils.py,sha256=lhArvfD5ROXA2J-QfbKfHkXC393LO-fs5LHhQqmRY1Y,5961
+tabgan-2.2.0.dist-info/AUTHORS.rst,sha256=W9n6ZnEnD4-CCJnQYfYD2TK2bAO50DlyZ26Iq0cLrys,84
+tabgan-2.2.0.dist-info/LICENSE,sha256=tyJzqT3CJebQ4J1RdlGJoIJobfuZyYz6LL-7XmWWtcI,11474
+tabgan-2.2.0.dist-info/METADATA,sha256=Abxl5nrfwBBv79n7f3ED5NokkBRQ3wd7lT04oIG1H6Q,11332
+tabgan-2.2.0.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+tabgan-2.2.0.dist-info/top_level.txt,sha256=jGz7lHjOpBcJOj1d2rrHpwuLyqmw32PmLfzpCEg2Eck,31
+tabgan-2.2.0.dist-info/RECORD,,
```

